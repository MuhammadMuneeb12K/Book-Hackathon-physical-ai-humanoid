"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[616],{2834:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4/visual-object-grounding","title":"4.4 Visual Object Grounding","description":"For a humanoid robot, receiving a natural language command and generating a high-level plan (as we saw in the previous chapter) is only half the battle. The robot must then connect the abstract concepts in the plan\u2014like \\"red apple\\" or \\"table\\"\u2014to concrete entities in its physical world. This crucial step is known as Visual Object Grounding. This chapter explores how computer vision techniques are applied to identify, localize, and semantically link objects mentioned in verbal commands to their visual representations in the robot\'s sensor data, enabling the robot to physically interact with its environment meaningfully.","source":"@site/docs/module-4/04-visual-object-grounding.mdx","sourceDirName":"module-4","slug":"/module-4/visual-object-grounding","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/visual-object-grounding","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"visual-object-grounding","title":"4.4 Visual Object Grounding"},"sidebar":"defaultSidebar","previous":{"title":"4.3 LLM-Based Cognitive Planning","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/llm-cognitive-planning"},"next":{"title":"4.5 Full Voice-to-Action Integration Pipeline","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/full-vla-pipeline"}}');var r=i(4848),s=i(8453);const o={id:"visual-object-grounding",title:"4.4 Visual Object Grounding"},a=void 0,c={},l=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Tools",id:"tools",level:2},{value:"Chapter Sections",id:"chapter-sections",level:2},{value:"4.4.1 The Challenge of Object Grounding",id:"441-the-challenge-of-object-grounding",level:3},{value:"4.4.2 Object Detection for Initial Localization (YOLO)",id:"442-object-detection-for-initial-localization-yolo",level:3},{value:"4.4.3 Object Segmentation for Precise Delineation (SAM)",id:"443-object-segmentation-for-precise-delineation-sam",level:3},{value:"4.4.4 3D Object Localization with Depth Information",id:"444-3d-object-localization-with-depth-information",level:3},{value:"4.4.5 Semantic Association and Robust Grounding",id:"445-semantic-association-and-robust-grounding",level:3},{value:"Required Diagrams",id:"required-diagrams",level:2},{value:"Hands-on Labs",id:"hands-on-labs",level:2},{value:"Lab 4.4.1: Detect Objects Using YOLO or SAM, and Localize in 3D",id:"lab-441-detect-objects-using-yolo-or-sam-and-localize-in-3d",level:3},{value:"Expected Output",id:"expected-output",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Real-world Applications",id:"real-world-applications",level:2},{value:"Edge Cases",id:"edge-cases",level:2},{value:"<strong>Key Entities</strong>",id:"key-entities",level:3},{value:"<strong>References</strong>",id:"references",level:3}];function d(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:'For a humanoid robot, receiving a natural language command and generating a high-level plan (as we saw in the previous chapter) is only half the battle. The robot must then connect the abstract concepts in the plan\u2014like "red apple" or "table"\u2014to concrete entities in its physical world. This crucial step is known as Visual Object Grounding. This chapter explores how computer vision techniques are applied to identify, localize, and semantically link objects mentioned in verbal commands to their visual representations in the robot\'s sensor data, enabling the robot to physically interact with its environment meaningfully.'}),"\n",(0,r.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,r.jsx)(n.p,{children:"The goal of this chapter is to teach students how to apply computer vision techniques (e.g., YOLO/SAM) for object identification and grounding, allowing humanoid robots to perceive and interact with specific objects in their environment, based on linguistic descriptions from an LLM-generated plan."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the concept of visual object grounding and its importance in VLA systems."}),"\n",(0,r.jsx)(n.li,{children:"Learn the principles behind modern object detection and segmentation models (e.g., YOLO, SAM)."}),"\n",(0,r.jsx)(n.li,{children:"Grasp how to integrate computer vision models with a ROS 2 system for real-time processing."}),"\n",(0,r.jsx)(n.li,{children:"Implement a node that takes an object name from an LLM plan and identifies its 3D location in the robot's visual field."}),"\n",(0,r.jsx)(n.li,{children:"Differentiate between 2D detection and 3D localization of objects."}),"\n",(0,r.jsx)(n.li,{children:"Understand the challenges of object grounding, such as occlusion, lighting, and novel objects."}),"\n",(0,r.jsx)(n.li,{children:"Explore methods for associating detected objects with semantic labels for interaction."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Familiarity with ROS 2 concepts (topics, messages)."}),"\n",(0,r.jsx)(n.li,{children:"Basic understanding of computer vision fundamentals (image processing, feature extraction)."}),"\n",(0,r.jsx)(n.li,{children:"Conceptual understanding of neural networks for object detection."}),"\n",(0,r.jsx)(n.li,{children:"A simulated robot with an RGB-D camera publishing to ROS 2 (e.g., from Module 2, Lab 2.4.1)."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual Object Grounding:"})," The process of associating linguistic descriptions of objects with their corresponding visual representations and physical locations in the real world as perceived by the robot's sensors."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Detection:"})," A computer vision task that identifies instances of semantic objects in an image and localizes each instance by drawing a bounding box around it."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Segmentation:"})," A computer vision task that goes beyond detection by identifying not just the bounding box, but the precise pixel-level mask of each object instance."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLO (You Only Look Once):"})," A popular family of real-time object detection models known for their speed and accuracy."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SAM (Segment Anything Model):"})," A powerful segmentation model developed by Meta AI that can segment any object in an image, given a prompt (e.g., a bounding box, a point, or text)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB-D Camera:"})," A sensor that provides both color (RGB) images and depth information (D), crucial for 3D object localization."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Point Cloud:"})," A set of 3D data points typically representing the external surface of an object or environment, generated from depth data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D Object Localization:"})," Determining the 3D position (x, y, z) and orientation (roll, pitch, yaw) of a detected object relative to the robot's frame."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TF2:"})," Used to transform object locations from the camera frame to the robot's base frame or world frame."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic Understanding:"}),' Linking visual detections to meaningful categories and properties (e.g., "apple" is a fruit, "red" is its color).']}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Humble:"})," For integrating vision components."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["OpenCV (or ",(0,r.jsx)(n.code,{children:"cv_bridge"}),"):"]})," For image processing and conversion between ROS 2 image messages and OpenCV formats."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLOv8/YOLO-NAS (or other models):"})," Object detection libraries (e.g., ",(0,r.jsx)(n.code,{children:"ultralytics"})," for YOLOv8)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SAM (Meta AI):"})," Segmentation model library."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PyTorch/TensorFlow:"})," Deep learning frameworks (if running custom models)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"tf2_ros"}),":"]})," For coordinate transformations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RViz2:"})," For visualizing detections and 3D localized objects."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"chapter-sections",children:"Chapter Sections"}),"\n",(0,r.jsx)(n.h3,{id:"441-the-challenge-of-object-grounding",children:"4.4.1 The Challenge of Object Grounding"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Bridging the gap between language and vision."}),"\n",(0,r.jsx)(n.li,{children:'Why "find the red apple" is hard: what does "red" mean visually? What defines an "apple"? Where is "the" table?'}),"\n",(0,r.jsx)(n.li,{children:"Need for 3D information for physical interaction."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"442-object-detection-for-initial-localization-yolo",children:"4.4.2 Object Detection for Initial Localization (YOLO)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Introduction to YOLO:"})," Real-time object detection, bounding box prediction, class labels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"How YOLO Works (High-Level):"})," Grid-based detection, unified network."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrating YOLO into ROS 2:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Subscribing to ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/Image"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Running inference with a pre-trained YOLO model."}),"\n",(0,r.jsxs)(n.li,{children:["Publishing results: ",(0,r.jsx)(n.code,{children:"vision_msgs/msg/Detection2DArray"})," (or custom message)."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Limitations of 2D detection for robot interaction."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"443-object-segmentation-for-precise-delineation-sam",children:"4.4.3 Object Segmentation for Precise Delineation (SAM)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Beyond Bounding Boxes:"})," The need for precise object masks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Introduction to SAM:"})," Promptable segmentation, zero-shot capabilities."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Using SAM with YOLO:"})," YOLO provides the bounding box, SAM refines it into a mask."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrating SAM:"})," Similar ROS 2 pipeline, but outputting pixel-level masks."]}),"\n",(0,r.jsx)(n.li,{children:"Benefits for grasping and manipulation: better understanding of object shape."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"444-3d-object-localization-with-depth-information",children:"4.4.4 3D Object Localization with Depth Information"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Combining RGB-D:"})," Using depth camera data to infer 3D positions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Converting 2D Pixel to 3D Point:"})," Using camera intrinsic parameters and depth map."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"From Camera Frame to Robot Base Frame:"})," Applying TF2 transformations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Centroid Calculation:"})," Finding the approximate center of a detected object in 3D."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"pcl_ros"})," (or similar):"]})," Working with point clouds in ROS 2."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"445-semantic-association-and-robust-grounding",children:"4.4.5 Semantic Association and Robust Grounding"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Matching LLM's Object Name to Detected Class:"})," Handling synonyms, slight variations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Filtering Detections:"}),' Using color, size, and context to refine object selection (e.g., "the ',(0,r.jsx)(n.em,{children:"red"}),' apple").']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Handling Multiple Instances:"}),' "Which one?" - disambiguation strategies.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Persistent Object Tracking:"})," Maintaining an object's identity over time and across frames."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"required-diagrams",children:"Required Diagrams"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Grounding Pipeline:"})," A flowchart showing raw image input -> detection/segmentation -> 2D bounding box/mask -> depth fusion -> 3D localization -> TF2 transformation -> confirmed object for LLM plan."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLO/SAM Workflow:"})," A diagram illustrating how YOLO identifies objects and SAM refines their masks, potentially feeding into 3D localization."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-labs",children:"Hands-on Labs"}),"\n",(0,r.jsx)(n.h3,{id:"lab-441-detect-objects-using-yolo-or-sam-and-localize-in-3d",children:"Lab 4.4.1: Detect Objects Using YOLO or SAM, and Localize in 3D"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective:"})," Create a ROS 2 node that subscribes to an RGB-D camera feed from Isaac Sim, uses a pre-trained YOLO model for object detection, and then leverages the depth information to estimate the 3D position of a specified object."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prerequisites:"})," Completed Lab 2.4.1 (humanoid with RGB-D camera in Gazebo/Isaac Sim), Lab 4.3.1 (LLM planner outputting object name). ",(0,r.jsx)(n.code,{children:"ultralytics"})," (for YOLOv8) installed (",(0,r.jsx)(n.code,{children:"pip install ultralytics"}),"). Ensure ",(0,r.jsx)(n.code,{children:"torch"})," and ",(0,r.jsx)(n.code,{children:"torchvision"})," are also installed."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Instructions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ensure your simulated humanoid in Isaac Sim/Gazebo is publishing RGB images and depth images."})," (e.g., ",(0,r.jsx)(n.code,{children:"/simple_humanoid/camera/image_raw"})," and ",(0,r.jsx)(n.code,{children:"/simple_humanoid/camera/depth_image_raw"})," as configured in Lab 2.4.1)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Create a new ROS 2 Python package for object detection:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>/src\r\nros2 pkg create --build-type ament_python object_detector_3d --dependencies rclpy sensor_msgs cv_bridge vision_msgs tf2_ros tf2_geometry_msgs\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Navigate into the package and create ",(0,r.jsx)(n.code,{children:"src/object_detector_3d/detector_node.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2 # Include PointCloud2 for depth processing\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO # For YOLOv8\r\nimport torch\r\nfrom tf2_ros import Buffer, TransformListener, TransformException\r\nfrom tf2_geometry_msgs import do_transform_point\r\nfrom geometry_msgs.msg import PointStamped, TransformStamped # For 3D point output\r\nfrom vision_msgs.msg import Detection2D, Detection2DArray, ObjectHypothesisWithPose # For publishing detections\r\n\r\nclass ObjectDetector3D(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__('object_detector_3d_node')\r\n        self.declare_parameter('model_path', 'yolov8n.pt') # Default YOLOv8 nano model\r\n        self.model_path = self.get_parameter('model_path').get_parameter_value().string_value\r\n        self.get_logger().info(f'Loading YOLOv8 model from: {self.model_path}')\r\n        self.model = YOLO(self.model_path)\r\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n        self.get_logger().info(f'Using device: {self.device}')\r\n\r\n        self.cv_bridge = CvBridge()\r\n        self.tf_buffer = Buffer()\r\n        self.tf_listener = TransformListener(self.tf_buffer, self)\r\n\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/simple_humanoid/camera/image_raw',\r\n            self.image_callback,\r\n            10)\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            '/simple_humanoid/camera/depth_image_raw',\r\n            self.depth_callback,\r\n            10)\r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            '/simple_humanoid/camera/camera_info',\r\n            self.camera_info_callback,\r\n            10)\r\n        \r\n        self.detection_pub = self.create_publisher(Detection2DArray, '/object_detections_2d', 10)\r\n        self.object_3d_location_pub = self.create_publisher(PointStamped, '/object_3d_location', 10)\r\n\r\n        self.latest_image = None\r\n        self.latest_depth_image = None\r\n        self.camera_info = None\r\n\r\n        self.image_header_frame_id = \"camera_link\" # Default, update from CameraInfo if available\r\n        self.camera_intrinsics = None # Will store K matrix\r\n\r\n        self.get_logger().info('Object Detector 3D Node initialized. Waiting for image, depth, and camera info...')\r\n\r\n    def camera_info_callback(self, msg: CameraInfo):\r\n        self.camera_info = msg\r\n        self.camera_intrinsics = np.array(msg.k).reshape(3, 3)\r\n        self.image_header_frame_id = msg.header.frame_id\r\n        self.get_logger().info(f'Received camera info for frame: {self.image_header_frame_id}')\r\n        # Unsubscribe after receiving once if camera info is static\r\n        self.destroy_subscription(self.camera_info_sub)\r\n\r\n    def image_callback(self, msg: Image):\r\n        self.latest_image = msg\r\n\r\n    def depth_callback(self, msg: Image):\r\n        self.latest_depth_image = msg\r\n        # Process image and depth only when both are available and camera info is loaded\r\n        if self.latest_image and self.latest_depth_image and self.camera_info:\r\n            self.process_frames()\r\n\r\n    def process_frames(self):\r\n        try:\r\n            cv_image = self.cv_bridge.imgmsg_to_cv2(self.latest_image, \"bgr8\")\r\n            # Depth image encoding can be tricky, typically mono16 or 32FC1\r\n            # For Gazebo default, often 32FC1 (float32)\r\n            cv_depth_image = self.cv_bridge.imgmsg_to_cv2(self.latest_depth_image, \"32FC1\")\r\n        except Exception as e:\r\n            self.get_logger().error(f\"CvBridge Error: {e}\")\r\n            return\r\n\r\n        # Perform YOLO detection\r\n        results = self.model(cv_image, verbose=False, device=self.device)\r\n        detections_array_msg = Detection2DArray()\r\n        detections_array_msg.header = self.latest_image.header # Use image header for detections\r\n        detections_array_msg.header.frame_id = self.image_header_frame_id\r\n\r\n        for r in results:\r\n            for box in r.boxes:\r\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\r\n                confidence = float(box.conf[0])\r\n                class_id = int(box.cls[0])\r\n                class_name = self.model.names[class_id]\r\n\r\n                detection = Detection2D()\r\n                detection.header = detections_array_msg.header\r\n                detection.bbox.center.x = float(x1 + x2) / 2.0\r\n                detection.bbox.center.y = float(y1 + y2) / 2.0\r\n                detection.bbox.size_x = float(x2 - x1)\r\n                detection.bbox.size_y = float(y2 - y1)\r\n\r\n                hypothesis = ObjectHypothesisWithPose()\r\n                hypothesis.hypothesis.class_id = class_name\r\n                hypothesis.hypothesis.score = confidence\r\n                detection.results.append(hypothesis)\r\n                \r\n                detections_array_msg.detections.append(detection)\r\n\r\n                # --- 3D Localization ---\r\n                # Get depth at the center of the bounding box\r\n                center_x, center_y = int(detection.bbox.center.x), int(detection.bbox.center.y)\r\n                if 0 <= center_y < cv_depth_image.shape[0] and 0 <= center_x < cv_depth_image.shape[1]:\r\n                    depth_value = cv_depth_image[center_y, center_x]\r\n                    \r\n                    if np.isfinite(depth_value) and depth_value > 0:\r\n                        # Convert 2D pixel to 3D point using camera intrinsics\r\n                        # P_x = Z * ( (u - c_x) / f_x )\r\n                        # P_y = Z * ( (v - c_y) / f_y )\r\n                        # P_z = Z\r\n                        \r\n                        fx = self.camera_intrinsics[0, 0]\r\n                        fy = self.camera_intrinsics[1, 1]\r\n                        cx = self.camera_intrinsics[0, 2]\r\n                        cy = self.camera_intrinsics[1, 2]\r\n\r\n                        # Point in camera frame\r\n                        point_camera_frame = PointStamped()\r\n                        point_camera_frame.header = detections_array_msg.header\r\n                        point_camera_frame.point.x = depth_value * ((center_x - cx) / fx)\r\n                        point_camera_frame.point.y = depth_value * ((center_y - cy) / fy)\r\n                        point_camera_frame.point.z = depth_value # Z is depth\r\n\r\n                        # Transform to robot base frame\r\n                        target_frame = 'base_link'\r\n                        source_frame = self.image_header_frame_id\r\n\r\n                        try:\r\n                            transform = self.tf_buffer.lookup_transform(\r\n                                target_frame,\r\n                                source_frame,\r\n                                detections_array_msg.header.stamp, # Use timestamp from image header\r\n                                rclpy.duration.Duration(seconds=0.1) # Timeout\r\n                            )\r\n                            point_robot_frame = do_transform_point(point_camera_frame, transform)\r\n                            self.get_logger().info(\r\n                                f\"Detected '{class_name}' at 3D pos in '{target_frame}': \"\r\n                                f\"x={point_robot_frame.point.x:.2f}, y={point_robot_frame.point.y:.2f}, z={point_robot_frame.point.z:.2f}\"\r\n                            )\r\n                            self.object_3d_location_pub.publish(point_robot_frame)\r\n\r\n                        except TransformException as ex:\r\n                            self.get_logger().warn(f\"Could not transform point from '{source_frame}' to '{target_frame}': {ex}\")\r\n                    else:\r\n                        self.get_logger().warn(f\"Depth value at ({center_x},{center_y}) for '{class_name}' is invalid.\")\r\n\r\n        self.detection_pub.publish(detections_array_msg)\r\n        \r\n        # Clear latest frames after processing\r\n        self.latest_image = None\r\n        self.latest_depth_image = None\r\n\r\n\r\ndef destroy_node(self):\r\n    super().destroy_node()\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"def main(args=None):\r\nrclpy.init(args=args)\r\ndetector_node = ObjectDetector3D()\r\ntry:\r\nrclpy.spin(detector_node)\r\nexcept KeyboardInterrupt:\r\npass\r\nfinally:\r\ndetector_node.destroy_node()\r\nrclpy.shutdown()"}),"\n",(0,r.jsxs)(n.p,{children:["if ",(0,r.jsx)(n.strong,{children:"name"})," == '",(0,r.jsx)(n.strong,{children:"main"}),"':\r\nmain()"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"4.  **Edit `setup.py` for `object_detector_3d`:** Add the entry point.\r\n    ```python\r\n    from setuptools import find_packages, setup\r\n\r\n    package_name = 'object_detector_3d'\r\n\r\n    setup(\r\n        name=package_name,\r\n        version='0.0.0',\r\n        packages=find_packages(exclude=['test']),\r\n        data_files=[\r\n            ('share/' + package_name, ['package.xml']),\r\n        ],\r\n        install_requires=['setuptools', 'opencv-python', 'ultralytics', 'torch', 'torchvision'], # Add dependencies\r\n        zip_safe=True,\r\n        maintainer='your_name',\r\n        maintainer_email='your_email@example.com',\r\n        description='ROS 2 package for 3D object detection and localization.',\r\n        license='Apache-2.0',\r\n        tests_require=['pytest'],\r\n        entry_points={\r\n            'console_scripts': [\r\n                'detector_node = object_detector_3d.detector_node:main',\r\n            ],\r\n        },\r\n    )\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build your package and source your workspace:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>\r\ncolcon build --packages-select object_detector_3d\r\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Launch Isaac Sim/Gazebo with your humanoid and RGB-D camera."})," Ensure the camera is publishing images and camera info. For example, using the ",(0,r.jsx)(n.code,{children:"display_humanoid.launch.py"})," from Lab 2.4.1."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"In a separate terminal, run the object detector node:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run object_detector_3d detector_node\n"})}),"\n",(0,r.jsxs)(n.em,{children:["Note: The first time ",(0,r.jsx)(n.code,{children:"ultralytics"})," runs, it will download the YOLO model, which takes time."]})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"In Isaac Sim/Gazebo, place some detectable objects"}),' (e.g., a "cup", "bottle", "chair") within the camera\'s field of view.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Monitor ",(0,r.jsx)(n.code,{children:"/object_detections_2d"})," and ",(0,r.jsx)(n.code,{children:"/object_3d_location"})," topics:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /object_detections_2d\r\nros2 topic echo /object_3d_location\n"})}),"\n","You should see 2D bounding box detections and the estimated 3D position of objects relative to the ",(0,r.jsx)(n.code,{children:"base_link"})," frame."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visualize in RViz2:"})," Add ",(0,r.jsx)(n.code,{children:"Image"}),", ",(0,r.jsx)(n.code,{children:"Depth Image"}),", ",(0,r.jsx)(n.code,{children:"CameraInfo"}),", ",(0,r.jsx)(n.code,{children:"Detection2DArray"})," (from ",(0,r.jsx)(n.code,{children:"vision_msgs"}),") and ",(0,r.jsx)(n.code,{children:"PointStamped"})," displays in RViz2 to see the camera feed, detections, and the 3D point of the object. Make sure the ",(0,r.jsx)(n.code,{children:"Fixed Frame"})," in RViz is set to ",(0,r.jsx)(n.code,{children:"base_link"})," or ",(0,r.jsx)(n.code,{children:"odom"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"expected-output",children:"Expected Output"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A functional ROS 2 node capable of processing RGB and depth images from a simulated camera."}),"\n",(0,r.jsx)(n.li,{children:"Successful real-time object detection using a YOLOv8 model."}),"\n",(0,r.jsx)(n.li,{children:"Accurate 3D localization of detected objects by combining 2D detections with depth information and camera intrinsics."}),"\n",(0,r.jsx)(n.li,{children:"The 2D detections and 3D locations published to respective ROS 2 topics."}),"\n",(0,r.jsx)(n.li,{children:"Visualization of detections and 3D points in RViz2."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Explain the concept of visual object grounding in a VLA system. Why is it more complex than just 2D object detection?"}),"\n",(0,r.jsx)(n.li,{children:"What is the key advantage of using an RGB-D camera over a monocular RGB camera for 3D object localization?"}),"\n",(0,r.jsx)(n.li,{children:"Describe the process of converting a 2D pixel coordinate and its corresponding depth value into a 3D point in the camera's coordinate frame."}),"\n",(0,r.jsx)(n.li,{children:"What are the benefits of using a segmentation model like SAM in conjunction with an object detector like YOLO for robot manipulation tasks?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"real-world-applications",children:"Real-world Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pick-and-Place Operations:"})," Humanoid robots accurately identifying and locating target objects for grasping and manipulation in unstructured environments."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inventory Management:"})," Robots autonomously scanning shelves, identifying specific products, and updating inventory records."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human-Robot Collaboration:"}),' Robots visually grounding objects of interest discussed by humans (e.g., "Pass me that tool"), enabling shared task execution.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Search and Rescue:"})," Identifying specific items or people in cluttered or hazardous environments, providing their precise 3D locations for intervention."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"edge-cases",children:"Edge Cases"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Occlusion:"})," Objects partially or fully hidden from the camera view, making detection and 3D localization difficult."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lighting Variations:"})," Extreme lighting conditions (too dark, too bright, harsh shadows) can degrade the performance of object detection models."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reflective/Transparent Objects:"})," Depth cameras often struggle with reflective or transparent surfaces, leading to inaccurate depth measurements and thus incorrect 3D localization."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Novel Objects:"})," Pre-trained object detectors might not recognize objects not present in their training data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational Latency:"})," Running complex vision models at high frame rates can introduce significant latency, impacting real-time control."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"key-entities",children:(0,r.jsx)(n.strong,{children:"Key Entities"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual Object Grounding:"})," The crucial step in VLA systems where abstract linguistic references to objects are linked to their concrete visual and spatial manifestations in the robot's perception."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Detection:"})," A computer vision technique that localizes and classifies objects within an image, typically outputting bounding boxes and class labels."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Segmentation:"})," A computer vision technique that provides a pixel-level mask for each object instance in an image, offering a more precise understanding of object shape and boundaries."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"YOLO (You Only Look Once):"})," A highly efficient and widely used real-time object detection algorithm, known for its speed and accuracy in localizing multiple objects in a single pass."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"SAM (Segment Anything Model):"})," A powerful, promptable image segmentation model that can generate high-quality object masks given various types of prompts (e.g., points, boxes, text)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB-D Camera:"})," A sensor that provides both a conventional color (RGB) image and a corresponding depth map, which is essential for determining the 3D position of objects."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D Object Localization:"})," The process of calculating the precise 3D coordinates (x, y, z) of an object in a given coordinate frame, often achieved by combining 2D detection with depth information."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"cv_bridge"}),":"]})," A ROS 2 package that facilitates conversion between ROS 2 ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," messages and OpenCV (CvImage) image formats, enabling the use of standard computer vision libraries."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"vision_msgs/msg/Detection2DArray"}),":"]})," A standard ROS 2 message type to represent an array of 2D object detections, each including a bounding box, confidence, and class label."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"references",children:(0,r.jsx)(n.strong,{children:"References"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Redmon, J., et al. (2016). You Only Look Once: Unified, Real-Time Object Detection. ",(0,r.jsx)(n.em,{children:"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}),". (Placeholder citation)"]}),"\n",(0,r.jsxs)(n.li,{children:["Kirillov, A., et al. (2023). Segment Anything. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:2304.02643"}),". (Placeholder citation)"]}),"\n",(0,r.jsxs)(n.li,{children:["Hussain, S. A., et al. (2020). 3D Object Detection for Autonomous Driving: A Review. ",(0,r.jsx)(n.em,{children:"Sensors, 20"}),"(22), 6561. (Placeholder citation)"]}),"\n",(0,r.jsxs)(n.li,{children:["Rosum, B. (2012). ",(0,r.jsx)(n.em,{children:"Learning ROS for Robotics Programming"}),". Packt Publishing. (Placeholder citation)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);