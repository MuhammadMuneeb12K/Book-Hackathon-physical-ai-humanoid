"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[515],{6899:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/llm-cognitive-planning","title":"4.3 LLM-Based Cognitive Planning","description":"With the ability to accurately transcribe human voice commands into text, the next critical component of a Vision-Language-Action (VLA) system for humanoid robots is to interpret these commands and translate them into a coherent sequence of robot-executable actions. This chapter delves into the exciting realm of LLM-based cognitive planning, where Large Language Models (LLMs) are leveraged not just for understanding human language, but for complex symbolic reasoning, task decomposition, and generating high-level plans that guide a robot\'s behavior.","source":"@site/docs/module-4/03-llm-cognitive-planning.mdx","sourceDirName":"module-4","slug":"/module-4/llm-cognitive-planning","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/llm-cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"llm-cognitive-planning","title":"4.3 LLM-Based Cognitive Planning"},"sidebar":"defaultSidebar","previous":{"title":"4.2 Voice Command Systems with Whisper","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/voice-command-whisper"},"next":{"title":"4.4 Visual Object Grounding","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/visual-object-grounding"}}');var s=i(4848),o=i(8453);const r={id:"llm-cognitive-planning",title:"4.3 LLM-Based Cognitive Planning"},a=void 0,l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Tools",id:"tools",level:2},{value:"Chapter Sections",id:"chapter-sections",level:2},{value:"4.3.1 The Rise of LLMs in Robotics",id:"431-the-rise-of-llms-in-robotics",level:3},{value:"4.3.2 Natural Language Task Decomposition with LLMs",id:"432-natural-language-task-decomposition-with-llms",level:3},{value:"4.3.3 Prompt Engineering for Robot Planning",id:"433-prompt-engineering-for-robot-planning",level:3},{value:"4.3.4 Designing an LLM-Based Planner ROS 2 Node",id:"434-designing-an-llm-based-planner-ros-2-node",level:3},{value:"4.3.5 Grounding LLM Plans in Physical Reality",id:"435-grounding-llm-plans-in-physical-reality",level:3},{value:"Required Diagrams",id:"required-diagrams",level:2},{value:"Hands-on Labs",id:"hands-on-labs",level:2},{value:"Lab 4.3.1: Generate Robot Action Plans with LLM",id:"lab-431-generate-robot-action-plans-with-llm",level:3},{value:"Expected Output",id:"expected-output",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Real-world Applications",id:"real-world-applications",level:2},{value:"Edge Cases",id:"edge-cases",level:2},{value:"<strong>Key Entities</strong>",id:"key-entities",level:3},{value:"<strong>References</strong>",id:"references",level:3}];function d(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"With the ability to accurately transcribe human voice commands into text, the next critical component of a Vision-Language-Action (VLA) system for humanoid robots is to interpret these commands and translate them into a coherent sequence of robot-executable actions. This chapter delves into the exciting realm of LLM-based cognitive planning, where Large Language Models (LLMs) are leveraged not just for understanding human language, but for complex symbolic reasoning, task decomposition, and generating high-level plans that guide a robot's behavior."}),"\n",(0,s.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,s.jsx)(n.p,{children:"The goal of this chapter is to teach students how to use LLMs (e.g., ChatGPT/Claude) to convert natural language commands into sequences of robotic actions, enabling high-level cognitive planning for humanoids within a VLA framework."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the role of Large Language Models (LLMs) in robotic cognitive planning."}),"\n",(0,s.jsx)(n.li,{children:"Grasp how LLMs can perform natural language task decomposition."}),"\n",(0,s.jsx)(n.li,{children:"Learn techniques for prompting LLMs to generate structured robot action plans."}),"\n",(0,s.jsx)(n.li,{children:"Differentiate between high-level human commands and low-level robot actions."}),"\n",(0,s.jsx)(n.li,{children:"Implement a ROS 2 node that communicates with an LLM API to generate action sequences."}),"\n",(0,s.jsx)(n.li,{children:"Understand the challenges of grounding abstract LLM plans into physical robot capabilities."}),"\n",(0,s.jsx)(n.li,{children:"Explore strategies for error handling and replanning with LLMs."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Familiarity with Python programming and ",(0,s.jsx)(n.code,{children:"rclpy"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2 topics and actions."}),"\n",(0,s.jsx)(n.li,{children:"Conceptual understanding of Large Language Models (LLMs) and their capabilities."}),"\n",(0,s.jsx)(n.li,{children:"An OpenAI (ChatGPT) or Anthropic (Claude) API key (or access to a local LLM)."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large Language Model (LLM):"})," A type of artificial intelligence model trained on vast amounts of text data, capable of understanding, generating, and reasoning with human-like language."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cognitive Planning:"})," The process of translating a high-level goal into a sequence of sub-goals or actions, often involving symbolic reasoning and state estimation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Task Decomposition:"})," Breaking down a complex natural language command into simpler, sequential steps that a robot can execute."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering:"})," The art and science of crafting effective inputs (prompts) to LLMs to elicit desired outputs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Function Calling / Tool Use:"})," The capability of modern LLMs to generate arguments for functions or use external tools to perform actions. Crucial for robotics."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Primitive:"})," A fundamental, low-level robotic action that can be executed directly by the robot's control system (e.g., ",(0,s.jsx)(n.code,{children:"navigate_to(location)"}),", ",(0,s.jsx)(n.code,{children:"grasp_object(object_id)"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Representation:"})," How the current state of the robot and environment is described to the LLM to inform its planning."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback Loop:"})," Providing the LLM with the outcome of executed actions to enable adaptive planning and error correction."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan Validation:"})," Ensuring that the LLM-generated plan is feasible and safe for the robot to execute."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenAI API (or Anthropic Claude API):"})," For accessing powerful LLMs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Python ",(0,s.jsx)(n.code,{children:"requests"})," library (or LLM client libraries):"]})," For making API calls."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Humble:"})," For integrating the LLM planner node into the robotic system."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"std_msgs/msg/String"}),":"]})," For receiving transcribed speech."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Custom ROS 2 message/action types:"})," For structured robot plans."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Code Editor:"})," Visual Studio Code."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chapter-sections",children:"Chapter Sections"}),"\n",(0,s.jsx)(n.h3,{id:"431-the-rise-of-llms-in-robotics",children:"4.3.1 The Rise of LLMs in Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Bridging the symbolic-subsymbolic gap in AI."}),"\n",(0,s.jsx)(n.li,{children:"LLMs as general-purpose reasoning engines for high-level tasks."}),"\n",(0,s.jsx)(n.li,{children:"From generating text to generating executable code/plans."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"432-natural-language-task-decomposition-with-llms",children:"4.3.2 Natural Language Task Decomposition with LLMs"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'How LLMs can break down complex commands like "make coffee" into sub-tasks: "get mug," "brew coffee," "add sugar."'}),"\n",(0,s.jsx)(n.li,{children:"Identifying implied steps and preconditions."}),"\n",(0,s.jsx)(n.li,{children:"The importance of context and domain knowledge for effective decomposition."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"433-prompt-engineering-for-robot-planning",children:"4.3.3 Prompt Engineering for Robot Planning"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Zero-shot, Few-shot, and Chain-of-Thought Prompting:"})," Strategies for guiding LLM behavior."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Defining Robot Capabilities (Tools/Functions):"})," Informing the LLM about the robot's available actions.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Example: ",(0,s.jsx)(n.code,{children:"navigate_to(location)"}),", ",(0,s.jsx)(n.code,{children:"grasp_object(object_id)"}),", ",(0,s.jsx)(n.code,{children:"report_status(message)"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Structured Output:"})," Requesting LLM to generate plans in a specific format (e.g., JSON, YAML, Python function calls)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Persona and Role-Playing:"}),' Instructing the LLM to act as a "robot planner."']}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"434-designing-an-llm-based-planner-ros-2-node",children:"4.3.4 Designing an LLM-Based Planner ROS 2 Node"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input:"})," Subscribing to a topic with transcribed human commands (e.g., ",(0,s.jsx)(n.code,{children:"/speech_to_text"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Query:"})," Formatting the prompt to the LLM API, including the command and robot capabilities."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output Parsing:"})," Interpreting the LLM's generated plan (e.g., a list of action primitives)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Dispatcher:"})," Publishing parsed actions to a ROS 2 action server or a topic for execution."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"State Representation:"})," How to inform the LLM about the current state of the robot and environment."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"435-grounding-llm-plans-in-physical-reality",children:"4.3.5 Grounding LLM Plans in Physical Reality"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Symbolic to Subsymbolic:"}),' Translating LLM\'s abstract concepts (e.g., "kitchen") into concrete coordinates or object IDs.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan Validation and Safety Checks:"})," Ensuring the LLM's proposed actions are physically feasible and safe."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling and Replanning:"})," What happens if an action fails? How can the LLM generate an alternative plan?"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"required-diagrams",children:"Required Diagrams"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Cognitive Planning Flow:"})," A diagram showing transcribed text input -> LLM -> structured action plan -> robot action dispatcher."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering Example:"})," Illustrating a prompt that defines robot tools and a task, and the expected LLM output."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-labs",children:"Hands-on Labs"}),"\n",(0,s.jsx)(n.h3,{id:"lab-431-generate-robot-action-plans-with-llm",children:"Lab 4.3.1: Generate Robot Action Plans with LLM"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective:"})," Create a ROS 2 Python node that subscribes to transcribed speech commands, sends them to an LLM (e.g., OpenAI's GPT-4 or Claude), and publishes a structured sequence of robot action primitives based on the LLM's response."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites:"})," Completed Lab 4.2.1 (Whisper ASR node), OpenAI/Anthropic API key, Python programming."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Instructions:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Create a new ROS 2 Python package for the LLM planner:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>/src\r\nros2 pkg create --build-type ament_python llm_robot_planner --dependencies rclpy std_msgs\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Install LLM client library:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai # Or anthropic for Claude\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Set up your API Key:"})," Store your OpenAI API key in an environment variable ",(0,s.jsx)(n.code,{children:"OPENAI_API_KEY"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Navigate into the package and create ",(0,s.jsx)(n.code,{children:"src/llm_robot_planner/llm_planner_node.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python"})}),"\n"]}),"\n"]}),"\n",'\n  import rclpy\n  from rclpy.node import Node\n  from std_msgs.msg import String\n  import openai # Or anthropic\n  import json\n  import os\n\n  class LLMRobotPlanner(Node):\n\n      def __init__(self):\n          super().__init__(\'llm_robot_planner\')\n          self.subscription = self.create_subscription(\n              String,\n              \'/speech_to_text\',\n              self.speech_callback,\n              10)\n          self.action_publisher_ = self.create_publisher(String, \'/robot_action_plan\', 10)\n          self.get_logger().info(\'LLM Robot Planner Node initialized.\')\n\n          # Initialize OpenAI client (or Anthropic for Claude)\n          self.api_key = os.getenv("OPENAI_API_KEY") # Ensure this env var is set\n          if not self.api_key:\n              self.get_logger().error("OPENAI_API_KEY environment variable not set.")\n              # rclpy.shutdown()\n              return\n\n          self.client = openai.OpenAI(api_key=self.api_key) # For OpenAI\n          # For Anthropic: self.client = anthropic.Anthropic(api_key=self.api_key)\n\n          # Define robot capabilities (action primitives) for the LLM\n          self.robot_capabilities = [\n              {\n                  "name": "navigate_to",\n                  "description": "Moves the robot to a specified location.",\n                  "parameters": {\n                      "type": "object",\n                      "properties": {\n                          "location": {"type": "string", "description": "The name of the location (e.g., \'kitchen\', \'table\')."}\n                      },\n                      "required": ["location"]\n                  }\n              },\n              {\n                  "name": "grasp_object",\n                  "description": "Instructs the robot to pick up a specific object.",\n                  "parameters": {\n                      "type": "object",\n                      "properties": {\n                          "object_name": {"type": "string", "description": "The name of the object to grasp (e.g., \'red apple\')."}\n                      },\n                      "required": ["object_name"]\n                  }\n              },\n              {\n                  "name": "place_object",\n                  "description": "Instructs the robot to place the currently held object at a specified location.",\n                  "parameters": {\n                      "type": "object",\n                      "properties": {\n                          "location": {"type": "string", "description": "The name of the location to place the object (e.g., \'counter\', \'bin\')."}\n                      },\n                      "required": ["location"]\n                  }\n              },\n               {\n                  "name": "deliver_object_to_person",\n                  "description": "Instructs the robot to deliver the held object to a person at a specified location.",\n                  "parameters": {\n                      "type": "object",\n                      "properties": {\n                          "person_location": {"type": "string", "description": "The location where the person is (e.g., \'living room\', \'my side\')."}\n                      },\n                      "required": ["person_location"]\n                  }\n              },\n              {\n                  "name": "report_status",\n                  "description": "The robot reports its current status or completion of a task.",\n                  "parameters": {\n                      "type": "object",\n                      "properties": {\n                          "message": {"type": "string", "description": "The status message to report."}\n                      },\n                      "required": ["message"]\n                  }\n              }\n          ]\n\n      self.system_prompt = f"""You are a helpful humanoid robot assistant. Your task is to translate human natural language commands into a sequence of robot actions.\nYou have the following tools available:\n{json.dumps(self.robot_capabilities, indent=2)}\n\nPlease respond with a JSON array of tool calls. Each element in the array should be a tool call object with \'name\' and \'parameters\'.\nExample:\n[\n{{\' \'}}"name": "navigate_to", "parameters": {{"location": "kitchen"}}{{\' \'}}},\n{{\' \'}}"name": "grasp_object", "parameters": {{"object_name": "red apple"}}{{\' \'}}},\n{{\' \'}}"name": "report_status", "parameters": {{"message": "Task complete!"}}{{\' \'}}}\n]\n\nBreak down complex tasks into logical, sequential steps. Be concise and use only the provided tool functions. If a command is unclear or requires more information, ask clarifying questions using the report_status tool (e.g., {{\' \'}}"name": "report_status", "parameters": {{"message": "Please specify which object to pick up."}}{{\' \'}}}).\nAssume objects are visible and reachable unless explicitly stated otherwise.\n"""\n  def speech_callback(self, msg: String):\n      command = msg.data\n      self.get_logger().info(f\'Received voice command: "{command}"\')\n      # Construct the messages for the LLM\n      messages = [\n          {"role": "system", "content": self.system_prompt},\n          {"role": "user", "content": command}\n      ]\n\n      try:\n          # Call the LLM API\n          # For OpenAI with function calling\n          response = self.client.chat.completions.create(\n              model="gpt-4o", # Use a capable model\n              messages=messages,\n              tools=self.robot_capabilities,\n              tool_choice="auto", # Let the model decide if it needs to use a tool\n              temperature=0.0 # For more deterministic output\n          )\n\n          response_message = response.choices[0].message\n          tool_calls = response_message.tool_calls\n\n          if tool_calls:\n              plan_steps = []\n              for tool_call in tool_calls:\n                  plan_step = {\n                      "name": tool_call.function.name,\n                      "parameters": json.loads(tool_call.function.arguments)\n                  }\n                  plan_steps.append(plan_step)\n              \n              plan_json = json.dumps(plan_steps, indent=2)\n              self.get_logger().info(f"Generated plan:\n{plan_json}")\n              \n              # Publish the structured plan\n              plan_msg = String()\n              plan_msg.data = plan_json\n              self.action_publisher_.publish(plan_msg)\n          else:\n              self.get_logger().warn(f"LLM did not generate tool calls for command: {command}. Response: {response_message.content}")\n              # You might publish a "report_status" message here asking for clarification\n\n      except openai.APIError as e:\n          self.get_logger().error(f"OpenAI API Error: {e}")\n      except Exception as e:\n          self.get_logger().error(f"An unexpected error occurred: {e}")\n\n  def destroy_node(self):\n      super().destroy_node()\n\ndef main(args=None):\n  rclpy.init(args=args)\n  llm_planner_node = LLMRobotPlanner()\n  rclpy.spin(llm_planner_node)\n  llm_planner_node.destroy_node()\n  rclpy.shutdown()\n\nif __name__ == \'__main__\':\n  main()\n',"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"5.  **Edit `setup.py` for `llm_robot_planner`:** Add the entry point.\r\n```python\r\nfrom setuptools import find_packages, setup\r\n\r\npackage_name = 'llm_robot_planner'\r\n\r\nsetup(\r\n    name=package_name,\r\n    version='0.0.0',\r\n    packages=find_packages(exclude=['test']),\r\n    data_files=[\r\n        ('share/' + package_name, ['package.xml']),\r\n    ],\r\n    install_requires=['setuptools', 'openai'], # Add openai\r\n    zip_safe=True,\r\n    maintainer='your_name',\r\n    maintainer_email='your_email@example.com',\r\n    description='ROS 2 package for LLM-based robot planning.',\r\n    license='Apache-2.0',\r\n    tests_require=['pytest'],\r\n    entry_points={\r\n        'console_scripts': [\r\n            'llm_planner = llm_robot_planner.llm_planner_node:main',\r\n        ],\r\n    },\r\n)\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"6",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build your package and source your workspace:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>\r\ncolcon build --packages-select llm_robot_planner\r\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Run the Whisper ASR node (from Lab 4.2.1) in one terminal:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run whisper_ros_asr whisper_node\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"In a separate terminal, run the LLM Robot Planner node:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run llm_robot_planner llm_planner\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speak a command into your microphone"}),' (e.g., "Robot, please go to the kitchen, pick up the red apple, and bring it to me here.").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Monitor the ",(0,s.jsx)(n.code,{children:"/robot_action_plan"})," topic:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /robot_action_plan\n"})}),"\n","You should see the LLM's generated JSON plan, breaking down your command into a sequence of ",(0,s.jsx)(n.code,{children:"navigate_to"}),", ",(0,s.jsx)(n.code,{children:"grasp_object"}),", and ",(0,s.jsx)(n.code,{children:"deliver_object_to_person"})," calls."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"expected-output",children:"Expected Output"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A functional ROS 2 node that subscribes to text commands."}),"\n",(0,s.jsx)(n.li,{children:"Successful interaction with an external LLM API (e.g., OpenAI)."}),"\n",(0,s.jsx)(n.li,{children:"The LLM generating a structured, robot-executable plan (sequence of function calls) in JSON format."}),"\n",(0,s.jsxs)(n.li,{children:["The generated plan published to a ROS 2 topic (",(0,s.jsx)(n.code,{children:"/robot_action_plan"}),")."]}),"\n",(0,s.jsx)(n.li,{children:"Understanding of prompt engineering to guide LLM planning for robotics."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'How does an LLM contribute to "cognitive planning" for a robot, going beyond simple natural language understanding?'}),"\n",(0,s.jsx)(n.li,{children:'Describe the process of "function calling" or "tool use" in LLMs and explain its significance for bridging natural language commands with robot action primitives.'}),"\n",(0,s.jsxs)(n.li,{children:["Design a prompt for an LLM that would enable a humanoid robot to assist in a simple cooking task, providing the LLM with relevant action primitives (e.g., ",(0,s.jsx)(n.code,{children:"cut_ingredient"}),", ",(0,s.jsx)(n.code,{children:"add_to_pot"}),", ",(0,s.jsx)(n.code,{children:"stir"}),")."]}),"\n",(0,s.jsx)(n.li,{children:"What are the security implications of integrating an external LLM API into a robotic system, especially one operating in a physical environment?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"real-world-applications",children:"Real-world Applications"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complex Task Automation:"})," Humanoid robots performing multi-step household chores or industrial assembly tasks from high-level natural language instructions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-Robot Teaming:"})," Robots acting as intelligent assistants, understanding ambiguous human requests and generating flexible plans to achieve shared goals."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adaptive Mission Planning:"})," Robots autonomously generating and adjusting their plans in dynamic, uncertain environments based on verbal updates or changing objectives."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robotics in Education and Training:"})," Simplifying the programming of complex robot behaviors by allowing students to define tasks in natural language."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"edge-cases",children:"Edge Cases"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM Hallucinations:"})," The LLM might generate actions or parameters that are nonsensical or do not align with the robot's capabilities."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Window Limitations:"})," For very long or highly complex tasks, the LLM's context window might be exceeded, leading to incomplete or flawed plans."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API Rate Limits and Cost:"})," Frequent API calls to powerful LLMs can incur significant costs and hit rate limits, requiring careful management."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Misinterpretation of State:"})," If the LLM's understanding of the robot's current state or environment is outdated or incorrect, it can generate an unexecutable plan."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"key-entities",children:(0,s.jsx)(n.strong,{children:"Key Entities"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large Language Model (LLM):"})," An advanced AI model capable of processing, understanding, and generating human language, used here for interpreting complex commands and generating action plans."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cognitive Planning:"})," The high-level reasoning process that transforms abstract goals (derived from natural language) into a concrete, sequential plan of robot actions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Task Decomposition:"})," The ability of an LLM to break down a single, complex natural language command into a series of smaller, more manageable sub-tasks or action primitives."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Prompt Engineering:"})," The technique of carefully designing the input (prompt) to an LLM, including instructions, examples, and definitions of available tools, to guide it toward generating a desired output format and content."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Function Calling / Tool Use:"}),' A feature of modern LLMs that allows them to generate structured calls to predefined functions (or "tools") based on user prompts, which is critical for converting natural language into robot-executable commands.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Primitive:"})," A discrete, low-level operation that a robot is physically capable of executing (e.g., ",(0,s.jsx)(n.code,{children:"navigate_to"}),", ",(0,s.jsx)(n.code,{children:"grasp_object"}),"), forming the building blocks of an LLM-generated plan."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API Key:"})," A secret token used to authenticate requests to external LLM services, ensuring secure access and billing."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"std_msgs/msg/String"}),":"]})," A basic ROS 2 message type suitable for transmitting raw text commands from ASR and structured JSON plans from the LLM."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"references",children:(0,s.jsx)(n.strong,{children:"References"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Huang, K., et al. (2022). Inner Monologue: Empowering Large Language Models to Reason about Physical Interactions. ",(0,s.jsx)(n.em,{children:"arXiv preprint arXiv:2207.05697"}),". (Placeholder citation)"]}),"\n",(0,s.jsxs)(n.li,{children:["OpenAI. (n.d.). ",(0,s.jsx)(n.em,{children:"Function calling and other API updates"}),". (Placeholder citation)"]}),"\n",(0,s.jsxs)(n.li,{children:["Ahn, L., et al. (2022). SayCan: Learning Language Grounded Robotic Skills from Natural Language Instructions. ",(0,s.jsx)(n.em,{children:"Conference on Robot Learning (CoRL)"}),". (Placeholder citation)"]}),"\n",(0,s.jsxs)(n.li,{children:["Wang, X., et al. (2023). Voyager: An Open-Ended Embodied Agent with Large Language Models. ",(0,s.jsx)(n.em,{children:"arXiv preprint arXiv:2305.16291"}),". (Placeholder citation)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);