{"allContent":{"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/Book-Hackathon-physical-ai-humanoid/","tagsPath":"/Book-Hackathon-physical-ai-humanoid/tags","isLast":true,"routePriority":-1,"contentPath":"D:\\mk\\giaic\\Book-Hackathon-physical-ai-humanoid\\humanoid-robotics-book\\docs","docs":[{"id":"appendices/appendices","title":"7.1 Appendices - Resources and Further Reading","description":"The Appendices serve as a comprehensive resource, consolidating crucial supplementary information that enhances the reader's understanding and practical application of the concepts covered in this book. This section provides detailed installation guides, common troubleshooting tips, a glossary of key terms, and a comprehensive list of academic and technical references, all formatted according to APA style.","source":"@site/docs/appendices/index.mdx","sourceDirName":"appendices","slug":"/appendices/","permalink":"/Book-Hackathon-physical-ai-humanoid/appendices/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"appendices","title":"7.1 Appendices - Resources and Further Reading"},"sidebar":"defaultSidebar","previous":{"title":"Technical Plan: Physical AI & Humanoid Robotics Project","permalink":"/Book-Hackathon-physical-ai-humanoid/misc/technical-plan"},"next":{"title":"5.1 Capstone - The Integrated Humanoid Robotics System","permalink":"/Book-Hackathon-physical-ai-humanoid/capstone/capstone-integration-guide"}},{"id":"capstone/capstone-integration-guide","title":"5.1 Capstone - The Integrated Humanoid Robotics System","description":"Congratulations on making it to the Capstone module! Throughout this book, you've journeyed through the intricate landscape of humanoid robotics, mastering the Robotic Nervous System (ROS 2), building Digital Twins in Gazebo and Unity, empowering the AI-Robot Brain with NVIDIA Isaac Sim and VSLAM, and developing Vision-Language-Action (VLA) capabilities. This Capstone is designed to unify all these disparate concepts and components into a single, cohesive, and functional humanoid robotics system. It is here that theory truly meets practice, and individual modules culminate in an autonomous, intelligent agent ready to respond to its world.","source":"@site/docs/capstone/integration-guide.mdx","sourceDirName":"capstone","slug":"/capstone/capstone-integration-guide","permalink":"/Book-Hackathon-physical-ai-humanoid/capstone/capstone-integration-guide","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"capstone-integration-guide","title":"5.1 Capstone - The Integrated Humanoid Robotics System"},"sidebar":"defaultSidebar","previous":{"title":"7.1 Appendices - Resources and Further Reading","permalink":"/Book-Hackathon-physical-ai-humanoid/appendices/"},"next":{"title":"Getting Started - Setting Up Your Environment","permalink":"/Book-Hackathon-physical-ai-humanoid/getting-started"}},{"id":"getting-started","title":"Getting Started - Setting Up Your Environment","description":"This section provides essential guidance for setting up your development environment, ensuring you have all the necessary tools and configurations to follow along with the book's practical exercises. A well-prepared environment is crucial for a smooth learning experience in the complex domain of humanoid robotics.","source":"@site/docs/getting-started.mdx","sourceDirName":".","slug":"/getting-started","permalink":"/Book-Hackathon-physical-ai-humanoid/getting-started","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"getting-started","title":"Getting Started - Setting Up Your Environment"},"sidebar":"defaultSidebar","previous":{"title":"5.1 Capstone - The Integrated Humanoid Robotics System","permalink":"/Book-Hackathon-physical-ai-humanoid/capstone/capstone-integration-guide"},"next":{"title":"6.1 Hardware & Lab Setup - Beyond Simulation","permalink":"/Book-Hackathon-physical-ai-humanoid/hardware-lab-setup/"}},{"id":"hardware-lab-setup/hardware-lab-setup","title":"6.1 Hardware & Lab Setup - Beyond Simulation","description":"While the previous modules have extensively leveraged the power of simulation for development and testing, the ultimate goal of humanoid robotics is deployment on physical hardware. This chapter bridges the gap between the virtual and real worlds, providing guidance on setting up a physical lab environment, understanding essential hardware components, and safely deploying the intelligence developed in simulation onto a real humanoid robot. Transitioning from simulation to reality presents unique challenges and demands careful consideration of safety, calibration, and hardware-specific configurations.","source":"@site/docs/hardware-lab-setup/index.mdx","sourceDirName":"hardware-lab-setup","slug":"/hardware-lab-setup/","permalink":"/Book-Hackathon-physical-ai-humanoid/hardware-lab-setup/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"hardware-lab-setup","title":"6.1 Hardware & Lab Setup - Beyond Simulation"},"sidebar":"defaultSidebar","previous":{"title":"Getting Started - Setting Up Your Environment","permalink":"/Book-Hackathon-physical-ai-humanoid/getting-started"},"next":{"title":"Overview - Physical AI & Humanoid Robotics","permalink":"/Book-Hackathon-physical-ai-humanoid/home/overview"}},{"id":"home/overview","title":"Overview - Physical AI & Humanoid Robotics","description":"Welcome to the comprehensive guide on Physical AI and Humanoid Robotics. This book is designed for aspiring roboticists, engineers, and researchers who wish to delve into the intricate world of intelligent humanoid systems. Our journey will cover the foundational principles of robotic control, advanced simulation techniques, cutting-edge AI integration for perception and cognition, and sophisticated vision-language-action pipelines that enable humanoids to interact intelligently with their environment.","source":"@site/docs/home/overview.mdx","sourceDirName":"home","slug":"/home/overview","permalink":"/Book-Hackathon-physical-ai-humanoid/home/overview","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"overview","title":"Overview - Physical AI & Humanoid Robotics"},"sidebar":"defaultSidebar","previous":{"title":"6.1 Hardware & Lab Setup - Beyond Simulation","permalink":"/Book-Hackathon-physical-ai-humanoid/hardware-lab-setup/"},"next":{"title":"Welcome to the Humanoid Robotics Book","permalink":"/Book-Hackathon-physical-ai-humanoid/"}},{"id":"index","title":"Welcome to the Humanoid Robotics Book","description":"This is the main introduction to the Humanoid Robotics Book.","source":"@site/docs/index.md","sourceDirName":".","slug":"/","permalink":"/Book-Hackathon-physical-ai-humanoid/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Overview - Physical AI & Humanoid Robotics","permalink":"/Book-Hackathon-physical-ai-humanoid/home/overview"}},{"id":"misc/technical-plan","title":"Technical Plan: Physical AI & Humanoid Robotics Project","description":"This document outlines the technical plan for the Physical AI & Humanoid Robotics Project, structured according to the 4-module specifications and including plans for documentation build and deployment using Docusaurus and GitHub Pages. This plan adheres to technical, actionable, and Spec-Kit Plus workflows, with APA citations where research is referenced.","source":"@site/docs/misc/technical-plan.md","sourceDirName":"misc","slug":"/misc/technical-plan","permalink":"/Book-Hackathon-physical-ai-humanoid/misc/technical-plan","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Miscellaneous","permalink":"/Book-Hackathon-physical-ai-humanoid/category/miscellaneous"},"next":{"title":"7.1 Appendices - Resources and Further Reading","permalink":"/Book-Hackathon-physical-ai-humanoid/appendices/"}},{"id":"module-1/building-python-nodes","title":"1.2 Building Python Nodes with rclpy","description":"Following our exploration of ROS 2's foundational architecture, this chapter shifts focus to practical implementation. We will learn how to develop functional ROS 2 nodes using rclpy, the Python client library for ROS 2. Building custom nodes is essential for extending robotic capabilities, integrating new sensors, and implementing control algorithms.","source":"@site/docs/module-1/02-building-python-nodes.mdx","sourceDirName":"module-1","slug":"/module-1/building-python-nodes","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/building-python-nodes","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"building-python-nodes","title":"1.2 Building Python Nodes with rclpy"},"sidebar":"defaultSidebar","previous":{"title":"1.1 ROS 2 Architecture & DDS - The Robotic Nervous System","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/ros2-architecture"},"next":{"title":"1.3 URDF for Humanoid Robots","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/urdf-humanoid-robots"}},{"id":"module-1/launch-files-multi-node","title":"1.5 Launch Files & Multi-node Systems","description":"As robotic systems grow in complexity, managing individual ROS 2 nodes manually becomes impractical. Imagine starting dozens of nodes, each with specific parameters and dependencies, across multiple terminals – it would be a logistical nightmare. This is where ROS 2 Launch Files come in. They provide a powerful, programmatic way to define, configure, and orchestrate multiple ROS 2 nodes, processes, and even entire systems, allowing for efficient startup and management of complex humanoid robot applications.","source":"@site/docs/module-1/05-launch-files-multi-node.mdx","sourceDirName":"module-1","slug":"/module-1/launch-files-multi-node","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/launch-files-multi-node","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"launch-files-multi-node","title":"1.5 Launch Files & Multi-node Systems"},"sidebar":"defaultSidebar","previous":{"title":"1.4 TF2: Frames & Transformations","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/tf2-frames-transformations"},"next":{"title":"Module 2: Humanoid Robot Kinematics","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-2-humanoid-robot-kinematics"}},{"id":"module-1/ros2-architecture","title":"1.1 ROS 2 Architecture & DDS - The Robotic Nervous System","description":"Welcome to Module 1: The Robotic Nervous System (ROS 2). In this chapter, we delve into the fundamental architecture of ROS 2, understanding how it forms the communication backbone for complex robotic systems, especially humanoid robots. We will explore the core concepts of the Data Distribution Service (DDS) and how it facilitates real-time, decentralized communication between various components.","source":"@site/docs/module-1/01-ros2-architecture.mdx","sourceDirName":"module-1","slug":"/module-1/ros2-architecture","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/ros2-architecture","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"ros2-architecture","title":"1.1 ROS 2 Architecture & DDS - The Robotic Nervous System"},"sidebar":"defaultSidebar","previous":{"title":"Module 1: The Robotic Nervous System (ROS 2)","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-1-the-robotic-nervous-system-ros-2"},"next":{"title":"1.2 Building Python Nodes with rclpy","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/building-python-nodes"}},{"id":"module-1/tf2-frames-transformations","title":"1.4 TF2: Frames & Transformations","description":"Building upon our understanding of URDF for describing a robot's physical structure, this chapter introduces TF2 – the ROS 2 framework for managing coordinate frames and transformations. In robotics, especially with complex humanoid systems, understanding where the robot is in its environment, where its sensors are relative to its body, and how its end-effectors move, all depend on accurate spatial relationships. TF2 provides a powerful, standardized way to keep track of these relationships over time.","source":"@site/docs/module-1/04-tf2-frames-transformations.mdx","sourceDirName":"module-1","slug":"/module-1/tf2-frames-transformations","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/tf2-frames-transformations","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"tf2-frames-transformations","title":"1.4 TF2: Frames & Transformations"},"sidebar":"defaultSidebar","previous":{"title":"1.3 URDF for Humanoid Robots","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/urdf-humanoid-robots"},"next":{"title":"1.5 Launch Files & Multi-node Systems","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/launch-files-multi-node"}},{"id":"module-1/urdf-humanoid-robots","title":"1.3 URDF for Humanoid Robots","description":"With a grasp of ROS 2's communication paradigms and the ability to craft Python nodes, the next crucial step in humanoid robotics is understanding how to describe the robot itself. The Unified Robot Description Format (URDF) provides a standardized XML format for modeling a robot's physical structure, kinematics, and visual properties. This chapter will guide you through creating and interpreting URDF files specifically tailored for humanoid robots, bridging the gap between mechanical design and software control.","source":"@site/docs/module-1/03-urdf-humanoid-robots.mdx","sourceDirName":"module-1","slug":"/module-1/urdf-humanoid-robots","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/urdf-humanoid-robots","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"urdf-humanoid-robots","title":"1.3 URDF for Humanoid Robots"},"sidebar":"defaultSidebar","previous":{"title":"1.2 Building Python Nodes with rclpy","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/building-python-nodes"},"next":{"title":"1.4 TF2: Frames & Transformations","permalink":"/Book-Hackathon-physical-ai-humanoid/module-1/tf2-frames-transformations"}},{"id":"module-2/building-gazebo-worlds","title":"2.2 Building Gazebo Worlds","description":"Having understood the theoretical underpinnings of digital twins, we now transition to practical implementation using Gazebo, a widely adopted and powerful 3D robot simulator. Gazebo provides a robust physics engine and rich graphical environment, making it an ideal platform for creating physics-accurate simulation worlds where humanoid robots can be tested and developed safely and efficiently. This chapter will guide you through the process of constructing custom Gazebo worlds, laying the foundation for realistic robot simulations.","source":"@site/docs/module-2/02-building-gazebo-worlds.mdx","sourceDirName":"module-2","slug":"/module-2/building-gazebo-worlds","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/building-gazebo-worlds","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"building-gazebo-worlds","title":"2.2 Building Gazebo Worlds"},"sidebar":"defaultSidebar","previous":{"title":"2.1 What Is a Digital Twin?","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/what-is-digital-twin"},"next":{"title":"2.3 Physics Simulation (Gravity, Rigid Bodies, Collisions)","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/physics-simulation"}},{"id":"module-2/physics-simulation","title":"2.3 Physics Simulation (Gravity, Rigid Bodies, Collisions)","description":"Having set the stage by building our Gazebo world, we now dive into the core of realistic simulation: the physics engine. For humanoid robots, accurate physics simulation is paramount, as it dictates how the robot interacts with its environment, maintains balance, and executes dynamic movements. This chapter will explore how Gazebo models fundamental physical phenomena like gravity, rigid-body dynamics, and collisions, enabling you to create truly physics-accurate simulations for your humanoid systems.","source":"@site/docs/module-2/03-physics-simulation.mdx","sourceDirName":"module-2","slug":"/module-2/physics-simulation","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/physics-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"physics-simulation","title":"2.3 Physics Simulation (Gravity, Rigid Bodies, Collisions)"},"sidebar":"defaultSidebar","previous":{"title":"2.2 Building Gazebo Worlds","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/building-gazebo-worlds"},"next":{"title":"2.4 Sensor Simulation (LiDAR, Depth, IMU)","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/sensor-simulation"}},{"id":"module-2/sensor-simulation","title":"2.4 Sensor Simulation (LiDAR, Depth, IMU)","description":"With a physics-accurate environment established in Gazebo, the next crucial step for developing intelligent humanoid robots is to equip them with virtual senses. Just as in the real world, robots rely heavily on sensor data to perceive their surroundings, localize themselves, and make informed decisions. This chapter focuses on simulating essential robotics sensors—LiDAR, IMU, and RGB-D cameras—within Gazebo, enabling you to generate realistic data streams for perception algorithms and control systems.","source":"@site/docs/module-2/04-sensor-simulation.mdx","sourceDirName":"module-2","slug":"/module-2/sensor-simulation","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/sensor-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"sensor-simulation","title":"2.4 Sensor Simulation (LiDAR, Depth, IMU)"},"sidebar":"defaultSidebar","previous":{"title":"2.3 Physics Simulation (Gravity, Rigid Bodies, Collisions)","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/physics-simulation"},"next":{"title":"2.5 Unity for High-Fidelity Robotics","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/unity-for-robotics"}},{"id":"module-2/unity-for-robotics","title":"2.5 Unity for High-Fidelity Robotics","description":"While Gazebo excels in physics-accurate simulation, especially for ROS 2 integrated robots, there are scenarios where visual fidelity, complex scene interactions, and advanced Human-Robot Interaction (HRI) prototyping are paramount. This is where Unity, a leading real-time 3D development platform, offers distinct advantages. This chapter explores how Unity can be leveraged to create high-fidelity HRI scenes and visually rich environments for humanoid robotics, often complementing Gazebo or Isaac Sim for specific use cases.","source":"@site/docs/module-2/05-unity-for-robotics.mdx","sourceDirName":"module-2","slug":"/module-2/unity-for-robotics","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/unity-for-robotics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"unity-for-robotics","title":"2.5 Unity for High-Fidelity Robotics"},"sidebar":"defaultSidebar","previous":{"title":"2.4 Sensor Simulation (LiDAR, Depth, IMU)","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/sensor-simulation"},"next":{"title":"Module 3: Humanoid Robot Dynamics","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-3-humanoid-robot-dynamics"}},{"id":"module-2/what-is-digital-twin","title":"2.1 What Is a Digital Twin?","description":"Welcome to Module 2: The Digital Twin (Gazebo & Unity). In the evolving landscape of robotics, particularly with complex humanoid systems, the concept of a \"digital twin\" has become indispensable. This chapter introduces the core idea of a digital twin, its significance in robot development and deployment, and how it serves as a critical bridge between the virtual and physical worlds. Understanding digital twins is fundamental to building, testing, and optimizing humanoid robots efficiently and safely.","source":"@site/docs/module-2/01-what-is-digital-twin.mdx","sourceDirName":"module-2","slug":"/module-2/what-is-digital-twin","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/what-is-digital-twin","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"what-is-digital-twin","title":"2.1 What Is a Digital Twin?"},"sidebar":"defaultSidebar","previous":{"title":"Module 2: Humanoid Robot Kinematics","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-2-humanoid-robot-kinematics"},"next":{"title":"2.2 Building Gazebo Worlds","permalink":"/Book-Hackathon-physical-ai-humanoid/module-2/building-gazebo-worlds"}},{"id":"module-3/introduction-isaac-sim","title":"3.1 Introduction to NVIDIA Isaac Sim","description":"Welcome to Module 3: The AI-Robot Brain (NVIDIA Isaac). Having explored general-purpose simulation with Gazebo and high-fidelity visualization with Unity, we now turn our attention to NVIDIA Isaac Sim – a powerful, GPU-accelerated robotics simulation platform built on NVIDIA Omniverse. Isaac Sim is specifically designed to accelerate the development, testing, and deployment of AI-powered robots, offering photorealistic rendering, advanced sensor simulation, and robust synthetic data generation capabilities crucial for training deep learning models for humanoid robots.","source":"@site/docs/module-3/01-introduction-isaac-sim.mdx","sourceDirName":"module-3","slug":"/module-3/introduction-isaac-sim","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/introduction-isaac-sim","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"introduction-isaac-sim","title":"3.1 Introduction to NVIDIA Isaac Sim"},"sidebar":"defaultSidebar","previous":{"title":"Module 3: Humanoid Robot Dynamics","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-3-humanoid-robot-dynamics"},"next":{"title":"3.2 Synthetic Data & Domain Randomization","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/synthetic-data-domain-randomization"}},{"id":"module-3/navigation-nav2-humanoids","title":"3.4 Navigation with Nav2 for Humanoids","description":"Having equipped our humanoid robot with the ability to perceive its environment and localize itself through VSLAM, the next logical step is to enable it to move autonomously. This chapter focuses on navigation, specifically using Nav2 – the powerful ROS 2 navigation stack – tailored for humanoid robots. Nav2 provides a robust framework for global and local path planning, obstacle avoidance, and executive control, allowing simulated humanoid robots to navigate complex environments intelligently and safely.","source":"@site/docs/module-3/04-navigation-nav2-humanoids.mdx","sourceDirName":"module-3","slug":"/module-3/navigation-nav2-humanoids","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/navigation-nav2-humanoids","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"navigation-nav2-humanoids","title":"3.4 Navigation with Nav2 for Humanoids"},"sidebar":"defaultSidebar","previous":{"title":"3.3 Perception with Isaac ROS (VSLAM)","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/perception-isaac-ros-vslam"},"next":{"title":"3.5 Training Robot Policies","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/training-robot-policies"}},{"id":"module-3/perception-isaac-ros-vslam","title":"3.3 Perception with Isaac ROS (VSLAM)","description":"With the ability to create photorealistic simulations and generate synthetic data in Isaac Sim, we now focus on endowing our humanoid robots with the critical sense of sight and self-awareness within their environment. This chapter delves into perception, specifically Visual Simultaneous Localization and Mapping (VSLAM), utilizing NVIDIA's hardware-accelerated Isaac ROS GEMs. VSLAM allows a robot to build a map of an unknown environment while simultaneously estimating its own pose within that map using visual input, a cornerstone for autonomous navigation and interaction.","source":"@site/docs/module-3/03-perception-isaac-ros-vslam.mdx","sourceDirName":"module-3","slug":"/module-3/perception-isaac-ros-vslam","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/perception-isaac-ros-vslam","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"perception-isaac-ros-vslam","title":"3.3 Perception with Isaac ROS (VSLAM)"},"sidebar":"defaultSidebar","previous":{"title":"3.2 Synthetic Data & Domain Randomization","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/synthetic-data-domain-randomization"},"next":{"title":"3.4 Navigation with Nav2 for Humanoids","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/navigation-nav2-humanoids"}},{"id":"module-3/synthetic-data-domain-randomization","title":"3.2 Synthetic Data & Domain Randomization","description":"In the previous chapter, we introduced NVIDIA Isaac Sim as a powerful platform for humanoid robotics. One of its most compelling features is its capability for Synthetic Data Generation (SDG). Training robust AI models, especially for computer vision tasks in robotics, often requires massive amounts of diverse, labeled data. Acquiring and annotating such data from the real world is incredibly time-consuming, expensive, and sometimes dangerous. This chapter explores how Isaac Sim addresses this challenge by generating synthetic data and employs a technique called domain randomization to bridge the gap between simulation and reality.","source":"@site/docs/module-3/02-synthetic-data-domain-randomization.mdx","sourceDirName":"module-3","slug":"/module-3/synthetic-data-domain-randomization","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/synthetic-data-domain-randomization","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"synthetic-data-domain-randomization","title":"3.2 Synthetic Data & Domain Randomization"},"sidebar":"defaultSidebar","previous":{"title":"3.1 Introduction to NVIDIA Isaac Sim","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/introduction-isaac-sim"},"next":{"title":"3.3 Perception with Isaac ROS (VSLAM)","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/perception-isaac-ros-vslam"}},{"id":"module-3/training-robot-policies","title":"3.5 Training Robot Policies","description":"We've explored how Isaac Sim provides photorealistic environments, how synthetic data and domain randomization bridge the sim-to-real gap, and how our humanoid can perceive and navigate its world. Now, we tackle the ultimate goal: enabling the robot to learn complex behaviors and make intelligent decisions. This chapter delves into the methodologies for training robotic policies, primarily through reinforcement learning (RL), leveraging the power of Isaac Sim for efficient and safe simulated training environments.","source":"@site/docs/module-3/05-training-robot-policies.mdx","sourceDirName":"module-3","slug":"/module-3/training-robot-policies","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/training-robot-policies","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"training-robot-policies","title":"3.5 Training Robot Policies"},"sidebar":"defaultSidebar","previous":{"title":"3.4 Navigation with Nav2 for Humanoids","permalink":"/Book-Hackathon-physical-ai-humanoid/module-3/navigation-nav2-humanoids"},"next":{"title":"Module 4: Humanoid Robot Control","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-4-humanoid-robot-control"}},{"id":"module-4/full-vla-pipeline","title":"4.5 Full Voice-to-Action Integration Pipeline","description":"Throughout Module 4, we have built the individual components of a Vision-Language-Action (VLA) system: transcribing voice commands with Whisper, planning robot actions with LLMs, and visually grounding objects with computer vision. This final chapter of the module brings all these pieces together to construct a complete, end-to-end VLA pipeline. Our goal is to demonstrate how a humanoid robot can interpret a spoken instruction, generate an intelligent plan, perceive the necessary objects, and execute a multi-step task autonomously, showcasing the true power of embodied AI.","source":"@site/docs/module-4/05-full-vla-pipeline.mdx","sourceDirName":"module-4","slug":"/module-4/full-vla-pipeline","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/full-vla-pipeline","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"full-vla-pipeline","title":"4.5 Full Voice-to-Action Integration Pipeline"},"sidebar":"defaultSidebar","previous":{"title":"4.4 Visual Object Grounding","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/visual-object-grounding"},"next":{"title":"Miscellaneous","permalink":"/Book-Hackathon-physical-ai-humanoid/category/miscellaneous"}},{"id":"module-4/llm-cognitive-planning","title":"4.3 LLM-Based Cognitive Planning","description":"With the ability to accurately transcribe human voice commands into text, the next critical component of a Vision-Language-Action (VLA) system for humanoid robots is to interpret these commands and translate them into a coherent sequence of robot-executable actions. This chapter delves into the exciting realm of LLM-based cognitive planning, where Large Language Models (LLMs) are leveraged not just for understanding human language, but for complex symbolic reasoning, task decomposition, and generating high-level plans that guide a robot's behavior.","source":"@site/docs/module-4/03-llm-cognitive-planning.mdx","sourceDirName":"module-4","slug":"/module-4/llm-cognitive-planning","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/llm-cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"llm-cognitive-planning","title":"4.3 LLM-Based Cognitive Planning"},"sidebar":"defaultSidebar","previous":{"title":"4.2 Voice Command Systems with Whisper","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/voice-command-whisper"},"next":{"title":"4.4 Visual Object Grounding","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/visual-object-grounding"}},{"id":"module-4/visual-object-grounding","title":"4.4 Visual Object Grounding","description":"For a humanoid robot, receiving a natural language command and generating a high-level plan (as we saw in the previous chapter) is only half the battle. The robot must then connect the abstract concepts in the plan—like \"red apple\" or \"table\"—to concrete entities in its physical world. This crucial step is known as Visual Object Grounding. This chapter explores how computer vision techniques are applied to identify, localize, and semantically link objects mentioned in verbal commands to their visual representations in the robot's sensor data, enabling the robot to physically interact with its environment meaningfully.","source":"@site/docs/module-4/04-visual-object-grounding.mdx","sourceDirName":"module-4","slug":"/module-4/visual-object-grounding","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/visual-object-grounding","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"visual-object-grounding","title":"4.4 Visual Object Grounding"},"sidebar":"defaultSidebar","previous":{"title":"4.3 LLM-Based Cognitive Planning","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/llm-cognitive-planning"},"next":{"title":"4.5 Full Voice-to-Action Integration Pipeline","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/full-vla-pipeline"}},{"id":"module-4/voice-command-whisper","title":"4.2 Voice Command Systems with Whisper","description":"The journey into Vision-Language-Action (VLA) begins with the most natural form of human communication: spoken language. For a humanoid robot to truly understand and respond to human directives, it must first accurately transcribe spoken words into text. This chapter introduces voice command systems and focuses on OpenAI Whisper, a state-of-the-art Automatic Speech Recognition (ASR) model. We will explore how Whisper can be integrated into a robotic pipeline to reliably convert human speech into text commands, serving as the foundational input for intelligent robot planning and action.","source":"@site/docs/module-4/02-voice-command-whisper.mdx","sourceDirName":"module-4","slug":"/module-4/voice-command-whisper","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/voice-command-whisper","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"voice-command-whisper","title":"4.2 Voice Command Systems with Whisper"},"sidebar":"defaultSidebar","previous":{"title":"4.1 What Is Vision-Language-Action (VLA)?","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/what-is-vla"},"next":{"title":"4.3 LLM-Based Cognitive Planning","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/llm-cognitive-planning"}},{"id":"module-4/what-is-vla","title":"4.1 What Is Vision-Language-Action (VLA)?","description":"Welcome to Module 4: Vision-Language-Action (VLA). As we advance towards truly intelligent humanoid robots, merely sensing and moving is no longer sufficient. The next frontier involves seamless, intuitive interaction with humans and complex environments. This chapter introduces Vision-Language-Action (VLA) systems, a revolutionary paradigm that seeks to bridge the gap between human-like communication and robotic autonomy. VLA systems enable robots to understand natural language commands, interpret visual information from their surroundings, and translate these insights into intelligent physical actions, opening up new possibilities for human-robot collaboration and service.","source":"@site/docs/module-4/01-what-is-vla.mdx","sourceDirName":"module-4","slug":"/module-4/what-is-vla","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/what-is-vla","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"what-is-vla","title":"4.1 What Is Vision-Language-Action (VLA)?"},"sidebar":"defaultSidebar","previous":{"title":"Module 4: Humanoid Robot Control","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-4-humanoid-robot-control"},"next":{"title":"4.2 Voice Command Systems with Whisper","permalink":"/Book-Hackathon-physical-ai-humanoid/module-4/voice-command-whisper"}}],"drafts":[],"sidebars":{"defaultSidebar":[{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"module-1/ros2-architecture"},{"type":"doc","id":"module-1/building-python-nodes"},{"type":"doc","id":"module-1/urdf-humanoid-robots"},{"type":"doc","id":"module-1/tf2-frames-transformations"},{"type":"doc","id":"module-1/launch-files-multi-node"}],"link":{"type":"generated-index","description":"Understanding the core of robot communication and control with ROS 2.","slug":"/category/module-1-the-robotic-nervous-system-ros-2","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-1-the-robotic-nervous-system-ros-2"}},{"type":"category","label":"Module 2: Humanoid Robot Kinematics","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"module-2/what-is-digital-twin"},{"type":"doc","id":"module-2/building-gazebo-worlds"},{"type":"doc","id":"module-2/physics-simulation"},{"type":"doc","id":"module-2/sensor-simulation"},{"type":"doc","id":"module-2/unity-for-robotics"}],"link":{"type":"generated-index","description":"Understanding the motion and structure of humanoid robots.","slug":"/category/module-2-humanoid-robot-kinematics","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-2-humanoid-robot-kinematics"}},{"type":"category","label":"Module 3: Humanoid Robot Dynamics","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"module-3/introduction-isaac-sim"},{"type":"doc","id":"module-3/synthetic-data-domain-randomization"},{"type":"doc","id":"module-3/perception-isaac-ros-vslam"},{"type":"doc","id":"module-3/navigation-nav2-humanoids"},{"type":"doc","id":"module-3/training-robot-policies"}],"link":{"type":"generated-index","description":"Exploring the forces and motion of humanoid robots.","slug":"/category/module-3-humanoid-robot-dynamics","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-3-humanoid-robot-dynamics"}},{"type":"category","label":"Module 4: Humanoid Robot Control","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"module-4/what-is-vla"},{"type":"doc","id":"module-4/voice-command-whisper"},{"type":"doc","id":"module-4/llm-cognitive-planning"},{"type":"doc","id":"module-4/visual-object-grounding"},{"type":"doc","id":"module-4/full-vla-pipeline"}],"link":{"type":"generated-index","description":"Implementing control strategies for humanoid robots.","slug":"/category/module-4-humanoid-robot-control","permalink":"/Book-Hackathon-physical-ai-humanoid/category/module-4-humanoid-robot-control"}},{"type":"category","label":"Miscellaneous","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"misc/technical-plan"}],"link":{"type":"generated-index","description":"Additional documents and plans.","slug":"/category/miscellaneous","permalink":"/Book-Hackathon-physical-ai-humanoid/category/miscellaneous"}},{"type":"doc","id":"appendices/appendices","label":"7.1 Appendices - Resources and Further Reading"},{"type":"category","label":"capstone","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"capstone/capstone-integration-guide"}]},{"type":"doc","id":"getting-started"},{"type":"doc","id":"hardware-lab-setup/hardware-lab-setup","label":"6.1 Hardware & Lab Setup - Beyond Simulation"},{"type":"category","label":"home","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"home/overview"}]},{"type":"doc","id":"index"}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/Book-Hackathon-physical-ai-humanoid/blog/tags"}},"docusaurus-plugin-content-pages":{"default":null},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}