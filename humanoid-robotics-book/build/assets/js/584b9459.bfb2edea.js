"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[295],{1667:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>g,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/full-vla-pipeline","title":"4.5 Full Voice-to-Action Integration Pipeline","description":"Throughout Module 4, we have built the individual components of a Vision-Language-Action (VLA) system: transcribing voice commands with Whisper, planning robot actions with LLMs, and visually grounding objects with computer vision. This final chapter of the module brings all these pieces together to construct a complete, end-to-end VLA pipeline. Our goal is to demonstrate how a humanoid robot can interpret a spoken instruction, generate an intelligent plan, perceive the necessary objects, and execute a multi-step task autonomously, showcasing the true power of embodied AI.","source":"@site/docs/module-4/05-full-vla-pipeline.mdx","sourceDirName":"module-4","slug":"/module-4/full-vla-pipeline","permalink":"/module-4/full-vla-pipeline","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"full-vla-pipeline","title":"4.5 Full Voice-to-Action Integration Pipeline"},"sidebar":"defaultSidebar","previous":{"title":"4.4 Visual Object Grounding","permalink":"/module-4/visual-object-grounding"},"next":{"title":"Miscellaneous","permalink":"/category/miscellaneous"}}');var i=r(4848),s=r(8453);const o={id:"full-vla-pipeline",title:"4.5 Full Voice-to-Action Integration Pipeline"},a=void 0,l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Tools",id:"tools",level:2},{value:"Chapter Sections",id:"chapter-sections",level:2},{value:"4.5.1 The Integrated VLA Architecture",id:"451-the-integrated-vla-architecture",level:3},{value:"4.5.2 Designing the VLA Orchestrator Node",id:"452-designing-the-vla-orchestrator-node",level:3},{value:"4.5.3 Implementing Robot Action Servers (Navigation, Grasping, Placing)",id:"453-implementing-robot-action-servers-navigation-grasping-placing",level:3},{value:"4.5.4 Multi-Step Task Execution and Error Handling",id:"454-multi-step-task-execution-and-error-handling",level:3},{value:"4.5.5 Demonstration: Robot Picks Up an Object Based on Voice Command",id:"455-demonstration-robot-picks-up-an-object-based-on-voice-command",level:3},{value:"Required Diagrams",id:"required-diagrams",level:2},{value:"Hands-on Labs",id:"hands-on-labs",level:2},{value:"Lab 4.5.1: Execute VLA Pipeline: Robot Picks Up an Object",id:"lab-451-execute-vla-pipeline-robot-picks-up-an-object",level:3},{value:"Expected Output",id:"expected-output",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Real-world Applications",id:"real-world-applications",level:2},{value:"Edge Cases",id:"edge-cases",level:2},{value:"<strong>Key Entities</strong>",id:"key-entities",level:3},{value:"<strong>References</strong>",id:"references",level:3}];function d(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"Throughout Module 4, we have built the individual components of a Vision-Language-Action (VLA) system: transcribing voice commands with Whisper, planning robot actions with LLMs, and visually grounding objects with computer vision. This final chapter of the module brings all these pieces together to construct a complete, end-to-end VLA pipeline. Our goal is to demonstrate how a humanoid robot can interpret a spoken instruction, generate an intelligent plan, perceive the necessary objects, and execute a multi-step task autonomously, showcasing the true power of embodied AI."}),"\n",(0,i.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,i.jsx)(n.p,{children:"The goal of this chapter is to educate students on building a complete VLA pipeline (Voice \u2192 Plan \u2192 Perception \u2192 Action), integrating voice commands, LLM planning, computer vision, and ROS 2 Actions to enable autonomous task completion by humanoid robots."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand how to integrate all VLA components (ASR, LLM Planner, Object Grounding, Action Execution) into a single, cohesive ROS 2 system."}),"\n",(0,i.jsx)(n.li,{children:"Implement a central VLA orchestrator node that manages the flow of information and control between components."}),"\n",(0,i.jsx)(n.li,{children:"Utilize ROS 2 Actions for robust and long-running task execution."}),"\n",(0,i.jsx)(n.li,{children:"Develop error handling and feedback mechanisms for the VLA pipeline."}),"\n",(0,i.jsx)(n.li,{children:"Demonstrate a multi-step task completion by a simulated humanoid robot based on a spoken command."}),"\n",(0,i.jsx)(n.li,{children:"Critically evaluate the performance and limitations of the integrated VLA system."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A functional ROS 2 environment."}),"\n",(0,i.jsx)(n.li,{children:"Completion of all previous chapters in Module 4 (Whisper ASR, LLM Planning, Object Grounding)."}),"\n",(0,i.jsx)(n.li,{children:"A simulated humanoid robot in Isaac Sim/Gazebo capable of basic navigation and manipulation (from Modules 1, 2, and 3)."}),"\n",(0,i.jsx)(n.li,{children:"Familiarity with ROS 2 Actions and their implementation (conceptual or basic)."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VLA Pipeline:"})," The sequential and interconnected series of modules that transform a human's natural language command into physical robot actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Orchestrator Node:"})," A central ROS 2 node responsible for coordinating the activities of different VLA components, subscribing to outputs, and publishing inputs."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Actions:"})," A communication pattern in ROS 2 designed for long-running, goal-oriented tasks with periodic feedback and the ability to be cancelled. Ideal for executing complex VLA plans."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Server:"})," A ROS 2 node that provides an action interface, executing goals and providing feedback."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Client:"})," A ROS 2 node that sends goals to an action server and processes feedback/results."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition:"})," The breakdown of a high-level command into a series of smaller, actionable steps by the LLM planner."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Machine (Conceptual):"})," A way to manage the different stages of a complex task (e.g., listening, planning, navigating, searching, grasping, placing)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback Mechanism:"})," Providing the human or the LLM with updates on the robot's progress or encountered errors."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Humble:"})," The integration framework."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Whisper ASR Node:"})," From Lab 4.2.1 (",(0,i.jsx)(n.code,{children:"whisper_ros_asr"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Robot Planner Node:"})," From Lab 4.3.1 (",(0,i.jsx)(n.code,{children:"llm_robot_planner"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Detector 3D Node:"})," From Lab 4.4.1 (",(0,i.jsx)(n.code,{children:"object_detector_3d"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Custom ROS 2 Action Server:"})," For robot navigation and manipulation primitives."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac Sim/Gazebo:"})," The simulation environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RViz2:"})," For visualization."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"chapter-sections",children:"Chapter Sections"}),"\n",(0,i.jsx)(n.h3,{id:"451-the-integrated-vla-architecture",children:"4.5.1 The Integrated VLA Architecture"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Review of Components:"})," Whisper (Voice), LLM (Plan), YOLO/SAM (Vision), ROS 2 Actions (Action)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Flow:"})," Tracing a command from speech to robot movement."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Central Orchestration:"})," The role of a VLA manager node."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"452-designing-the-vla-orchestrator-node",children:"4.5.2 Designing the VLA Orchestrator Node"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Input Subscriptions:"})," Listening to ",(0,i.jsx)(n.code,{children:"/speech_to_text"})," (from Whisper) and ",(0,i.jsx)(n.code,{children:"/object_3d_location"})," (from detector)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Plan Dispatch:"})," Sending transcribed commands to the LLM planner node (e.g., via a service call or direct function call if integrated)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Client Implementation:"})," Connecting to various ROS 2 Action Servers for navigation, grasping, and other manipulation tasks."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Management:"})," Keeping track of the current task, sub-task, and robot state."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"453-implementing-robot-action-servers-navigation-grasping-placing",children:"4.5.3 Implementing Robot Action Servers (Navigation, Grasping, Placing)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation Action Server:"})," Accepts a target pose/location and uses Nav2 to guide the robot."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grasping Action Server:"})," Takes an object's 3D location and executes a grasping primitive."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Placing Action Server:"})," Takes a target location and releases a held object."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback and Result Handling:"})," Providing intermediate status and final outcome for each action."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"454-multi-step-task-execution-and-error-handling",children:"4.5.4 Multi-Step Task Execution and Error Handling"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Executing the LLM-Generated Plan:"})," Iterating through action primitives."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrating Visual Grounding:"})," Using the object detector to verify/find objects before grasping."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Handling Action Failures:"})," Strategies for retries, replanning (involving the LLM again), or reporting failure."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reporting Status:"})," Communicating progress and errors back to the human user."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"455-demonstration-robot-picks-up-an-object-based-on-voice-command",children:"4.5.5 Demonstration: Robot Picks Up an Object Based on Voice Command"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A practical example combining all modules."}),"\n",(0,i.jsx)(n.li,{children:'"Robot, please pick up the red block from the table and place it on the shelf."'}),"\n",(0,i.jsx)(n.li,{children:"Tracing the command through each VLA stage."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"required-diagrams",children:"Required Diagrams"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Full Voice \u2192 LLM \u2192 Plan \u2192 Action Pipeline:"})," A detailed diagram showing all nodes, topics, actions, and data flow from microphone to robot actuators, highlighting the orchestrator."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Action Server Execution Graph:"})," Illustrating how the VLA orchestrator acts as an action client to specialized action servers (navigation, manipulation)."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-labs",children:"Hands-on Labs"}),"\n",(0,i.jsx)(n.h3,{id:"lab-451-execute-vla-pipeline-robot-picks-up-an-object",children:"Lab 4.5.1: Execute VLA Pipeline: Robot Picks Up an Object"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Objective:"})," Integrate all previously developed components to enable a simulated humanoid robot to respond to a spoken instruction, autonomously identify and pick up an object, and optionally place it."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Prerequisites:"})," All previous labs in Module 4 completed and their nodes functional. A simulated humanoid robot in Isaac Sim/Gazebo that can respond to basic navigation commands (e.g., ",(0,i.jsx)(n.code,{children:"/cmd_vel"}),") and has a simulated gripper (even a simple one) that can be controlled by a ROS 2 Action Server for grasping. For this lab, we'll implement ",(0,i.jsx)(n.em,{children:"mock"})," action servers to focus on the VLA orchestration."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Instructions:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Ensure all individual VLA components are built and runnable:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Whisper ASR Node (",(0,i.jsx)(n.code,{children:"whisper_ros_asr"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["LLM Robot Planner Node (",(0,i.jsx)(n.code,{children:"llm_robot_planner"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["Object Detector 3D Node (",(0,i.jsx)(n.code,{children:"object_detector_3d"}),")"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Create Mock ROS 2 Action Servers for Navigation and Grasping:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation Action Server Mock:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# <your_ros2_ws>/src/robot_actions/robot_actions/navigate_action_server.py\r\nimport rclpy\r\nfrom rclpy.action import ActionServer\r\nfrom rclpy.node import Node\r\nfrom example_interfaces.action import NavigateToPose # Assuming a custom action, or use std_msgs for simplicity\r\n\r\n# If no custom action, define a simple structure or use an existing one like nav2_msgs/action/NavigateToPose\r\n\r\n# For this example, let's just use a simple String as goal for demonstration\r\nfrom std_msgs.action import String as StringAction # A trick to use string as action\r\n\r\nclass NavigateActionServer(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__('navigate_action_server')\r\n        self._action_server = ActionServer(\r\n            self,\r\n            StringAction, # Replace with actual NavigateToPose action\r\n            'navigate_to_location',\r\n            self.execute_callback)\r\n        self.get_logger().info('Navigate Action Server started.')\r\n\r\n    def execute_callback(self, goal_handle):\r\n        self.get_logger().info(f'Executing navigation goal: \"{goal_handle.request.data}\"')\r\n        feedback_msg = StringAction.Feedback()\r\n        feedback_msg.sequence = [\"Moving to \" + goal_handle.request.data + \"...\", \"Almost there!\"] # Example feedback\r\n\r\n        for i in range(2):\r\n            goal_handle.publish_feedback(feedback_msg)\r\n            self.get_logger().info(f'Feedback: {feedback_msg.sequence[-1]}')\r\n            rclpy.spin_once(self, timeout_sec=1) # Simulate work\r\n        \r\n        goal_handle.succeed()\r\n        result = StringAction.Result()\r\n        result.sequence = [\"Navigation to \" + goal_handle.request.data + \" complete.\"]\r\n        self.get_logger().info(f'Result: {result.sequence[-1]}')\r\n        return result\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    navigate_action_server = NavigateActionServer()\r\n    rclpy.spin(navigate_action_server)\r\n    navigate_action_server.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grasp Action Server Mock:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# <your_ros2_ws>/src/robot_actions/robot_actions/grasp_action_server.py\r\nimport rclpy\r\nfrom rclpy.action import ActionServer\r\nfrom rclpy.node import Node\r\nfrom std_msgs.action import String as StringAction\r\n\r\nclass GraspActionServer(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__('grasp_action_server')\r\n        self._action_server = ActionServer(\r\n            self,\r\n            StringAction,\r\n            'grasp_object_action',\r\n            self.execute_callback)\r\n        self.get_logger().info('Grasp Action Server started.')\r\n\r\n    def execute_callback(self, goal_handle):\r\n        object_name = goal_handle.request.data\r\n        self.get_logger().info(f'Executing grasp goal for: \"{object_name}\"')\r\n        feedback_msg = StringAction.Feedback()\r\n        feedback_msg.sequence = [\"Approaching object...\", \"Closing gripper...\"] # Example feedback\r\n\r\n        for i in range(2):\r\n            goal_handle.publish_feedback(feedback_msg)\r\n            self.get_logger().info(f'Feedback: {feedback_msg.sequence[-1]}')\r\n            rclpy.spin_once(self, timeout_sec=1) # Simulate work\r\n\r\n        # Simulate success\r\n        goal_handle.succeed()\r\n        result = StringAction.Result()\r\n        result.sequence = [f\"Grasped {object_name} successfully.\"]\r\n        self.get_logger().info(f'Result: {result.sequence[-1]}')\r\n        return result\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    grasp_action_server = GraspActionServer()\r\n    rclpy.spin(grasp_action_server)\r\n    grasp_action_server.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["Create a new package ",(0,i.jsx)(n.code,{children:"robot_actions"})," and add these nodes, update ",(0,i.jsx)(n.code,{children:"setup.py"})," for entry points. Build and source."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Create the VLA Orchestrator Node:"}),"\r\nThis node will be the brain, subscribing to ",(0,i.jsx)(n.code,{children:"/speech_to_text"}),", calling the LLM Planner (if separated), processing its JSON output, then acting as an Action Client to the Navigation and Grasping Action Servers."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# <your_ros2_ws>/src/vla_orchestrator/vla_orchestrator/orchestrator_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom rclpy.action import ActionClient\r\nfrom std_msgs.action import String as StringAction # Use StringAction for mocks\r\n\r\nimport json\r\nimport time\r\n\r\nclass VLAOrchestrator(Node):\r\n\r\n    def __init__(self):\r\n        super().__init__('vla_orchestrator')\r\n        self.speech_sub = self.create_subscription(\r\n            String,\r\n            '/speech_to_text',\r\n            self.speech_command_callback,\r\n            10\r\n        )\r\n        self.plan_sub = self.create_subscription(\r\n            String,\r\n            '/robot_action_plan', # Subscribes to the LLM Planner's output\r\n            self.llm_plan_callback,\r\n            10\r\n        )\r\n        self.object_location_sub = self.create_subscription(\r\n            PointStamped, # From object detector\r\n            '/object_3d_location',\r\n            self.object_location_callback,\r\n            10\r\n        )\r\n\r\n        # Action Clients for robot primitives\r\n        self._navigate_action_client = ActionClient(self, StringAction, 'navigate_to_location')\r\n        self._grasp_action_client = ActionClient(self, StringAction, 'grasp_object_action')\r\n        self._deliver_action_client = ActionClient(self, StringAction, 'deliver_object_to_person_action') # If implemented\r\n\r\n        self.current_plan = []\r\n        self.current_plan_step_index = 0\r\n        self.last_object_location = None # Store latest detected object location\r\n\r\n        self.get_logger().info('VLA Orchestrator Node initialized.')\r\n\r\n    def speech_command_callback(self, msg: String):\r\n        self.get_logger().info(f'Orchestrator received speech command: \"{msg.data}\"')\r\n        # Here, you would typically pass this command to the LLM_Robot_Planner node (e.g., via a service or another topic)\r\n        # For this lab, assume LLM_Robot_Planner directly subscribes to /speech_to_text and publishes to /robot_action_plan\r\n\r\n    def llm_plan_callback(self, msg: String):\r\n        self.get_logger().info(f'Orchestrator received LLM plan:\\n{msg.data}')\r\n        try:\r\n            self.current_plan = json.loads(msg.data)\r\n            self.current_plan_step_index = 0\r\n            self.execute_next_plan_step()\r\n        except json.JSONDecodeError as e:\r\n            self.get_logger().error(f\"Failed to parse LLM plan JSON: {e}\")\r\n\r\n    def object_location_callback(self, msg: PointStamped):\r\n        self.last_object_location = msg\r\n        # You might use this to update internal state for 'grasp' actions etc.\r\n\r\n    def execute_next_plan_step(self):\r\n        if self.current_plan_step_index >= len(self.current_plan):\r\n            self.get_logger().info(\"VLA pipeline: All plan steps executed!\")\r\n            self.current_plan = []\r\n            self.current_plan_step_index = 0\r\n            return\r\n\r\n        step = self.current_plan[self.current_plan_step_index]\r\n        action_name = step.get(\"name\")\r\n        params = step.get(\"parameters\", {})\r\n        \r\n        self.get_logger().info(f\"Executing plan step {self.current_plan_step_index + 1}: {action_name} with {params}\")\r\n\r\n        # Match action_name to appropriate action client\r\n        if action_name == \"navigate_to\":\r\n            if not self._navigate_action_client.wait_for_server(timeout_sec=5.0):\r\n                self.get_logger().error(\"Navigate action server not available.\")\r\n                return\r\n            goal_msg = StringAction.Goal()\r\n            goal_msg.data = params.get(\"location\", \"unknown_location\")\r\n            self.get_logger().info(f\"Sending navigate_to goal: {goal_msg.data}\")\r\n            self._send_goal(self._navigate_action_client, goal_msg, self._navigate_goal_response_callback, self._navigate_feedback_callback)\r\n        \r\n        elif action_name == \"grasp_object\":\r\n            if not self._grasp_action_client.wait_for_server(timeout_sec=5.0):\r\n                self.get_logger().error(\"Grasp action server not available.\")\r\n                return\r\n            object_name = params.get(\"object_name\", \"unknown_object\")\r\n            self.get_logger().info(f\"Sending grasp_object goal: {object_name}\")\r\n            \r\n            # Here, in a real system, you'd use self.last_object_location to refine the grasp,\r\n            # possibly re-triggering detection if needed. For this mock, we just use the name.\r\n            goal_msg = StringAction.Goal()\r\n            goal_msg.data = object_name\r\n            self._send_goal(self._grasp_action_client, goal_msg, self._grasp_goal_response_callback, self._grasp_feedback_callback)\r\n\r\n        elif action_name == \"place_object\":\r\n            # Implement place_object logic, similar to grasp\r\n            self.get_logger().info(f\"Simulating place_object at {params.get('location', 'unknown_location')}\")\r\n            self._advance_plan_step() # Immediately advance for mock\r\n        \r\n        elif action_name == \"deliver_object_to_person\":\r\n            if not self._deliver_action_client.wait_for_server(timeout_sec=5.0):\r\n                self.get_logger().error(\"Deliver action server not available.\")\r\n                return\r\n            person_location = params.get(\"person_location\", \"unknown_person_location\")\r\n            self.get_logger().info(f\"Sending deliver_object_to_person goal: {person_location}\")\r\n            goal_msg = StringAction.Goal()\r\n            goal_msg.data = person_location\r\n            self._send_goal(self._deliver_action_client, goal_msg, self._deliver_goal_response_callback, self._deliver_feedback_callback)\r\n\r\n        elif action_name == \"report_status\":\r\n            self.get_logger().info(f\"Robot status: {params.get('message', 'No message')}\")\r\n            self._advance_plan_step() # Immediately advance for report_status\r\n\r\n        else:\r\n            self.get_logger().warn(f\"Unknown action: {action_name}. Skipping step.\")\r\n            self._advance_plan_step()\r\n\r\n    def _send_goal(self, client, goal_msg, response_callback, feedback_callback):\r\n        self.get_logger().info(f'Waiting for {client.action_name} action server...')\r\n        client.wait_for_server()\r\n        self.get_logger().info(f'Sending goal to {client.action_name} action server...')\r\n        self._send_goal_future = client.send_goal_async(goal_msg, feedback_callback=feedback_callback)\r\n        self._send_goal_future.add_done_callback(response_callback)\r\n\r\n    def _navigate_goal_response_callback(self, future):\r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().error('Navigation goal rejected :(')\r\n            # Handle error / replan\r\n            self.current_plan_step_index = len(self.current_plan) # Stop plan\r\n            return\r\n        self.get_logger().info('Navigation goal accepted :)')\r\n        self._get_result_future = goal_handle.get_result_async()\r\n        self._get_result_future.add_done_callback(self._navigate_get_result_callback)\r\n\r\n    def _navigate_get_result_callback(self, future):\r\n        result = future.result().result\r\n        status = future.result().status\r\n        if status == ActionClient.GoalStatus.SUCCEEDED:\r\n            self.get_logger().info(f'Navigation goal succeeded: {result.sequence[-1]}')\r\n        else:\r\n            self.get_logger().error(f'Navigation goal failed with status: {status}')\r\n            # Handle error / replan\r\n        self._advance_plan_step()\r\n\r\n    def _navigate_feedback_callback(self, feedback_msg):\r\n        self.get_logger().info(f'Navigation feedback: {feedback_msg.feedback.sequence[-1]}')\r\n\r\n    def _grasp_goal_response_callback(self, future):\r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().error('Grasp goal rejected :(')\r\n            # Handle error / replan\r\n            self.current_plan_step_index = len(self.current_plan) # Stop plan\r\n            return\r\n        self.get_logger().info('Grasp goal accepted :)')\r\n        self._get_result_future = goal_handle.get_result_async()\r\n        self._get_result_future.add_done_callback(self._grasp_get_result_callback)\r\n\r\n    def _grasp_get_result_callback(self, future):\r\n        result = future.result().result\r\n        status = future.result().status\r\n        if status == ActionClient.GoalStatus.SUCCEEDED:\r\n            self.get_logger().info(f'Grasp goal succeeded: {result.sequence[-1]}')\r\n        else:\r\n            self.get_logger().error(f'Grasp goal failed with status: {status}')\r\n            # Handle error / replan\r\n        self._advance_plan_step()\r\n\r\n    def _grasp_feedback_callback(self, feedback_msg):\r\n        self.get_logger().info(f'Grasp feedback: {feedback_msg.feedback.sequence[-1]}')\r\n    \r\n    def _deliver_goal_response_callback(self, future):\r\n        goal_handle = future.result()\r\n        if not goal_handle.accepted:\r\n            self.get_logger().error('Deliver goal rejected :(')\r\n            self.current_plan_step_index = len(self.current_plan)\r\n            return\r\n        self.get_logger().info('Deliver goal accepted :)')\r\n        self._get_result_future = goal_handle.get_result_async()\r\n        self._get_result_future.add_done_callback(self._deliver_get_result_callback)\r\n\r\n    def _deliver_get_result_callback(self, future):\r\n        result = future.result().result\r\n        status = future.result().status\r\n        if status == ActionClient.GoalStatus.SUCCEEDED:\r\n            self.get_logger().info(f'Deliver goal succeeded: {result.sequence[-1]}')\r\n        else:\r\n            self.get_logger().error(f'Deliver goal failed with status: {status}')\r\n        self._advance_plan_step()\r\n\r\n    def _deliver_feedback_callback(self, feedback_msg):\r\n        self.get_logger().info(f'Deliver feedback: {feedback_msg.feedback.sequence[-1]}')\r\n\r\n\r\n    def _advance_plan_step(self):\r\n        self.current_plan_step_index += 1\r\n        rclpy.timer.Timer(self, 0.1, self.execute_next_plan_step).once() # Schedule next step\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    vla_orchestrator = VLAOrchestrator()\r\n    rclpy.spin(vla_orchestrator)\r\n    vla_orchestrator.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsxs)(n.strong,{children:["Create a new package ",(0,i.jsx)(n.code,{children:"vla_orchestrator"})," and add this node, update ",(0,i.jsx)(n.code,{children:"setup.py"})," for entry points."]})," Build and source."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Launch All Components:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Terminal 1: Isaac Sim/Gazebo"})," (with humanoid, RGB-D camera publishing, etc.)","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 launch my_humanoid_description display_humanoid.launch.py\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Terminal 2: Whisper ASR Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run whisper_ros_asr whisper_node\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Terminal 3: LLM Robot Planner Node"})," (ensure ",(0,i.jsx)(n.code,{children:"OPENAI_API_KEY"})," is set)","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run llm_robot_planner llm_planner\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Terminal 4: Object Detector 3D Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run object_detector_3d detector_node\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Terminal 5: Mock Robot Action Servers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run robot_actions navigate_action_server\r\nros2 run robot_actions grasp_action_server\r\n# ros2 run robot_actions deliver_action_server # If implemented\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Terminal 6: VLA Orchestrator Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run vla_orchestrator orchestrator_node\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Speak a multi-step command into your microphone."}),'\r\nExample: "Robot, please navigate to the kitchen, then pick up the red apple, and finally deliver it to the living room."\r\n',(0,i.jsx)(n.em,{children:"Observe the logs from all terminals."})," You should see:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Whisper transcribing your speech."}),"\n",(0,i.jsx)(n.li,{children:"LLM Planner receiving the speech, generating a JSON plan, and publishing it."}),"\n",(0,i.jsx)(n.li,{children:"VLA Orchestrator receiving the plan, dispatching actions to the mock servers."}),"\n",(0,i.jsx)(n.li,{children:'Mock action servers logging their "execution."'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"expected-output",children:"Expected Output"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A fully integrated VLA pipeline demonstrated through logs and the simulated robot's (mocked) actions."}),"\n",(0,i.jsx)(n.li,{children:"Successful conversion of a natural language voice command into a sequence of executed robot actions."}),"\n",(0,i.jsx)(n.li,{children:"Understanding of the orchestration logic required to manage the VLA workflow."}),"\n",(0,i.jsx)(n.li,{children:"Clear visualization of the data flow between ASR, LLM planning, object detection, and robot action execution."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Describe the role of the VLA orchestrator node in the full pipeline. How does it ensure the smooth flow of information and execution of tasks?"}),"\n",(0,i.jsx)(n.li,{children:"Explain how ROS 2 Actions are particularly well-suited for implementing the individual steps within an LLM-generated robot plan."}),"\n",(0,i.jsx)(n.li,{children:"What feedback mechanisms could be implemented in the VLA pipeline to enhance its robustness and user experience?"}),"\n",(0,i.jsxs)(n.li,{children:["If an action server reports a failure (e.g., ",(0,i.jsx)(n.code,{children:"grasp_object"})," fails), how would you design the VLA orchestrator to handle this error and potentially trigger a replanning step by the LLM?"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"real-world-applications",children:"Real-world Applications"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrated Home Robotics:"})," A humanoid robot capable of understanding complex, multi-step instructions for tasks around the house, adapting to the environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intelligent Factory Assistants:"})," Robots in manufacturing plants taking verbal orders to retrieve tools, assist with assembly, or reconfigure workstations."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced Disaster Response:"})," Humanoid robots receiving high-level commands for reconnaissance, object manipulation, or victim assistance in dynamic and dangerous environments."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Personalized Healthcare Robotics:"})," Robots assisting patients with complex daily routines based on natural language communication, combining navigation, manipulation, and interaction."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"edge-cases",children:"Edge Cases"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pipeline Latency:"})," The cumulative latency of ASR, LLM inference, vision processing, and action execution can lead to a sluggish response, impacting real-time interaction."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Component Failures:"})," Any single component failure (e.g., ASR error, LLM API outage, object detection failure, action server crash) can halt or derail the entire pipeline."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Conflicting Information:"})," If the LLM plan contradicts visual evidence, the orchestrator needs robust conflict resolution."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Humanoid Body Awareness:"})," If the robot lacks precise self-awareness (e.g., current pose, gripper state, held object), it may execute invalid actions."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"key-entities",children:(0,i.jsx)(n.strong,{children:"Key Entities"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VLA Pipeline:"})," The end-to-end system that translates natural language commands into robot actions, composed of interconnected modules for voice recognition, cognitive planning, visual perception, and physical execution."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Orchestrator Node:"})," A central ROS 2 node that manages the flow and coordination of tasks across the VLA pipeline, acting as a dispatcher for LLM plans and a client for robot action servers."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Actions:"})," A robust communication mechanism in ROS 2 for long-duration, goal-oriented tasks, providing feedback on progress and allowing for cancellation, essential for complex robot behaviors."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Server:"})," A ROS 2 node that implements the logic for a specific robot capability (e.g., navigation, grasping), receives goals from action clients, executes them, and provides feedback/results."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Client:"})," A ROS 2 node that initiates and monitors the execution of a goal on an action server, receiving feedback and waiting for a final result."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition:"})," The LLM's ability to break down complex natural language commands into a sequence of simpler, predefined action primitives, which the orchestrator then executes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback Mechanism:"})," The system's ability to communicate progress, status, or errors back to the human user or internal planning modules, enabling transparency and recovery."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Management:"})," The internal logic within the orchestrator or individual action servers to keep track of the robot's current status, ongoing tasks, and environmental conditions."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"references",children:(0,i.jsx)(n.strong,{children:"References"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Kollar, W., et al. (2018). Learning to follow language instructions in 3D environments. ",(0,i.jsx)(n.em,{children:"Conference on Robot Learning (CoRL)"}),". (Placeholder citation)"]}),"\n",(0,i.jsxs)(n.li,{children:["Paxton, C., et al. (2019). Rethinking Robotic Perception with Deep Learning. ",(0,i.jsx)(n.em,{children:"Science Robotics, 4"}),"(36), eaax2340. (Placeholder citation)"]}),"\n",(0,i.jsxs)(n.li,{children:["Open Robotics. (n.d.). ",(0,i.jsx)(n.em,{children:"ROS 2 Documentation: Actions"}),". (Placeholder citation)"]}),"\n",(0,i.jsxs)(n.li,{children:["Anderson, P., et al. (2018). Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments. ",(0,i.jsx)(n.em,{children:"Conference on Robot Learning (CoRL)"}),". (Placeholder citation)"]}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var t=r(6540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);