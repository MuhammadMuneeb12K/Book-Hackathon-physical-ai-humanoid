"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[399],{6481:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/voice-command-whisper","title":"4.2 Voice Command Systems with Whisper","description":"The journey into Vision-Language-Action (VLA) begins with the most natural form of human communication: spoken language. For a humanoid robot to truly understand and respond to human directives, it must first accurately transcribe spoken words into text. This chapter introduces voice command systems and focuses on OpenAI Whisper, a state-of-the-art Automatic Speech Recognition (ASR) model. We will explore how Whisper can be integrated into a robotic pipeline to reliably convert human speech into text commands, serving as the foundational input for intelligent robot planning and action.","source":"@site/docs/module-4/02-voice-command-whisper.mdx","sourceDirName":"module-4","slug":"/module-4/voice-command-whisper","permalink":"/module-4/voice-command-whisper","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"voice-command-whisper","title":"4.2 Voice Command Systems with Whisper"},"sidebar":"defaultSidebar","previous":{"title":"4.1 What Is Vision-Language-Action (VLA)?","permalink":"/module-4/what-is-vla"},"next":{"title":"4.3 LLM-Based Cognitive Planning","permalink":"/module-4/llm-cognitive-planning"}}');var r=i(4848),t=i(8453);const o={id:"voice-command-whisper",title:"4.2 Voice Command Systems with Whisper"},a=void 0,l={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Tools",id:"tools",level:2},{value:"Chapter Sections",id:"chapter-sections",level:2},{value:"4.2.1 Introduction to Automatic Speech Recognition (ASR) in Robotics",id:"421-introduction-to-automatic-speech-recognition-asr-in-robotics",level:3},{value:"4.2.2 OpenAI Whisper: Architecture and Capabilities",id:"422-openai-whisper-architecture-and-capabilities",level:3},{value:"4.2.3 Setting Up Whisper in a Python Environment",id:"423-setting-up-whisper-in-a-python-environment",level:3},{value:"4.2.4 Integrating Whisper into a ROS 2 Node",id:"424-integrating-whisper-into-a-ros-2-node",level:3},{value:"4.2.5 Practical Considerations for Voice Commands",id:"425-practical-considerations-for-voice-commands",level:3},{value:"Required Diagrams",id:"required-diagrams",level:2},{value:"Hands-on Labs",id:"hands-on-labs",level:2},{value:"Lab 4.2.1: Build Voice Command Translator with Whisper",id:"lab-421-build-voice-command-translator-with-whisper",level:3},{value:"Expected Output",id:"expected-output",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Real-world Applications",id:"real-world-applications",level:2},{value:"Edge Cases",id:"edge-cases",level:2},{value:"<strong>Key Entities</strong>",id:"key-entities",level:3},{value:"<strong>References</strong>",id:"references",level:3}];function d(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"The journey into Vision-Language-Action (VLA) begins with the most natural form of human communication: spoken language. For a humanoid robot to truly understand and respond to human directives, it must first accurately transcribe spoken words into text. This chapter introduces voice command systems and focuses on OpenAI Whisper, a state-of-the-art Automatic Speech Recognition (ASR) model. We will explore how Whisper can be integrated into a robotic pipeline to reliably convert human speech into text commands, serving as the foundational input for intelligent robot planning and action."}),"\n",(0,r.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,r.jsx)(n.p,{children:"The goal of this chapter is to teach students how to use OpenAI Whisper for voice-to-text commands, forming the initial input mechanism for VLA-controlled humanoid robots, and to understand the process of speech recognition in robotics."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the basic principles and challenges of Automatic Speech Recognition (ASR)."}),"\n",(0,r.jsx)(n.li,{children:"Grasp the capabilities and architecture of OpenAI Whisper for speech-to-text transcription."}),"\n",(0,r.jsx)(n.li,{children:"Learn how to integrate Whisper into a ROS 2 robotic system."}),"\n",(0,r.jsx)(n.li,{children:"Process audio input from microphones or audio files using Whisper."}),"\n",(0,r.jsx)(n.li,{children:"Convert spoken natural language commands into accurate text for downstream VLA components."}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the performance of Whisper in various acoustic environments."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Basic understanding of digital audio concepts (sampling rate, channels)."}),"\n",(0,r.jsxs)(n.li,{children:["Familiarity with Python programming and ",(0,r.jsx)(n.code,{children:"rclpy"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"A functional ROS 2 environment."}),"\n",(0,r.jsx)(n.li,{children:"(Optional but recommended): Access to a microphone on your development machine."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR):"})," The technology that enables computers to identify and process human speech and convert it into text."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI Whisper:"})," A pre-trained, general-purpose ASR model capable of transcribing speech in multiple languages and translating them into English."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech-to-Text:"})," The primary function of ASR, converting spoken words into written text."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Processing:"})," Capturing, filtering, and preparing audio data for ASR."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Natural Language Processing (NLP):"})," The field of AI that deals with understanding and generating human language, which often begins with ASR output."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness:"})," The ability of an ASR system to perform well across different speakers, accents, noise levels, and environmental conditions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency:"})," The time delay between when speech is uttered and when its transcription is available."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Word Error Rate (WER):"})," A common metric for evaluating the accuracy of ASR systems."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI Whisper:"})," The primary ASR model."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"whisper"})," Python package:"]})," For programmatic access to Whisper."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"PyAudio"})," (or similar audio library):"]})," For capturing live audio from a microphone."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Humble:"})," For integrating the Whisper node into the robotic system."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"audio_common"})," (ROS 2 package):"]})," For handling audio streams in ROS 2 (optional)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Code Editor:"})," Visual Studio Code."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"chapter-sections",children:"Chapter Sections"}),"\n",(0,r.jsx)(n.h3,{id:"421-introduction-to-automatic-speech-recognition-asr-in-robotics",children:"4.2.1 Introduction to Automatic Speech Recognition (ASR) in Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The need for ASR in human-robot interaction."}),"\n",(0,r.jsx)(n.li,{children:"Challenges: background noise, multiple speakers, accents, domain-specific terminology."}),"\n",(0,r.jsx)(n.li,{children:"Traditional ASR vs. End-to-End Deep Learning ASR."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"422-openai-whisper-architecture-and-capabilities",children:"4.2.2 OpenAI Whisper: Architecture and Capabilities"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Encoder-Decoder Transformer Architecture:"})," How Whisper processes audio and generates text."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-training on Diverse Data:"})," The reason for Whisper's strong generalization."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilinguality and Translation:"})," Capabilities beyond simple transcription."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Sizes:"})," Different Whisper models (tiny, base, small, medium, large) and their trade-offs (accuracy vs. speed vs. resource usage)."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"423-setting-up-whisper-in-a-python-environment",children:"4.2.3 Setting Up Whisper in a Python Environment"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Installing the ",(0,r.jsx)(n.code,{children:"whisper"})," Python package and dependencies (",(0,r.jsx)(n.code,{children:"ffmpeg"}),", ",(0,r.jsx)(n.code,{children:"torch"}),")."]}),"\n",(0,r.jsx)(n.li,{children:"Downloading Whisper models."}),"\n",(0,r.jsx)(n.li,{children:"Basic usage: transcribing an audio file."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"424-integrating-whisper-into-a-ros-2-node",children:"4.2.4 Integrating Whisper into a ROS 2 Node"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Input:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Using ",(0,r.jsx)(n.code,{children:"PyAudio"})," to capture live microphone input."]}),"\n",(0,r.jsxs)(n.li,{children:["Reading audio from a file or a ROS 2 audio topic (e.g., from ",(0,r.jsx)(n.code,{children:"audio_common"}),")."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Node Design:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A subscriber node that receives audio data (or a node that continuously captures from microphone)."}),"\n",(0,r.jsx)(n.li,{children:"Processing the audio through the Whisper model."}),"\n",(0,r.jsxs)(n.li,{children:["Publishing the transcribed text to a ROS 2 topic (e.g., ",(0,r.jsx)(n.code,{children:"std_msgs/msg/String"}),")."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"425-practical-considerations-for-voice-commands",children:"4.2.5 Practical Considerations for Voice Commands"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Reduction:"})," Pre-processing audio to improve ASR accuracy."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VAD (Voice Activity Detection):"})," Detecting when speech begins and ends to avoid transcribing silence or noise."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Push-to-Talk (PTT):"})," Implementing a button-activated recording to minimize unwanted transcriptions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Confidence Scores:"})," Using Whisper's confidence scores to filter out unreliable transcriptions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resource Management:"})," Running Whisper on a dedicated machine, GPU, or edge device (e.g., Jetson)."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"required-diagrams",children:"Required Diagrams"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice Command System Pipeline:"})," A diagram showing microphone input -> audio processing -> Whisper ASR -> text output -> ROS 2 topic."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Whisper Model Architecture (Simplified):"})," A high-level block diagram of Whisper's encoder-decoder structure."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-labs",children:"Hands-on Labs"}),"\n",(0,r.jsx)(n.h3,{id:"lab-421-build-voice-command-translator-with-whisper",children:"Lab 4.2.1: Build Voice Command Translator with Whisper"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective:"})," Create a ROS 2 Python node that captures live audio from a microphone, uses OpenAI Whisper to transcribe it, and publishes the transcribed text to a ROS 2 topic."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prerequisites:"})," ROS 2 Humble installed, Python 3, ",(0,r.jsx)(n.code,{children:"pip"}),", a working microphone. Install ",(0,r.jsx)(n.code,{children:"openai-whisper"}),", ",(0,r.jsx)(n.code,{children:"pyaudio"}),", and ",(0,r.jsx)(n.code,{children:"soundfile"})," via pip (",(0,r.jsx)(n.code,{children:"pip install openai-whisper pyaudio soundfile"}),"). You might also need ",(0,r.jsx)(n.code,{children:"ffmpeg"})," (",(0,r.jsx)(n.code,{children:"sudo apt update && sudo apt install ffmpeg"}),")."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Instructions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Create a new ROS 2 Python package:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>/src\r\nros2 pkg create --build-type ament_python whisper_ros_asr --dependencies rclpy std_msgs\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Navigate into the package and create ",(0,r.jsx)(n.code,{children:"src/whisper_ros_asr/whisper_node.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python"})}),"\n"]}),"\n"]}),"\n","\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyaudio\nimport numpy as np\nimport whisper\nimport time\nimport threading\nimport collections\nimport scipy.signal\n\n# Audio parameters\nFORMAT = pyaudio.paInt16\nCHANNELS = 1\nRATE = 16000 # Whisper expects 16kHz audio\nCHUNK = 1024 # Buffer size\nRECORD_SECONDS_BUFFER = 5 # Keep last 5 seconds of audio\nACTIVATION_THRESHOLD = 500 # Adjust based on your microphone and noise level\nVOICE_DETECTION_WINDOW = 0.5 # Seconds of continuous voice to trigger transcription\nSILENCE_DETECTION_WINDOW = 1.0 # Seconds of continuous silence to stop transcription\nMIN_AUDIO_DURATION = 1.0 # Minimum duration of audio to transcribe\n\nclass WhisperASRNode(Node):\n\n  def __init__(self):\n      super().__init__('whisper_ros_asr_node')\n      self.publisher_ = self.create_publisher(String, '/speech_to_text', 10)\n      self.get_logger().info('Whisper ASR Node initialized.')\n\n      # Declare a parameter for the Whisper model size\n      self.declare_parameter('whisper_model', 'base.en')\n      self.model_name = self.get_parameter('whisper_model').get_parameter_value().string_value\n      self.get_logger().info('Loading Whisper model: {}'.format(self.model_name))\n      self.model = whisper.load_model(self.model_name)\n      self.get_logger().info('Whisper model loaded.')\n\n      # Audio buffer for recording\n      self.audio_buffer = collections.deque()\n      self.recording_started_time = None\n      self.is_transcribing = False\n      self.last_voice_activity_time = time.time()\n      self.voice_activity_detected = False\n\n      # PyAudio setup\n      self.p = pyaudio.PyAudio()\n      self.stream = self.p.open(format=FORMAT,\n                                channels=CHANNELS,\n                                rate=RATE,\n                                input=True,\n                                frames_per_buffer=CHUNK,\n                                stream_callback=self._audio_callback)\n\n      self.get_logger().info('Audio stream opened. Listening for speech...')\n      self.stream.start_stream()\n\n      # Thread for transcription\n      self.transcription_thread = threading.Thread(target=self._transcription_loop)\n      self.transcription_thread.daemon = True\n      self.transcription_thread.start()\n\n  def _audio_callback(self, in_data, frame_count, time_info, status):\n      # Convert bytes to numpy array\n      audio_chunk = np.frombuffer(in_data, dtype=np.int16)\n      \n      # Simple voice activity detection based on amplitude\n      amplitude = np.max(np.abs(audio_chunk))\n\n      current_time = time.time()\n\n      if amplitude > ACTIVATION_THRESHOLD:\n          self.last_voice_activity_time = current_time\n          if not self.voice_activity_detected:\n              # Started speaking\n              if self.recording_started_time is None:\n                  self.recording_started_time = current_time\n              elif (current_time - self.recording_started_time) > VOICE_DETECTION_WINDOW:\n                  self.voice_activity_detected = True\n                  self.get_logger().info(\"Voice activity detected, starting recording.\")\n      else:\n          if self.voice_activity_detected and (current_time - self.last_voice_activity_time) > SILENCE_DETECTION_WINDOW:\n              self.voice_activity_detected = False\n              self.get_logger().info(\"Silence detected, stopping recording and queueing for transcription.\")\n              self.is_transcribing = True # Signal transcription thread to process current buffer\n              self.recording_started_time = None # Reset recording start time\n\n      if self.voice_activity_detected or self.recording_started_time is not None:\n           self.audio_buffer.append(audio_chunk)\n\n\n      # Keep only the last N seconds of audio for transcription context\n      max_chunks = int(RATE / CHUNK * RECORD_SECONDS_BUFFER)\n      while len(self.audio_buffer) > max_chunks:\n          self.audio_buffer.popleft()\n\n      return (in_data, pyaudio.paContinue)\n\n  def _transcription_loop(self):\n      while rclpy.ok():\n          if self.is_transcribing:\n              if len(self.audio_buffer) == 0:\n                  self.get_logger().warn(\"Transcription triggered but audio buffer is empty.\")\n                  self.is_transcribing = False\n                  continue\n\n              self.get_logger().info(\"Transcribing audio...\")\n              \n              # Concatenate buffered audio chunks\n              recorded_audio = np.concatenate(self.audio_buffer)\n              \n              # Normalize to float32 and convert to Whisper's expected format (mono, 16kHz)\n              # Pyaudio already handles mono 16kHz for us if configured.\n              # Whisper expects float32 in range [-1, 1]\n              audio_float32 = recorded_audio.astype(np.float32) / 32768.0\n\n              if audio_float32.shape[0] / RATE < MIN_AUDIO_DURATION:\n                  self.get_logger().info(\"Audio too short ({:.2f}s), discarding.\".format(audio_float32.shape[0] / RATE))\n                  self.audio_buffer.clear()\n                  self.is_transcribing = False\n                  continue\n\n              try:\n                  result = self.model.transcribe(audio_float32, fp16={'type': 'boolean', 'value': 'torch.cuda.is_available()'})\n                  transcribed_text = result[\"text\"].strip()\n                  if transcribed_text:\n                      msg = String()\n                      msg.data = transcribed_text\n                      self.publisher_.publish(msg)\n                      self.get_logger().info(f'Published: \"{transcribed_text}\"')\n                  else:\n                      self.get_logger().info(\"Transcription was empty.\")\n              except Exception as e:\n                  self.get_logger().error(f\"Error during transcription: {e}\")\n              finally:\n                  self.audio_buffer.clear()\n                  self.is_transcribing = False\n          time.sleep(0.1) # Small delay to prevent busy-waiting\n\n  def destroy_node(self):\n      self.stream.stop_stream()\n      self.stream.close()\n      self.p.terminate()\n      super().destroy_node()\n\ndef main(args=None):\n  rclpy.init(args=args)\n  whisper_node = WhisperASRNode()\n  try:\n      rclpy.spin(whisper_node)\n  except KeyboardInterrupt:\n      pass\n  finally:\n      whisper_node.destroy_node()\n      rclpy.shutdown()\n\nif __name__ == '__main__':\n  main()\n","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"3.  **Edit `setup.py` for `whisper_ros_asr`:** Add the entry point.\r\n    ```python\r\n    from setuptools import find_packages, setup\r\n\r\n    package_name = 'whisper_ros_asr'\r\n\r\n    setup(\r\n        name=package_name,\r\n        version='0.0.0',\r\n        packages=find_packages(exclude=['test']),\r\n        data_files=[\r\n            ('share/' + package_name, ['package.xml']),\r\n        ],\r\n        install_requires=['setuptools', 'pyaudio', 'numpy', 'openai-whisper', 'soundfile', 'scipy'], # Added dependencies\r\n        zip_safe=True,\r\n        maintainer='your_name',\r\n        maintainer_email='your_email@example.com',\r\n        description='ROS 2 package for voice command translation using OpenAI Whisper.',\r\n        license='Apache-2.0',\r\n        tests_require=['pytest'],\r\n        entry_points={\r\n            'console_scripts': [\r\n                'whisper_node = whisper_ros_asr.whisper_node:main',\r\n            ],\r\n        },\r\n    )\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build your package and source your workspace:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>\r\ncolcon build --packages-select whisper_ros_asr\r\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Run the Whisper ASR node:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run whisper_ros_asr whisper_node --ros-args -p whisper_model:='base.en' # Or 'small.en' for more accuracy, but slower\n"})}),"\n",(0,r.jsx)(n.em,{children:"Note: The first time you run this, Whisper will download the specified model, which can take some time."})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speak into your microphone."})," After a short period of silence, the node should transcribe your speech and publish it."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Monitor the ",(0,r.jsx)(n.code,{children:"/speech_to_text"})," topic:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /speech_to_text\n"})}),"\n","You should see your transcribed speech printed in the terminal."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"expected-output",children:"Expected Output"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A functional ROS 2 node that continuously monitors microphone input."}),"\n",(0,r.jsx)(n.li,{children:"Accurate transcription of spoken commands into text using OpenAI Whisper."}),"\n",(0,r.jsxs)(n.li,{children:["The transcribed text published to a ROS 2 topic (",(0,r.jsx)(n.code,{children:"/speech_to_text"}),")."]}),"\n",(0,r.jsx)(n.li,{children:"Understanding of how to integrate external Python libraries (like Whisper and PyAudio) into a ROS 2 system."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"What are the main advantages of using a pre-trained model like OpenAI Whisper for ASR in robotics compared to training a custom model from scratch?"}),"\n",(0,r.jsx)(n.li,{children:"How can environmental noise affect the accuracy of Whisper's transcription, and what strategies might be employed to mitigate this in a real-world robotic setting?"}),"\n",(0,r.jsxs)(n.li,{children:["Explain the role of ",(0,r.jsx)(n.code,{children:"PyAudio"})," in the provided example and how it interfaces with the microphone hardware."]}),"\n",(0,r.jsxs)(n.li,{children:["Describe how the ",(0,r.jsx)(n.code,{children:"whisper_model"})," parameter allows you to balance transcription accuracy with computational resource usage."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"real-world-applications",children:"Real-world Applications"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Humanoid Home Assistants:"})," Enabling robots to understand and execute verbal instructions for household tasks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Command and Control in Hazardous Environments:"})," Allowing human operators to verbally command robots in situations where physical interaction is difficult or dangerous."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robotics for Individuals with Disabilities:"})," Providing an intuitive voice interface for controlling assistive humanoid robots."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interactive Kiosks and Service Robots:"})," Enhancing user experience by allowing natural language interaction at public service points."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"edge-cases",children:"Edge Cases"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Background Noise:"})," High levels of ambient noise can significantly degrade transcription accuracy."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multiple Speakers:"})," Differentiating between commands from different users or handling overlapping speech is challenging."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accents and Dialects:"})," While Whisper is robust, very strong or unfamiliar accents might reduce accuracy."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Domain-Specific Terminology:"})," Uncommon technical terms or proper nouns might be transcribed incorrectly without fine-tuning or contextual hints."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational Latency:"})," Larger Whisper models require more processing power, potentially leading to noticeable delays between speech and transcription."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"key-entities",children:(0,r.jsx)(n.strong,{children:"Key Entities"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR):"})," The technology that converts spoken language into written text, a critical component for natural human-robot interaction."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI Whisper:"})," A highly accurate, general-purpose, pre-trained ASR model developed by OpenAI, capable of transcribing multilingual speech and translating it into English."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"PyAudio"}),":"]})," A Python library that provides bindings for PortAudio, enabling cross-platform access to audio input and output devices (e.g., microphones)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"whisper"})," Python Package:"]})," The official Python wrapper for OpenAI Whisper, facilitating easy loading of models and transcription of audio data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice Activity Detection (VAD):"})," An algorithm or technique used to detect the presence or absence of human speech in an audio stream, helping to filter out silence or noise."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:[(0,r.jsx)(n.code,{children:"std_msgs/msg/String"}),":"]})," A standard ROS 2 message type used to publish simple text data, perfect for carrying the transcribed speech."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational Latency:"})," The delay introduced by the ASR process, from the end of a spoken phrase to the availability of its transcribed text, which is a key consideration for real-time robotic response."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Sizes:"})," Different versions of the Whisper model (e.g., ",(0,r.jsx)(n.code,{children:"tiny"}),", ",(0,r.jsx)(n.code,{children:"base"}),", ",(0,r.jsx)(n.code,{children:"small"}),", ``medium",(0,r.jsx)(n.code,{children:", "}),"large`) offering trade-offs between transcription accuracy, inference speed, and memory footprint."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"references",children:(0,r.jsx)(n.strong,{children:"References"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Radford, A., et al. (2022). Robust Speech Recognition via Large-Scale Weak Supervision. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:2212.04356"}),". (Placeholder citation)"]}),"\n",(0,r.jsxs)(n.li,{children:["OpenAI. (n.d.). ",(0,r.jsx)(n.em,{children:"Introducing Whisper"}),". (Placeholder citation)"]}),"\n",(0,r.jsxs)(n.li,{children:["PortAudio. (n.d.). ",(0,r.jsx)(n.em,{children:"PyAudio documentation"}),". (Placeholder citation)"]}),"\n",(0,r.jsxs)(n.li,{children:["Dhar, P., et al. (2021). A Survey on Speech Recognition for Robotics. ",(0,r.jsx)(n.em,{children:"Sensors, 21"}),"(14), 4811. (Placeholder citation)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);