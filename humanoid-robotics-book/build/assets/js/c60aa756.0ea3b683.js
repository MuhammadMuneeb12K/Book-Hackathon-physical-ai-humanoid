"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[928],{6104:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-2/sensor-simulation","title":"2.4 Sensor Simulation (LiDAR, Depth, IMU)","description":"With a physics-accurate environment established in Gazebo, the next crucial step for developing intelligent humanoid robots is to equip them with virtual senses. Just as in the real world, robots rely heavily on sensor data to perceive their surroundings, localize themselves, and make informed decisions. This chapter focuses on simulating essential robotics sensors\u2014LiDAR, IMU, and RGB-D cameras\u2014within Gazebo, enabling you to generate realistic data streams for perception algorithms and control systems.","source":"@site/docs/module-2/04-sensor-simulation.mdx","sourceDirName":"module-2","slug":"/module-2/sensor-simulation","permalink":"/module-2/sensor-simulation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"sensor-simulation","title":"2.4 Sensor Simulation (LiDAR, Depth, IMU)"},"sidebar":"defaultSidebar","previous":{"title":"2.3 Physics Simulation (Gravity, Rigid Bodies, Collisions)","permalink":"/module-2/physics-simulation"},"next":{"title":"2.5 Unity for High-Fidelity Robotics","permalink":"/module-2/unity-for-robotics"}}');var s=i(4848),a=i(8453);const o={id:"sensor-simulation",title:"2.4 Sensor Simulation (LiDAR, Depth, IMU)"},t=void 0,l={},d=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Tools",id:"tools",level:2},{value:"Chapter Sections",id:"chapter-sections",level:2},{value:"2.4.1 Introduction to Sensor Simulation and the Reality Gap",id:"241-introduction-to-sensor-simulation-and-the-reality-gap",level:3},{value:"2.4.2 Simulating LiDAR Sensors",id:"242-simulating-lidar-sensors",level:3},{value:"2.4.3 Simulating IMU Sensors",id:"243-simulating-imu-sensors",level:3},{value:"2.4.4 Simulating RGB-D Cameras",id:"244-simulating-rgb-d-cameras",level:3},{value:"2.4.5 Bridging Sensor Data to ROS 2 and Visualization",id:"245-bridging-sensor-data-to-ros-2-and-visualization",level:3},{value:"Required Diagrams",id:"required-diagrams",level:2},{value:"Hands-on Labs",id:"hands-on-labs",level:2},{value:"Lab 2.4.1: Adding Camera + LiDAR Plugins to a Humanoid URDF",id:"lab-241-adding-camera--lidar-plugins-to-a-humanoid-urdf",level:3},{value:"Expected Output",id:"expected-output",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Real-world Applications",id:"real-world-applications",level:2},{value:"Edge Cases",id:"edge-cases",level:2},{value:"<strong>Key Entities</strong>",id:"key-entities",level:3},{value:"<strong>References</strong>",id:"references",level:3}];function c(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"With a physics-accurate environment established in Gazebo, the next crucial step for developing intelligent humanoid robots is to equip them with virtual senses. Just as in the real world, robots rely heavily on sensor data to perceive their surroundings, localize themselves, and make informed decisions. This chapter focuses on simulating essential robotics sensors\u2014LiDAR, IMU, and RGB-D cameras\u2014within Gazebo, enabling you to generate realistic data streams for perception algorithms and control systems."}),"\n",(0,s.jsx)(n.h2,{id:"goal",children:"Goal"}),"\n",(0,s.jsx)(n.p,{children:"The goal of this chapter is to teach students how to simulate various sensors (LiDAR, IMU, RGB-D cameras) within Gazebo, enabling them to test sensor-driven robot behaviors, perception algorithms, and data fusion techniques for humanoid robots."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the principles behind common robotics sensors: LiDAR, IMU, and RGB-D cameras."}),"\n",(0,s.jsx)(n.li,{children:"Learn how to integrate sensor plugins into Gazebo models via SDF."}),"\n",(0,s.jsx)(n.li,{children:"Configure parameters for simulated LiDAR sensors (e.g., range, resolution, scan rate)."}),"\n",(0,s.jsx)(n.li,{children:"Configure parameters for simulated IMU sensors (e.g., noise, bias, update rate)."}),"\n",(0,s.jsx)(n.li,{children:"Set up RGB-D cameras to generate color images, depth maps, and point clouds."}),"\n",(0,s.jsx)(n.li,{children:"Bridge simulated sensor data from Gazebo to ROS 2 topics for processing by downstream nodes."}),"\n",(0,s.jsx)(n.li,{children:"Grasp the concept of sensor noise and its importance in simulation."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A functional Gazebo world and an understanding of SDF/URDF from previous chapters."}),"\n",(0,s.jsxs)(n.li,{children:["Familiarity with ROS 2 topics and message types, especially for sensor data (",(0,s.jsx)(n.code,{children:"sensor_msgs"}),")."]}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of how real-world LiDAR, IMU, and cameras function."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Plugin:"})," A Gazebo plugin that simulates the behavior of a physical sensor, generating data and often publishing it to ROS 2 topics."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging):"})," A sensor that measures distances to objects by illuminating the target with laser light and measuring the reflection time. Simulated as a ",(0,s.jsx)(n.code,{children:"ray"})," sensor in Gazebo."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Point Cloud:"})," A set of data points in a 3D coordinate system, typically generated by LiDAR or depth cameras."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU (Inertial Measurement Unit):"})," A sensor that measures a body's specific force (acceleration) and angular rate (gyroscope), and sometimes magnetic field (magnetometer). Simulated to provide ",(0,s.jsx)(n.code,{children:"Imu"})," messages."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RGB-D Camera:"})," A camera that captures both color (RGB) images and depth information (D), often using structured light or time-of-flight principles. Simulated to produce ",(0,s.jsx)(n.code,{children:"Image"})," and ",(0,s.jsx)(n.code,{children:"PointCloud2"})," messages."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Noise:"})," Unwanted variations or fluctuations in sensor measurements, crucial to simulate for realistic algorithm testing."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Fusion:"})," Combining data from multiple sensors to obtain a more accurate or complete understanding of the environment or robot state."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Bridge:"})," The ",(0,s.jsx)(n.code,{children:"ros_gz_sim"})," package, which translates data between Gazebo's internal topics and ROS 2 topics."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"tools",children:"Tools"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gazebo (Ignition Fortress):"})," The simulation environment."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SDF (Simulation Description Format):"})," For defining sensor plugins within models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Humble:"})," For receiving and processing simulated sensor data."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RViz2:"})," For visualizing sensor data (point clouds, images, IMU orientation)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"ros_gz_sim"}),":"]})," The ROS 2 to Ignition Gazebo bridge."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chapter-sections",children:"Chapter Sections"}),"\n",(0,s.jsx)(n.h3,{id:"241-introduction-to-sensor-simulation-and-the-reality-gap",children:"2.4.1 Introduction to Sensor Simulation and the Reality Gap"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Why simulate sensors? Safety, cost, reproducibility, synthetic data generation."}),"\n",(0,s.jsx)(n.li,{children:'Understanding the "reality gap" and how realistic sensor simulation helps bridge it.'}),"\n",(0,s.jsx)(n.li,{children:"The importance of modeling sensor noise and limitations."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"242-simulating-lidar-sensors",children:"2.4.2 Simulating LiDAR Sensors"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR Principles:"})," How it works (briefly)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Gazebo's ",(0,s.jsx)(n.code,{children:"gpu_ray"})," and ",(0,s.jsx)(n.code,{children:"ray"})," sensors:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Defining the sensor type in SDF (",(0,s.jsx)(n.code,{children:"<sensor type='gpu_ray'>"})," or ",(0,s.jsx)(n.code,{children:"<sensor type='ray'>"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"horizontal/scan/ranges"}),": min angle, max angle, resolution."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"vertical/scan/ranges"}),": (for 3D LiDARs)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"range"}),": min, max range, resolution."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"update_rate"}),": How often the sensor publishes data."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"noise"}),": Adding Gaussian noise to range measurements."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ros/topic"}),": Publishing data to a ROS 2 topic (",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/LaserScan"})," or ",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/PointCloud2"}),")."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Integrating a LiDAR sensor onto a humanoid model."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"243-simulating-imu-sensors",children:"2.4.3 Simulating IMU Sensors"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU Principles:"})," Accelerometers and gyroscopes."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Gazebo's ",(0,s.jsx)(n.code,{children:"imu"})," sensor:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Defining the sensor type (",(0,s.jsx)(n.code,{children:"<sensor type='imu'>"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"orientation_reference_frame"}),': Which frame defines "north" and "down."']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"update_rate"}),": Sensor data update frequency."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"noise"}),": Modeling gyroscope and accelerometer noise (",(0,s.jsx)(n.code,{children:"rate_noise"}),", ",(0,s.jsx)(n.code,{children:"rate_bias_stability"}),", ",(0,s.jsx)(n.code,{children:"accel_noise"}),", ",(0,s.jsx)(n.code,{children:"accel_bias_stability"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ros/topic"}),": Publishing data to a ROS 2 topic (",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/Imu"}),")."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Placing an IMU on the torso or base of a humanoid."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"244-simulating-rgb-d-cameras",children:"2.4.4 Simulating RGB-D Cameras"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RGB-D Camera Principles:"})," Color image, depth image, and point clouds."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Gazebo's ",(0,s.jsx)(n.code,{children:"camera"})," and ",(0,s.jsx)(n.code,{children:"depth_camera"})," sensors:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Defining the sensor type (",(0,s.jsx)(n.code,{children:"<sensor type='camera'>"}),", ",(0,s.jsx)(n.code,{children:"<sensor type='depth_camera'>"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"image"}),": width, height, format."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"clip"}),": near, far planes."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"horizontal_fov"}),", ",(0,s.jsx)(n.code,{children:"vertical_fov"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"update_rate"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"noise"}),": Gaussian, Poisson."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ros/camera_info_topic"}),", ",(0,s.jsx)(n.code,{children:"ros/image_topic"}),", ",(0,s.jsx)(n.code,{children:"ros/depth_image_topic"}),", ",(0,s.jsx)(n.code,{children:"ros/point_cloud_topic"}),": Publishing to ROS 2."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Mounting an RGB-D camera on a humanoid's head."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"245-bridging-sensor-data-to-ros-2-and-visualization",children:"2.4.5 Bridging Sensor Data to ROS 2 and Visualization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Utilizing ",(0,s.jsx)(n.code,{children:"ros_gz_sim"})," plugins (e.g., ",(0,s.jsx)(n.code,{children:"ignition_ros_sensor_bridge"}),") within your Gazebo world or robot SDF to enable ROS 2 communication for each sensor."]}),"\n",(0,s.jsx)(n.li,{children:"Subscribing to simulated sensor topics in ROS 2 nodes."}),"\n",(0,s.jsxs)(n.li,{children:["Visualizing ",(0,s.jsx)(n.code,{children:"LaserScan"}),", ",(0,s.jsx)(n.code,{children:"PointCloud2"}),", and ",(0,s.jsx)(n.code,{children:"Image"})," messages in RViz2."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TF2 Integration:"})," Ensuring sensors publish their transforms correctly to the TF2 tree."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"required-diagrams",children:"Required Diagrams"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR Scan Pattern:"})," An illustration showing how a LiDAR sweeps to generate distance measurements."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RGB-D Camera Output:"})," A visual representation of an RGB image, corresponding depth map, and generated point cloud."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simulated Sensor Data Flow:"})," A diagram showing Gazebo sensor plugins generating data, ",(0,s.jsx)(n.code,{children:"ros_gz_sim"})," bridging it, and ROS 2 nodes consuming it."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-labs",children:"Hands-on Labs"}),"\n",(0,s.jsx)(n.h3,{id:"lab-241-adding-camera--lidar-plugins-to-a-humanoid-urdf",children:"Lab 2.4.1: Adding Camera + LiDAR Plugins to a Humanoid URDF"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective:"})," Enhance the ",(0,s.jsx)(n.code,{children:"simple_humanoid"})," URDF from Module 1 by integrating simulated LiDAR and RGB-D camera sensors, then visualize their output in RViz2 while the robot is in Gazebo."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prerequisites:"})," Completed Lab 1.3.1 (humanoid URDF) and Lab 2.2.1 (basic Gazebo world). Ensure ",(0,s.jsx)(n.code,{children:"ros-humble-ros-ign-gazebo"})," and ",(0,s.jsx)(n.code,{children:"ros-humble-ros-ign-bridge"})," are installed."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Instructions:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Modify ",(0,s.jsx)(n.code,{children:"my_humanoid_description/urdf/humanoid.urdf.xacro"}),":"]})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Add a camera link and a LiDAR link."}),"\n",(0,s.jsxs)(n.li,{children:["Attach them to the ",(0,s.jsx)(n.code,{children:"head_link"})," or ",(0,s.jsx)(n.code,{children:"torso_link"})," using ",(0,s.jsx)(n.code,{children:"fixed"})," joints."]}),"\n",(0,s.jsx)(n.li,{children:"Define the sensor plugins within the respective links' Gazebo tags."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<robot name="simple_humanoid" xmlns:xacro="http://ros.org/xacro">\r\n\r\n    <xacro:property name="M_PI" value="3.1415926535897931" />\r\n\r\n    \x3c!-- ... existing base_link, torso_link, torso_joint, head_link, head_yaw_joint ... --\x3e\r\n    \x3c!-- ... existing arm_segment macro and right arm instance ... --\x3e\r\n\r\n    \x3c!-- CAMERA SENSOR --\x3e\r\n    <link name="camera_link">\r\n        <visual>\r\n            <geometry><box size="0.02 0.05 0.05"/></geometry>\r\n            <material name="black"><color rgba="0 0 0 1"/></material>\r\n        </visual>\r\n        <collision>\r\n            <geometry><box size="0.02 0.05 0.05"/></geometry>\r\n        </collision>\r\n        <inertial>\r\n            <mass value="0.1"/>\r\n            <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\r\n        </inertial>\r\n    </link>\r\n    <joint name="camera_joint" type="fixed">\r\n        <parent link="head_link"/>\r\n        <child link="camera_link"/>\r\n        <origin xyz="0.06 0 0" rpy="0 0 0"/> \x3c!-- Slightly in front of the head --\x3e\r\n    </joint>\r\n\r\n    <gazebo reference="camera_link">\r\n        <sensor name="camera_sensor" type="camera">\r\n            <pose>0 0 0 0 0 0</pose>\r\n            <visualize>true</visualize>\r\n            <update_rate>30.0</update_rate>\r\n            <camera>\r\n                <horizontal_fov>1.047</horizontal_fov>\r\n                <image>\r\n                    <width>640</width>\r\n                    <height>480</height>\r\n                    <format>R8G8B8</format>\r\n                </image>\r\n                <clip>\r\n                    <near>0.05</near>\r\n                    <far>300</far>\r\n                </clip>\r\n                <noise>\r\n                    <type>gaussian</type>\r\n                    <mean>0.0</mean>\r\n                    <stddev>0.007</stddev>\r\n                </noise>\r\n            </camera>\r\n            <plugin name="camera_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">\r\n                <ros>\r\n                    <namespace>/simple_humanoid/camera</namespace>\r\n                    <ros_remap>rgb/image_raw:=image_raw</ros_remap>\r\n                    <ros_remap>rgb/camera_info:=camera_info</ros_remap>\r\n                </ros>\r\n                <sensor_name>camera_sensor</sensor_name>\r\n                <frame_id>camera_link</frame_id>\r\n            </plugin>\r\n        </sensor>\r\n        <sensor name="depth_camera_sensor" type="depth_camera">\r\n            <pose>0 0 0 0 0 0</pose>\r\n            <visualize>false</visualize>\r\n            <update_rate>30.0</update_rate>\r\n            <camera>\r\n                <horizontal_fov>1.047</horizontal_fov>\r\n                <image>\r\n                    <width>640</width>\r\n                    <height>480</height>\r\n                    <format>L_INT16</format>\r\n                </image>\r\n                <clip>\r\n                    <near>0.05</near>\r\n                    <far>10</far>\r\n                </clip>\r\n                <noise>\r\n                    <type>gaussian</type>\r\n                    <mean>0.0</mean>\r\n                    <stddev>0.007</stddev>\r\n                </noise>\r\n            </camera>\r\n            <plugin name="depth_camera_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">\r\n                <ros>\r\n                    <namespace>/simple_humanoid/camera</namespace>\r\n                    <ros_remap>depth/image_raw:=depth_image_raw</ros_remap>\r\n                    <ros_remap>depth/camera_info:=depth_camera_info</ros_remap>\r\n                    <ros_remap>points:=depth/points</ros_remap>\r\n                </ros>\r\n                <sensor_name>depth_camera_sensor</sensor_name>\r\n                <frame_id>camera_link</frame_id>\r\n            </plugin>\r\n        </sensor>\r\n    </gazebo>\r\n\r\n    \x3c!-- LIDAR SENSOR --\x3e\r\n    <link name="lidar_link">\r\n        <visual>\r\n            <geometry><cylinder radius="0.03" length="0.04"/></geometry>\r\n            <material name="grey"><color rgba="0.5 0.5 0.5 1"/></material>\r\n        </visual>\r\n        <collision>\r\n            <geometry><cylinder radius="0.03" length="0.04"/></geometry>\r\n        </collision>\r\n        <inertial>\r\n            <mass value="0.2"/>\r\n            <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\r\n        </inertial>\r\n    </link>\r\n    <joint name="lidar_joint" type="fixed">\r\n        <parent link="head_link"/>\r\n        <child link="lidar_link"/>\r\n        <origin xyz="0.07 0 0.05" rpy="0 0 0"/> \x3c!-- Slightly above and in front of head --\x3e\r\n    </joint>\r\n\r\n    <gazebo reference="lidar_link">\r\n        <sensor name="lidar_sensor" type="gpu_ray">\r\n            <pose>0 0 0 0 0 0</pose>\r\n            <visualize>true</visualize>\r\n            <update_rate>10.0</update_rate>\r\n            <ray>\r\n                <scan>\r\n                    <horizontal>\r\n                        <samples>360</samples>\r\n                        <resolution>1</resolution>\r\n                        <min_angle>-${M_PI}</min_angle>\r\n                        <max_angle>${M_PI}</max_angle>\r\n                    </horizontal>\r\n                </scan>\r\n                <range>\r\n                    <min>0.1</min>\r\n                    <max>10.0</max>\r\n                    <resolution>0.01</resolution>\r\n                </range>\r\n                <noise>\r\n                    <type>gaussian</type>\r\n                    <mean>0.0</mean>\r\n                    <stddev>0.01</stddev>\r\n                </noise>\r\n            </ray>\r\n            <plugin name="lidar_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">\r\n                <ros>\r\n                    <namespace>/simple_humanoid/lidar</namespace>\r\n                    <ros_remap>scan:=scan</ros_remap>\r\n                    <ros_remap>points:=points</ros_remap>\r\n                </ros>\r\n                <sensor_name>lidar_sensor</sensor_name>\r\n                <frame_id>lidar_link</frame_id>\r\n            </plugin>\r\n        </sensor>\r\n    </gazebo>\r\n\r\n    \x3c!-- IMU SENSOR --\x3e\r\n    <link name="imu_link">\r\n        <inertial>\r\n            <mass value="0.01"/>\r\n            <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\r\n        </inertial>\r\n    </link>\r\n    <joint name="imu_joint" type="fixed">\r\n        <parent link="torso_link"/>\r\n        <child link="imu_link"/>\r\n        <origin xyz="0 0 0" rpy="0 0 0"/> \x3c!-- Centered in torso --\x3e\r\n    </joint>\r\n\r\n    <gazebo reference="imu_link">\r\n        <sensor name="imu_sensor" type="imu">\r\n            <always_on>true</always_on>\r\n            <update_rate>100.0</update_rate>\r\n            <visualize>false</visualize>\r\n            <imu>\r\n                <angular_velocity>\r\n                    <x>\r\n                        <noise type="gaussian">\r\n                            <mean>0.0</mean>\r\n                            <stddev>2e-4</stddev>\r\n                            <bias_mean>0.0000075</bias_mean>\r\n                            <bias_stddev>0.0000008</bias_stddev>\r\n                        </noise>\r\n                    </x>\r\n                    <y>\r\n                        <noise type="gaussian">\r\n                            <mean>0.0</mean>\r\n                            <stddev>2e-4</stddev>\r\n                            <bias_mean>0.0000075</bias_mean>\r\n                            <bias_stddev>0.0000008</bias_stddev>\r\n                        </noise>\r\n                    </y>\r\n                    <z>\r\n                        <noise type="gaussian">\r\n                            <mean>0.0</mean>\r\n                            <stddev>2e-4</stddev>\r\n                            <bias_mean>0.0000075</bias_mean>\r\n                            <bias_stddev>0.0000008</bias_stddev>\r\n                        </noise>\r\n                    </z>\r\n                </angular_velocity>\r\n                <linear_acceleration>\r\n                    <x>\r\n                        <noise type="gaussian">\r\n                            <mean>0.0</mean>\r\n                            <stddev>2e-2</stddev>\r\n                            <bias_mean>0.1</bias_mean>\r\n                            <bias_stddev>0.001</bias_stddev>\r\n                        </noise>\r\n                    </x>\r\n                    <y>\r\n                        <noise type="gaussian">\r\n                            <mean>0.0</mean>\r\n                            <stddev>2e-2</stddev>\r\n                            <bias_mean>0.1</bias_mean>\r\n                            <bias_stddev>0.001</bias_stddev>\r\n                        </noise>\r\n                    </y>\r\n                    <z>\r\n                        <noise type="gaussian">\r\n                            <mean>0.0</mean>\r\n                            <stddev>2e-2</stddev>\r\n                            <bias_mean>0.1</bias_mean>\r\n                            <bias_stddev>0.001</bias_stddev>\r\n                        </noise>\r\n                        </noise>\r\n                    </z>\r\n                </linear_acceleration>\r\n            </imu>\r\n            <plugin name="imu_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">\r\n                <ros>\r\n                    <namespace>/simple_humanoid/imu</namespace>\r\n                    <ros_remap>data:=data</ros_remap>\r\n                </ros>\r\n                <sensor_name>imu_sensor</sensor_name>\r\n                <frame_id>imu_link</frame_id>\r\n            </plugin>\r\n        </sensor>\r\n    </gazebo>\r\n\r\n</robot>\n'})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Generate the updated URDF:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>/src/my_humanoid_description/urdf\r\nros2 run xacro xacro humanoid.urdf.xacro > humanoid.urdf\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Update ",(0,s.jsx)(n.code,{children:"my_humanoid_description/launch/display_humanoid.launch.py"})," to spawn the robot in Gazebo and open RViz2:"]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import os\r\nfrom ament_index_python.packages import get_package_share_directory\r\nfrom launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, OpaqueFunction\r\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\r\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\r\nfrom launch_ros.actions import Node\r\nfrom launch_ros.substitutions import FindPackageShare\r\n\r\ndef launch_setup(context, *args, **kwargs):\r\n    # Get the path to the robot description package\r\n    robot_description_path = get_package_share_directory('my_humanoid_description')\r\n    urdf_file_path = os.path.join(robot_description_path, 'urdf', 'humanoid.urdf')\r\n\r\n    # Load the URDF file\r\n    with open(urdf_file_path, 'r') as infp:\r\n        robot_desc = infp.read()\r\n\r\n    # Get the path to the simple_room.world file\r\n    world_file_path = os.path.join(\r\n        get_package_share_directory('my_gazebo_worlds'),\r\n        'worlds',\r\n        'simple_room.world'\r\n    )\r\n\r\n    # Include the Gazebo launch file with our custom world\r\n    gazebo_launch = IncludeLaunchDescription(\r\n        PythonLaunchDescriptionSource([os.path.join(\r\n            get_package_share_directory('ros_gz_sim'), 'launch'),\r\n            '/gz_sim.launch.py']),\r\n        launch_arguments={\r\n            'gz_args': world_file_path,\r\n            'force_system_paths': 'True' # Ensure models from fuel are found\r\n        }.items()\r\n    )\r\n\r\n    # Define the robot state publisher node\r\n    robot_state_publisher_node = Node(\r\n        package='robot_state_publisher',\r\n        executable='robot_state_publisher',\r\n        name='robot_state_publisher',\r\n        output='screen',\r\n        parameters=[{'robot_description': robot_desc,\r\n                     'use_sim_time': True}] # Important for simulated robots\r\n    )\r\n\r\n    # Define the joint state publisher GUI node\r\n    joint_state_publisher_gui_node = Node(\r\n        package='joint_state_publisher_gui',\r\n        executable='joint_state_publisher_gui',\r\n        name='joint_state_publisher_gui',\r\n        output='screen'\r\n    )\r\n\r\n    # Spawn the robot into Gazebo\r\n    spawn_robot_node = Node(\r\n        package='ros_gz_sim',\r\n        executable='create',\r\n        arguments=[\r\n            '-name', 'simple_humanoid',\r\n            '-topic', 'robot_description',\r\n            '-x', '0.0', '-y', '0.0', '-z', '1.0' # Spawn robot slightly above ground\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    # RViz2 Node (re-using previous setup)\r\n    # Create rviz config directory if it doesn't exist\r\n    rviz_config_dir = os.path.join(get_package_share_directory('my_humanoid_description'), 'rviz')\r\n    if not os.path.exists(rviz_config_dir):\r\n        os.makedirs(rviz_config_dir)\r\n    # Point to a default rviz config or create a minimal one\r\n    default_rviz_config_path = os.path.join(rviz_config_dir, 'humanoid_with_sensors.rviz')\r\n    if not os.path.exists(default_rviz_config_path):\r\n        with open(default_rviz_config_path, 'w') as f:\r\n            f.write(\"\"\"\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Global Options:\r\nFixed Frame: base_link\r\nDisplays:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Class: rviz_default_plugins/RobotModel\r\nName: RobotModel\r\nEnabled: true\r\nRobot Description: robot_description"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Class: rviz_default_plugins/Image\r\nName: RGB Image\r\nTopic: /simple_humanoid/camera/image_raw\r\nEnabled: true"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Class: rviz_default_plugins/PointCloud2\r\nName: Depth PointCloud\r\nTopic: /simple_humanoid/camera/depth/points\r\nEnabled: true"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Class: rviz_default_plugins/LaserScan\r\nName: LiDAR Scan\r\nTopic: /simple_humanoid/lidar/scan\r\nEnabled: true"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Class: rviz_default_plugins/Imu\r\nName: IMU\r\nTopic: /simple_humanoid/imu/data\r\nEnabled: true\r\nCovariance Type: 'ellipses'\r\nLinear Acceleration: true\r\nAngular Velocity: true\r\nOrientation: true\r\nOrientation Show Arrows: true\r\nOrientation Arrow Length: 0.5\r\nFlat 2D: false\r\nUnreliable: false\r\nColor: 255; 25; 25\r\n\"\"\")\r\nrviz_node = Node(\r\npackage='rviz2',\r\nexecutable='rviz2',\r\nname='rviz2',\r\noutput='screen',\r\narguments=['-d', default_rviz_config_path]\r\n)"}),"\n",(0,s.jsx)(n.p,{children:"return [\r\ngazebo_launch,\r\nrobot_state_publisher_node,\r\njoint_state_publisher_gui_node,\r\nspawn_robot_node,\r\nrviz_node\r\n]"}),"\n",(0,s.jsx)(n.p,{children:"def generate_launch_description():\r\nreturn LaunchDescription([OpaqueFunction(function=launch_setup)])"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Add ",(0,s.jsx)(n.code,{children:"ros_gz_sim"})," as a dependency to ",(0,s.jsx)(n.code,{children:"my_humanoid_description/package.xml"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- ... --\x3e\r\n  <depend>ros_gz_sim</depend>\r\n  <exec_depend>joint_state_publisher_gui</exec_depend>\r\n\x3c!-- ... --\x3e\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Build both ",(0,s.jsx)(n.code,{children:"my_humanoid_description"})," and ",(0,s.jsx)(n.code,{children:"my_gazebo_worlds"})," packages and source your workspace:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd <your_ros2_ws>\r\ncolcon build --packages-select my_humanoid_description my_gazebo_worlds\r\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Launch the combined Gazebo and RViz2 setup:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch my_humanoid_description display_humanoid.launch.py\n"})}),"\n","You should see Gazebo GUI with your simple room and the humanoid robot. RViz2 should also launch, displaying the robot model, its camera feed, depth point cloud, LiDAR scan, and IMU data (as the robot falls to the ground)."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"expected-output",children:"Expected Output"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"An updated humanoid URDF (or Xacro) that includes simulated LiDAR, RGB-D camera, and IMU sensors."}),"\n",(0,s.jsx)(n.li,{children:"A Gazebo simulation where the humanoid robot with its sensors is spawned in your custom world."}),"\n",(0,s.jsx)(n.li,{children:"RViz2 displaying the robot model and real-time data streams from the simulated sensors (images, point clouds, laser scans, IMU orientation/acceleration)."}),"\n",(0,s.jsx)(n.li,{children:"Understanding of how to configure sensor plugins in URDF/SDF and bridge their data to ROS 2."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Why is it crucial to simulate sensor noise, and how can you configure it for an IMU sensor in Gazebo?"}),"\n",(0,s.jsx)(n.li,{children:"Describe the different types of output you would expect from a simulated RGB-D camera in Gazebo, and the corresponding ROS 2 message types."}),"\n",(0,s.jsxs)(n.li,{children:["How does the ",(0,s.jsx)(n.code,{children:"ros_gz_sim"})," bridge facilitate the transfer of data from Gazebo's internal sensor topics to ROS 2 topics?"]}),"\n",(0,s.jsx)(n.li,{children:"If you wanted to simulate a 3D LiDAR (e.g., a Velodyne-like sensor), what key parameters would you adjust in its SDF definition compared to a 2D LiDAR?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"real-world-applications",children:"Real-world Applications"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Autonomous Navigation Testing:"})," Generating realistic LiDAR and camera data in varied environments to train and test path planning and obstacle avoidance algorithms for humanoid robots."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception Algorithm Development:"})," Providing diverse datasets for developing object detection, segmentation, and tracking algorithms without needing physical hardware."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Fusion for State Estimation:"})," Combining simulated IMU, odometry, and visual data to develop robust localization and mapping (SLAM) systems."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Humanoid Interaction Perception:"})," Using simulated RGB-D cameras to recognize gestures, human poses, or track objects for human-robot interaction studies."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"edge-cases",children:"Edge Cases"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Impact:"})," High-resolution cameras and dense LiDAR scans at high update rates can significantly slow down the Gazebo simulation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Incorrect Sensor Placement/Orientation:"})," Improper ",(0,s.jsx)(n.code,{children:"pose"})," in the sensor definition can lead to misleading data (e.g., camera pointing backward)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Environmental Factors:"})," Lack of texture for depth cameras, reflective surfaces for LiDAR, or feature-poor environments can challenge simulated perception algorithms."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Synchronization Issues:"})," Discrepancies in ",(0,s.jsx)(n.code,{children:"update_rate"})," between sensors or between Gazebo and ROS 2 can lead to unsynchronized data streams."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"key-entities",children:(0,s.jsx)(n.strong,{children:"Key Entities"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR (Light Detection and Ranging):"})," A sensor simulated in Gazebo as a ",(0,s.jsx)(n.code,{children:"ray"})," or ",(0,s.jsx)(n.code,{children:"gpu_ray"})," type, producing ",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/LaserScan"})," or ",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/PointCloud2"})," messages."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMU (Inertial Measurement Unit):"})," A sensor simulated in Gazebo as an ",(0,s.jsx)(n.code,{children:"imu"})," type, providing ",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/Imu"})," messages (accelerometer, gyroscope, orientation)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RGB-D Camera:"})," A sensor simulated as ",(0,s.jsx)(n.code,{children:"camera"})," and ",(0,s.jsx)(n.code,{children:"depth_camera"})," types in Gazebo, yielding ",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," (RGB and depth) and ",(0,s.jsx)(n.code,{children:"sensor_msgs/msg/PointCloud2"})," messages."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Plugin:"})," An SDF element within a model's ",(0,s.jsx)(n.code,{children:"link"})," that defines a simulated sensor and its properties, including how its data is published (often via ",(0,s.jsx)(n.code,{children:"ros_gz_sim"})," bridge plugins)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor Noise:"})," Programmable parameters within Gazebo sensor definitions (e.g., ",(0,s.jsx)(n.code,{children:"gaussian"})," noise) to mimic real-world sensor imperfections, critical for robust algorithm development."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"ros_gz_sim"})," (ROS-Ignition Bridge):"]})," A crucial package that enables bidirectional communication between ROS 2 topics and Gazebo (Ignition) simulation topics, allowing ROS 2 nodes to access simulated sensor data and send commands."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.code,{children:"frame_id"}),":"]})," A field in ROS 2 message headers (",(0,s.jsx)(n.code,{children:"std_msgs/Header"}),") that identifies the coordinate frame to which the data is relative, essential for TF2 integration."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"references",children:(0,s.jsx)(n.strong,{children:"References"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Open Robotics. (n.d.). ",(0,s.jsx)(n.em,{children:"Gazebo Documentation: Sensors"}),". (Placeholder citation)"]}),"\n",(0,s.jsxs)(n.li,{children:["Open Robotics. (2022). ",(0,s.jsx)(n.em,{children:"ROS 2 Documentation: sensor_msgs"}),". (Placeholder citation)"]}),"\n",(0,s.jsxs)(n.li,{children:["NVIDIA. (2023). ",(0,s.jsx)(n.em,{children:"Isaac Sim Documentation: ROS 2 Sensors"}),". (Placeholder citation)"]}),"\n",(0,s.jsxs)(n.li,{children:["Quigley, M., et al. (2009). ROS: an open-source Robot Operating System. ",(0,s.jsx)(n.em,{children:"OSROSE. Citeseer"}),". (Placeholder citation)"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var r=i(6540);const s={},a=r.createContext(s);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);