"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[93],{2086:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/what-is-vla","title":"4.1 What Is Vision-Language-Action (VLA)?","description":"Welcome to Module 4: Vision-Language-Action (VLA). As we advance towards truly intelligent humanoid robots, merely sensing and moving is no longer sufficient. The next frontier involves seamless, intuitive interaction with humans and complex environments. This chapter introduces Vision-Language-Action (VLA) systems, a revolutionary paradigm that seeks to bridge the gap between human-like communication and robotic autonomy. VLA systems enable robots to understand natural language commands, interpret visual information from their surroundings, and translate these insights into intelligent physical actions, opening up new possibilities for human-robot collaboration and service.","source":"@site/docs/module-4/01-what-is-vla.mdx","sourceDirName":"module-4","slug":"/module-4/what-is-vla","permalink":"/module-4/what-is-vla","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"what-is-vla","title":"4.1 What Is Vision-Language-Action (VLA)?"},"sidebar":"defaultSidebar","previous":{"title":"Module 4: Humanoid Robot Control","permalink":"/category/module-4-humanoid-robot-control"},"next":{"title":"4.2 Voice Command Systems with Whisper","permalink":"/module-4/voice-command-whisper"}}');var s=i(4848),o=i(8453);const a={id:"what-is-vla",title:"4.1 What Is Vision-Language-Action (VLA)?"},l=void 0,r={},c=[{value:"Goal",id:"goal",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Tools (Conceptual)",id:"tools-conceptual",level:2},{value:"Chapter Sections",id:"chapter-sections",level:2},{value:"4.1.1 The Evolution Towards Intelligent Humanoid Interaction",id:"411-the-evolution-towards-intelligent-humanoid-interaction",level:3},{value:"4.1.2 Defining Vision-Language-Action (VLA)",id:"412-defining-vision-language-action-vla",level:3},{value:"4.1.3 The VLA Architecture Pipeline",id:"413-the-vla-architecture-pipeline",level:3},{value:"4.1.4 Benefits of VLA for Humanoid Robots",id:"414-benefits-of-vla-for-humanoid-robots",level:3},{value:"4.1.5 Challenges in VLA Development",id:"415-challenges-in-vla-development",level:3},{value:"Required Diagrams",id:"required-diagrams",level:2},{value:"Hands-on Labs (Conceptual - foundational chapter)",id:"hands-on-labs-conceptual---foundational-chapter",level:2},{value:"Lab 4.1.1: Deconstructing a VLA Command",id:"lab-411-deconstructing-a-vla-command",level:3},{value:"Expected Output",id:"expected-output",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Real-world Applications",id:"real-world-applications",level:2},{value:"Edge Cases",id:"edge-cases",level:2},{value:"<strong>Key Entities</strong>",id:"key-entities",level:3},{value:"<strong>References</strong>",id:"references",level:3}];function d(n){const e={em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.p,{children:"Welcome to Module 4: Vision-Language-Action (VLA). As we advance towards truly intelligent humanoid robots, merely sensing and moving is no longer sufficient. The next frontier involves seamless, intuitive interaction with humans and complex environments. This chapter introduces Vision-Language-Action (VLA) systems, a revolutionary paradigm that seeks to bridge the gap between human-like communication and robotic autonomy. VLA systems enable robots to understand natural language commands, interpret visual information from their surroundings, and translate these insights into intelligent physical actions, opening up new possibilities for human-robot collaboration and service."}),"\n",(0,s.jsx)(e.h2,{id:"goal",children:"Goal"}),"\n",(0,s.jsx)(e.p,{children:"The goal of this chapter is to teach students what VLA is, its fundamental architecture, and its theoretical underpinnings, explaining how it integrates speech, vision, and LLM-based cognitive planning to enable humanoid robots to respond intelligently to human commands and environmental cues."}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Define Vision-Language-Action (VLA) in the context of advanced robotics."}),"\n",(0,s.jsx)(e.li,{children:"Understand the conceptual architecture of a VLA pipeline and its interlinking components."}),"\n",(0,s.jsx)(e.li,{children:"Grasp the significance of combining vision, language, and action for intelligent robot behavior."}),"\n",(0,s.jsx)(e.li,{children:"Identify the key challenges and benefits of developing VLA systems for humanoid robots."}),"\n",(0,s.jsx)(e.li,{children:"Differentiate VLA from traditional robotics control paradigms."}),"\n",(0,s.jsx)(e.li,{children:"Recognize the role of Large Language Models (LLMs) and advanced perception in VLA."}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Familiarity with ROS 2 concepts (nodes, topics, actions)."}),"\n",(0,s.jsx)(e.li,{children:"Basic understanding of computer vision (object detection, image processing)."}),"\n",(0,s.jsx)(e.li,{children:"Conceptual understanding of natural language processing (NLP) and Large Language Models (LLMs)."}),"\n",(0,s.jsx)(e.li,{children:"Basic understanding of robot kinematics and control."}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision-Language-Action (VLA):"})," An integrated robotic paradigm where robots interpret natural language commands, perceive the world through vision, and execute physical actions based on a high-level cognitive plan."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Natural Language Understanding (NLU):"})," The ability of a system to understand human language, including commands, questions, and context."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cognitive Planning:"})," The process by which an AI system generates a sequence of high-level actions to achieve a given goal, often involving symbolic reasoning and problem-solving."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Grounding:"}),' The task of connecting abstract linguistic concepts (e.g., "the red block") to concrete entities in the robot\'s sensory input (e.g., a specific red object detected in an image).']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robot Manipulation:"})," The ability of a robot to physically interact with objects in its environment, including grasping, placing, and reorienting."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Embodied AI:"})," AI systems that are physically situated in the real world and interact with it through sensors and actuators."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multimodality:"})," The ability of an AI system to process and integrate information from multiple sensory inputs, such as vision and language."]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"tools-conceptual",children:"Tools (Conceptual)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"OpenAI Whisper:"})," For voice-to-text transcription."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ChatGPT / Claude (LLMs):"})," For natural language understanding and cognitive planning."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"YOLO / SAM:"})," For visual object detection and segmentation."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ROS 2 Actions:"})," For executing multi-step robot tasks."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Isaac Sim / Gazebo:"})," For simulated environments."]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"chapter-sections",children:"Chapter Sections"}),"\n",(0,s.jsx)(e.h3,{id:"411-the-evolution-towards-intelligent-humanoid-interaction",children:"4.1.1 The Evolution Towards Intelligent Humanoid Interaction"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"From hard-coded automation to adaptable autonomy."}),"\n",(0,s.jsx)(e.li,{children:"The gap: Robots understanding high-level human intent versus low-level commands."}),"\n",(0,s.jsx)(e.li,{children:"The promise of VLA: intuitive, natural human-robot collaboration."}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"412-defining-vision-language-action-vla",children:"4.1.2 Defining Vision-Language-Action (VLA)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision:"})," Robot's ability to perceive and interpret its environment (objects, scenes, humans)."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language:"})," Robot's ability to understand and process natural language commands or queries."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action:"})," Robot's ability to execute physical tasks in the world, informed by vision and language."]}),"\n",(0,s.jsx)(e.li,{children:"The synergy: How these three modalities combine to create higher-level intelligence."}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"413-the-vla-architecture-pipeline",children:"4.1.3 The VLA Architecture Pipeline"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Voice Input / Text Command:"})," Receiving instructions from a human."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language Understanding (LLM):"})," Parsing the command, extracting intent, and identifying objects/targets."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cognitive Planning (LLM):"})," Decomposing high-level goals into a sequence of executable, robot-centric sub-tasks."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Visual Perception:"})," Identifying and localizing objects mentioned in the command."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Grounding:"})," Linking linguistic entities to visual entities."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Execution:"})," Translating the plan into robot-specific movements (e.g., joint commands, navigation goals)."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feedback Loop:"})," Robot's state and success influencing subsequent planning."]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"414-benefits-of-vla-for-humanoid-robots",children:"4.1.4 Benefits of VLA for Humanoid Robots"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Intuitive Interaction:"})," Natural language is the most human-like interface."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Flexibility and Adaptability:"})," Robots can perform a wider range of tasks in unstructured environments without explicit reprogramming."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reduced Programming Complexity:"})," High-level commands replace detailed code."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Enhanced Human-Robot Collaboration:"})," Facilitating shared workspaces and tasks."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robustness to Ambiguity:"})," LLMs can infer context and handle incomplete instructions better."]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"415-challenges-in-vla-development",children:"4.1.5 Challenges in VLA Development"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Ambiguity in Natural Language:"})," Words can have multiple meanings, context dependence."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Grounding Robustness:"})," Dealing with occlusion, lighting changes, novel objects."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Generalization of Policies:"})," Training robots to apply learned skills to new scenarios."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Error Recovery:"})," How robots gracefully handle unexpected failures or misinterpretations."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Computational Demands:"})," LLMs and advanced vision models are resource-intensive."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Safety:"})," Ensuring reliable and safe execution of tasks based on interpreted commands."]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"required-diagrams",children:"Required Diagrams"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Voice \u2192 LLM \u2192 Plan \u2192 Action Pipeline:"})," A comprehensive flowchart illustrating the entire VLA workflow from human voice command to robot physical action."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"VLA Modality Integration:"})," A diagram (e.g., Venn diagram or interconnected circles) showing how Vision, Language, and Action are distinct yet deeply integrated in VLA."]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-labs-conceptual---foundational-chapter",children:"Hands-on Labs (Conceptual - foundational chapter)"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"This chapter is primarily theoretical and conceptual, setting the stage for the subsequent practical chapters within Module 4. Hands-on exercises here focus on conceptual understanding."}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"lab-411-deconstructing-a-vla-command",children:"Lab 4.1.1: Deconstructing a VLA Command"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Objective:"})," Analyze a complex natural language command and conceptually break it down into the VLA pipeline stages."]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Instructions:"})}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Consider the command:"}),' "Robot, please go to the kitchen, find the red apple on the table, and bring it to me here in the living room."']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Voice Input Stage:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"What is the raw audio input?"}),"\n",(0,s.jsx)(e.li,{children:"What would Whisper (or a similar ASR) transcribe this into?"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language Understanding / Cognitive Planning Stage (LLM):"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"What are the distinct sub-goals or actions embedded in this command?"}),"\n",(0,s.jsx)(e.li,{children:'Identify the objects mentioned and their properties (e.g., "red apple").'}),"\n",(0,s.jsx)(e.li,{children:'Identify the locations mentioned ("kitchen," "table," "living room," "here").'}),"\n",(0,s.jsx)(e.li,{children:'How would the LLM generate a high-level sequence of steps (e.g., "navigate to kitchen," "search for object," "pick up object," "navigate to living room," "deliver object")?'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Visual Object Grounding Stage:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'When the robot is in the "kitchen," how would its vision system identify "the red apple" on "the table"?'}),"\n",(0,s.jsx)(e.li,{children:"What challenges might arise (e.g., other red objects, occlusion)?"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Execution Stage:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'What ROS 2 Actions or sequences of lower-level commands would be invoked for "navigate to kitchen"?'}),"\n",(0,s.jsx)(e.li,{children:'What robotic manipulation primitives would be needed for "pick up the red apple"?'}),"\n",(0,s.jsx)(e.li,{children:'How would "bring it to me here" be translated into a final delivery action?'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feedback Loop:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'What kind of feedback would the robot need (e.g., "apple found," "apple dropped," "path blocked")?'}),"\n",(0,s.jsx)(e.li,{children:"How might this feedback influence the planning or re-planning?"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"expected-output",children:"Expected Output"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"A strong conceptual understanding of VLA systems and their components."}),"\n",(0,s.jsx)(e.li,{children:"The ability to conceptually trace a natural language command through the VLA pipeline."}),"\n",(0,s.jsx)(e.li,{children:"An appreciation for the benefits and inherent challenges of building such intelligent systems."}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"How does VLA fundamentally change the way humans can interact with humanoid robots compared to traditional teleoperation or predefined programs?"}),"\n",(0,s.jsx)(e.li,{children:"Identify and briefly explain the three main components that give VLA its name. How do they interrelate?"}),"\n",(0,s.jsx)(e.li,{children:'In the context of VLA, what is "object grounding," and why is it a non-trivial problem for a robot?'}),"\n",(0,s.jsx)(e.li,{children:"Describe a real-world scenario where a VLA-enabled humanoid robot would provide significant advantages over a robot controlled by a purely vision-based or purely language-based system."}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"real-world-applications",children:"Real-world Applications"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Home-assistant Humanoid Robots:"})," Performing household chores, fetching items, or assisting elderly residents based on spoken commands."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Warehouse Command-driven Robots:"})," Manipulating specific inventory items, restocking shelves, or fulfilling orders based on natural language instructions."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Elder-care and Rehabilitation Robotics:"})," Humanoid robots assisting patients with therapy exercises, providing companionship, or performing personal care tasks through intuitive voice interactions."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Exploration and Inspection:"})," Robots navigating complex environments and reporting findings or taking actions based on high-level human directives."]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"edge-cases",children:"Edge Cases"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Ambiguous Commands:"}),' "Pick up the block" when there are multiple blocks of the same color/shape.']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Unreachable Goals:"}),' Commands that require actions physically impossible for the robot (e.g., "reach the ceiling").']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Dynamic Environment Changes:"})," The environment changes significantly between the time a command is given and when the robot acts upon it."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Misinterpretation of Intent:"})," The LLM's understanding of the command differs from the human's actual intent."]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h3,{id:"key-entities",children:(0,s.jsx)(e.strong,{children:"Key Entities"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision-Language-Action (VLA):"})," An integrated robotic system paradigm that combines visual perception, natural language understanding, and physical action execution to enable intelligent and intuitive human-robot interaction."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Natural Language Understanding (NLU):"})," The computational process of interpreting human language, extracting meaning, and identifying key entities and intents from spoken or written commands."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cognitive Planning:"})," The high-level reasoning capability within a VLA system that translates abstract human goals into a structured, executable sequence of robot actions."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Grounding:"}),' The critical link within VLA that associates linguistic descriptions of objects (e.g., "the red cup") with their corresponding visual representations and physical locations in the robot\'s sensory world.']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Embodied AI:"})," Artificial intelligence that is integrated into physical robotic systems, allowing the AI to learn and act within the real world, experiencing embodiment."]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multimodal AI:"}),' AI systems that can process and make sense of information from multiple data types or "modalities" (e.g., combining visual data with linguistic data).']}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Large Language Models (LLMs):"})," Advanced neural network models trained on vast amounts of text data, capable of understanding, generating, and reasoning with human language, central to VLA's planning capabilities."]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h3,{id:"references",children:(0,s.jsx)(e.strong,{children:"References"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:["Kruijff, G. J. M., et al. (2012). The Gaze of the Robot: On the Role of Attention in Human-Robot Interaction. ",(0,s.jsx)(e.em,{children:"ACM Transactions on Interactive Intelligent Systems, 2"}),"(2), 1-28. (Placeholder citation)"]}),"\n",(0,s.jsxs)(e.li,{children:["Paxton, C., et al. (2019). Rethinking Robotic Perception with Deep Learning. ",(0,s.jsx)(e.em,{children:"Science Robotics, 4"}),"(36), eaax2340. (Placeholder citation)"]}),"\n",(0,s.jsxs)(e.li,{children:["Singh, R., et al. (2022). SayCan: Learning Language Grounded Robotic Skills from Natural Language Instructions. ",(0,s.jsx)(e.em,{children:"Conference on Robot Learning (CoRL)"}),". (Placeholder citation)"]}),"\n",(0,s.jsxs)(e.li,{children:["Huang, K., et al. (2022). Inner Monologue: Empowering Large Language Models to Reason about Physical Interactions. ",(0,s.jsx)(e.em,{children:"arXiv preprint arXiv:2207.05697"}),". (Placeholder citation)"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var t=i(6540);const s={},o=t.createContext(s);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);