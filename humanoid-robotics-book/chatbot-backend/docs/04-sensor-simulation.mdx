---
id: sensor-simulation
title: 2.4 Sensor Simulation (LiDAR, Depth, IMU)
---
With a physics-accurate environment established in Gazebo, the next crucial step for developing intelligent humanoid robots is to equip them with virtual senses. Just as in the real world, robots rely heavily on sensor data to perceive their surroundings, localize themselves, and make informed decisions. This chapter focuses on simulating essential robotics sensors—LiDAR, IMU, and RGB-D cameras—within Gazebo, enabling you to generate realistic data streams for perception algorithms and control systems.

## Goal

The goal of this chapter is to teach students how to simulate various sensors (LiDAR, IMU, RGB-D cameras) within Gazebo, enabling them to test sensor-driven robot behaviors, perception algorithms, and data fusion techniques for humanoid robots.

## Learning Objectives

*   Understand the principles behind common robotics sensors: LiDAR, IMU, and RGB-D cameras.
*   Learn how to integrate sensor plugins into Gazebo models via SDF.
*   Configure parameters for simulated LiDAR sensors (e.g., range, resolution, scan rate).
*   Configure parameters for simulated IMU sensors (e.g., noise, bias, update rate).
*   Set up RGB-D cameras to generate color images, depth maps, and point clouds.
*   Bridge simulated sensor data from Gazebo to ROS 2 topics for processing by downstream nodes.
*   Grasp the concept of sensor noise and its importance in simulation.

## Prerequisites

*   A functional Gazebo world and an understanding of SDF/URDF from previous chapters.
*   Familiarity with ROS 2 topics and message types, especially for sensor data (`sensor_msgs`).
*   Basic understanding of how real-world LiDAR, IMU, and cameras function.

## Key Concepts

*   **Sensor Plugin:** A Gazebo plugin that simulates the behavior of a physical sensor, generating data and often publishing it to ROS 2 topics.
*   **LiDAR (Light Detection and Ranging):** A sensor that measures distances to objects by illuminating the target with laser light and measuring the reflection time. Simulated as a `ray` sensor in Gazebo.
*   **Point Cloud:** A set of data points in a 3D coordinate system, typically generated by LiDAR or depth cameras.
*   **IMU (Inertial Measurement Unit):** A sensor that measures a body's specific force (acceleration) and angular rate (gyroscope), and sometimes magnetic field (magnetometer). Simulated to provide `Imu` messages.
*   **RGB-D Camera:** A camera that captures both color (RGB) images and depth information (D), often using structured light or time-of-flight principles. Simulated to produce `Image` and `PointCloud2` messages.
*   **Sensor Noise:** Unwanted variations or fluctuations in sensor measurements, crucial to simulate for realistic algorithm testing.
*   **Sensor Fusion:** Combining data from multiple sensors to obtain a more accurate or complete understanding of the environment or robot state.
*   **ROS 2 Bridge:** The `ros_gz_sim` package, which translates data between Gazebo's internal topics and ROS 2 topics.

## Tools

*   **Gazebo (Ignition Fortress):** The simulation environment.
*   **SDF (Simulation Description Format):** For defining sensor plugins within models.
*   **ROS 2 Humble:** For receiving and processing simulated sensor data.
*   **RViz2:** For visualizing sensor data (point clouds, images, IMU orientation).
*   **`ros_gz_sim`:** The ROS 2 to Ignition Gazebo bridge.

## Chapter Sections

### 2.4.1 Introduction to Sensor Simulation and the Reality Gap

*   Why simulate sensors? Safety, cost, reproducibility, synthetic data generation.
*   Understanding the "reality gap" and how realistic sensor simulation helps bridge it.
*   The importance of modeling sensor noise and limitations.

### 2.4.2 Simulating LiDAR Sensors

*   **LiDAR Principles:** How it works (briefly).
*   **Gazebo's `gpu_ray` and `ray` sensors:**
    *   Defining the sensor type in SDF (`<sensor type='gpu_ray'>` or `<sensor type='ray'>`).
    *   **Configuration:**
        *   `horizontal/scan/ranges`: min angle, max angle, resolution.
        *   `vertical/scan/ranges`: (for 3D LiDARs).
        *   `range`: min, max range, resolution.
        *   `update_rate`: How often the sensor publishes data.
        *   `noise`: Adding Gaussian noise to range measurements.
        *   `ros/topic`: Publishing data to a ROS 2 topic (`sensor_msgs/msg/LaserScan` or `sensor_msgs/msg/PointCloud2`).
*   Integrating a LiDAR sensor onto a humanoid model.

### 2.4.3 Simulating IMU Sensors

*   **IMU Principles:** Accelerometers and gyroscopes.
*   **Gazebo's `imu` sensor:**
    *   Defining the sensor type (`<sensor type='imu'>`).
    *   **Configuration:**
        *   `orientation_reference_frame`: Which frame defines "north" and "down."
        *   `update_rate`: Sensor data update frequency.
        *   `noise`: Modeling gyroscope and accelerometer noise (`rate_noise`, `rate_bias_stability`, `accel_noise`, `accel_bias_stability`).
        *   `ros/topic`: Publishing data to a ROS 2 topic (`sensor_msgs/msg/Imu`).
*   Placing an IMU on the torso or base of a humanoid.

### 2.4.4 Simulating RGB-D Cameras

*   **RGB-D Camera Principles:** Color image, depth image, and point clouds.
*   **Gazebo's `camera` and `depth_camera` sensors:**
    *   Defining the sensor type (`<sensor type='camera'>`, `<sensor type='depth_camera'>`).
    *   **Configuration:**
        *   `image`: width, height, format.
        *   `clip`: near, far planes.
        *   `horizontal_fov`, `vertical_fov`.
        *   `update_rate`.
        *   `noise`: Gaussian, Poisson.
        *   `ros/camera_info_topic`, `ros/image_topic`, `ros/depth_image_topic`, `ros/point_cloud_topic`: Publishing to ROS 2.
*   Mounting an RGB-D camera on a humanoid's head.

### 2.4.5 Bridging Sensor Data to ROS 2 and Visualization

*   Utilizing `ros_gz_sim` plugins (e.g., `ignition_ros_sensor_bridge`) within your Gazebo world or robot SDF to enable ROS 2 communication for each sensor.
*   Subscribing to simulated sensor topics in ROS 2 nodes.
*   Visualizing `LaserScan`, `PointCloud2`, and `Image` messages in RViz2.
*   **TF2 Integration:** Ensuring sensors publish their transforms correctly to the TF2 tree.

## Required Diagrams

*   **LiDAR Scan Pattern:** An illustration showing how a LiDAR sweeps to generate distance measurements.
*   **RGB-D Camera Output:** A visual representation of an RGB image, corresponding depth map, and generated point cloud.
*   **Simulated Sensor Data Flow:** A diagram showing Gazebo sensor plugins generating data, `ros_gz_sim` bridging it, and ROS 2 nodes consuming it.

## Hands-on Labs

### Lab 2.4.1: Adding Camera + LiDAR Plugins to a Humanoid URDF

**Objective:** Enhance the `simple_humanoid` URDF from Module 1 by integrating simulated LiDAR and RGB-D camera sensors, then visualize their output in RViz2 while the robot is in Gazebo.

**Prerequisites:** Completed Lab 1.3.1 (humanoid URDF) and Lab 2.2.1 (basic Gazebo world). Ensure `ros-humble-ros-ign-gazebo` and `ros-humble-ros-ign-bridge` are installed.

**Instructions:**

1.  **Modify `my_humanoid_description/urdf/humanoid.urdf.xacro`:**
    *   Add a camera link and a LiDAR link.
    *   Attach them to the `head_link` or `torso_link` using `fixed` joints.
    *   Define the sensor plugins within the respective links' Gazebo tags.

    ```xml
    <?xml version="1.0"?>
    <robot name="simple_humanoid" xmlns:xacro="http://ros.org/xacro">

        <xacro:property name="M_PI" value="3.1415926535897931" />

        <!-- ... existing base_link, torso_link, torso_joint, head_link, head_yaw_joint ... -->
        <!-- ... existing arm_segment macro and right arm instance ... -->

        <!-- CAMERA SENSOR -->
        <link name="camera_link">
            <visual>
                <geometry><box size="0.02 0.05 0.05"/></geometry>
                <material name="black"><color rgba="0 0 0 1"/></material>
            </visual>
            <collision>
                <geometry><box size="0.02 0.05 0.05"/></geometry>
            </collision>
            <inertial>
                <mass value="0.1"/>
                <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>
            </inertial>
        </link>
        <joint name="camera_joint" type="fixed">
            <parent link="head_link"/>
            <child link="camera_link"/>
            <origin xyz="0.06 0 0" rpy="0 0 0"/> <!-- Slightly in front of the head -->
        </joint>

        <gazebo reference="camera_link">
            <sensor name="camera_sensor" type="camera">
                <pose>0 0 0 0 0 0</pose>
                <visualize>true</visualize>
                <update_rate>30.0</update_rate>
                <camera>
                    <horizontal_fov>1.047</horizontal_fov>
                    <image>
                        <width>640</width>
                        <height>480</height>
                        <format>R8G8B8</format>
                    </image>
                    <clip>
                        <near>0.05</near>
                        <far>300</far>
                    </clip>
                    <noise>
                        <type>gaussian</type>
                        <mean>0.0</mean>
                        <stddev>0.007</stddev>
                    </noise>
                </camera>
                <plugin name="camera_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">
                    <ros>
                        <namespace>/simple_humanoid/camera</namespace>
                        <ros_remap>rgb/image_raw:=image_raw</ros_remap>
                        <ros_remap>rgb/camera_info:=camera_info</ros_remap>
                    </ros>
                    <sensor_name>camera_sensor</sensor_name>
                    <frame_id>camera_link</frame_id>
                </plugin>
            </sensor>
            <sensor name="depth_camera_sensor" type="depth_camera">
                <pose>0 0 0 0 0 0</pose>
                <visualize>false</visualize>
                <update_rate>30.0</update_rate>
                <camera>
                    <horizontal_fov>1.047</horizontal_fov>
                    <image>
                        <width>640</width>
                        <height>480</height>
                        <format>L_INT16</format>
                    </image>
                    <clip>
                        <near>0.05</near>
                        <far>10</far>
                    </clip>
                    <noise>
                        <type>gaussian</type>
                        <mean>0.0</mean>
                        <stddev>0.007</stddev>
                    </noise>
                </camera>
                <plugin name="depth_camera_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">
                    <ros>
                        <namespace>/simple_humanoid/camera</namespace>
                        <ros_remap>depth/image_raw:=depth_image_raw</ros_remap>
                        <ros_remap>depth/camera_info:=depth_camera_info</ros_remap>
                        <ros_remap>points:=depth/points</ros_remap>
                    </ros>
                    <sensor_name>depth_camera_sensor</sensor_name>
                    <frame_id>camera_link</frame_id>
                </plugin>
            </sensor>
        </gazebo>

        <!-- LIDAR SENSOR -->
        <link name="lidar_link">
            <visual>
                <geometry><cylinder radius="0.03" length="0.04"/></geometry>
                <material name="grey"><color rgba="0.5 0.5 0.5 1"/></material>
            </visual>
            <collision>
                <geometry><cylinder radius="0.03" length="0.04"/></geometry>
            </collision>
            <inertial>
                <mass value="0.2"/>
                <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>
            </inertial>
        </link>
        <joint name="lidar_joint" type="fixed">
            <parent link="head_link"/>
            <child link="lidar_link"/>
            <origin xyz="0.07 0 0.05" rpy="0 0 0"/> <!-- Slightly above and in front of head -->
        </joint>

        <gazebo reference="lidar_link">
            <sensor name="lidar_sensor" type="gpu_ray">
                <pose>0 0 0 0 0 0</pose>
                <visualize>true</visualize>
                <update_rate>10.0</update_rate>
                <ray>
                    <scan>
                        <horizontal>
                            <samples>360</samples>
                            <resolution>1</resolution>
                            <min_angle>-${M_PI}</min_angle>
                            <max_angle>${M_PI}</max_angle>
                        </horizontal>
                    </scan>
                    <range>
                        <min>0.1</min>
                        <max>10.0</max>
                        <resolution>0.01</resolution>
                    </range>
                    <noise>
                        <type>gaussian</type>
                        <mean>0.0</mean>
                        <stddev>0.01</stddev>
                    </noise>
                </ray>
                <plugin name="lidar_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">
                    <ros>
                        <namespace>/simple_humanoid/lidar</namespace>
                        <ros_remap>scan:=scan</ros_remap>
                        <ros_remap>points:=points</ros_remap>
                    </ros>
                    <sensor_name>lidar_sensor</sensor_name>
                    <frame_id>lidar_link</frame_id>
                </plugin>
            </sensor>
        </gazebo>

        <!-- IMU SENSOR -->
        <link name="imu_link">
            <inertial>
                <mass value="0.01"/>
                <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>
            </inertial>
        </link>
        <joint name="imu_joint" type="fixed">
            <parent link="torso_link"/>
            <child link="imu_link"/>
            <origin xyz="0 0 0" rpy="0 0 0"/> <!-- Centered in torso -->
        </joint>

        <gazebo reference="imu_link">
            <sensor name="imu_sensor" type="imu">
                <always_on>true</always_on>
                <update_rate>100.0</update_rate>
                <visualize>false</visualize>
                <imu>
                    <angular_velocity>
                        <x>
                            <noise type="gaussian">
                                <mean>0.0</mean>
                                <stddev>2e-4</stddev>
                                <bias_mean>0.0000075</bias_mean>
                                <bias_stddev>0.0000008</bias_stddev>
                            </noise>
                        </x>
                        <y>
                            <noise type="gaussian">
                                <mean>0.0</mean>
                                <stddev>2e-4</stddev>
                                <bias_mean>0.0000075</bias_mean>
                                <bias_stddev>0.0000008</bias_stddev>
                            </noise>
                        </y>
                        <z>
                            <noise type="gaussian">
                                <mean>0.0</mean>
                                <stddev>2e-4</stddev>
                                <bias_mean>0.0000075</bias_mean>
                                <bias_stddev>0.0000008</bias_stddev>
                            </noise>
                        </z>
                    </angular_velocity>
                    <linear_acceleration>
                        <x>
                            <noise type="gaussian">
                                <mean>0.0</mean>
                                <stddev>2e-2</stddev>
                                <bias_mean>0.1</bias_mean>
                                <bias_stddev>0.001</bias_stddev>
                            </noise>
                        </x>
                        <y>
                            <noise type="gaussian">
                                <mean>0.0</mean>
                                <stddev>2e-2</stddev>
                                <bias_mean>0.1</bias_mean>
                                <bias_stddev>0.001</bias_stddev>
                            </noise>
                        </y>
                        <z>
                            <noise type="gaussian">
                                <mean>0.0</mean>
                                <stddev>2e-2</stddev>
                                <bias_mean>0.1</bias_mean>
                                <bias_stddev>0.001</bias_stddev>
                            </noise>
                            </noise>
                        </z>
                    </linear_acceleration>
                </imu>
                <plugin name="imu_controller" filename="libignition-gazebo-sensors-ros2-bridge-plugin.so">
                    <ros>
                        <namespace>/simple_humanoid/imu</namespace>
                        <ros_remap>data:=data</ros_remap>
                    </ros>
                    <sensor_name>imu_sensor</sensor_name>
                    <frame_id>imu_link</frame_id>
                </plugin>
            </sensor>
        </gazebo>

    </robot>
    ```
2.  **Generate the updated URDF:**
    ```bash
    cd <your_ros2_ws>/src/my_humanoid_description/urdf
    ros2 run xacro xacro humanoid.urdf.xacro > humanoid.urdf
    ```
3.  **Update `my_humanoid_description/launch/display_humanoid.launch.py` to spawn the robot in Gazebo and open RViz2:**
    ```python
    import os
    from ament_index_python.packages import get_package_share_directory
    from launch import LaunchDescription
    from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, OpaqueFunction
    from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
    from launch.launch_description_sources import PythonLaunchDescriptionSource
    from launch_ros.actions import Node
    from launch_ros.substitutions import FindPackageShare

    def launch_setup(context, *args, **kwargs):
        # Get the path to the robot description package
        robot_description_path = get_package_share_directory('my_humanoid_description')
        urdf_file_path = os.path.join(robot_description_path, 'urdf', 'humanoid.urdf')

        # Load the URDF file
        with open(urdf_file_path, 'r') as infp:
            robot_desc = infp.read()

        # Get the path to the simple_room.world file
        world_file_path = os.path.join(
            get_package_share_directory('my_gazebo_worlds'),
            'worlds',
            'simple_room.world'
        )

        # Include the Gazebo launch file with our custom world
        gazebo_launch = IncludeLaunchDescription(
            PythonLaunchDescriptionSource([os.path.join(
                get_package_share_directory('ros_gz_sim'), 'launch'),
                '/gz_sim.launch.py']),
            launch_arguments={
                'gz_args': world_file_path,
                'force_system_paths': 'True' # Ensure models from fuel are found
            }.items()
        )

        # Define the robot state publisher node
        robot_state_publisher_node = Node(
            package='robot_state_publisher',
            executable='robot_state_publisher',
            name='robot_state_publisher',
            output='screen',
            parameters=[{'robot_description': robot_desc,
                         'use_sim_time': True}] # Important for simulated robots
        )

        # Define the joint state publisher GUI node
        joint_state_publisher_gui_node = Node(
            package='joint_state_publisher_gui',
            executable='joint_state_publisher_gui',
            name='joint_state_publisher_gui',
            output='screen'
        )

        # Spawn the robot into Gazebo
        spawn_robot_node = Node(
            package='ros_gz_sim',
            executable='create',
            arguments=[
                '-name', 'simple_humanoid',
                '-topic', 'robot_description',
                '-x', '0.0', '-y', '0.0', '-z', '1.0' # Spawn robot slightly above ground
            ],
            output='screen'
        )

        # RViz2 Node (re-using previous setup)
        # Create rviz config directory if it doesn't exist
        rviz_config_dir = os.path.join(get_package_share_directory('my_humanoid_description'), 'rviz')
        if not os.path.exists(rviz_config_dir):
            os.makedirs(rviz_config_dir)
        # Point to a default rviz config or create a minimal one
        default_rviz_config_path = os.path.join(rviz_config_dir, 'humanoid_with_sensors.rviz')
        if not os.path.exists(default_rviz_config_path):
            with open(default_rviz_config_path, 'w') as f:
                f.write("""
Global Options:
  Fixed Frame: base_link
Displays:
  - Class: rviz_default_plugins/RobotModel
    Name: RobotModel
    Enabled: true
    Robot Description: robot_description
  - Class: rviz_default_plugins/Image
    Name: RGB Image
    Topic: /simple_humanoid/camera/image_raw
    Enabled: true
  - Class: rviz_default_plugins/PointCloud2
    Name: Depth PointCloud
    Topic: /simple_humanoid/camera/depth/points
    Enabled: true
  - Class: rviz_default_plugins/LaserScan
    Name: LiDAR Scan
    Topic: /simple_humanoid/lidar/scan
    Enabled: true
  - Class: rviz_default_plugins/Imu
    Name: IMU
    Topic: /simple_humanoid/imu/data
    Enabled: true
    Covariance Type: 'ellipses'
    Linear Acceleration: true
    Angular Velocity: true
    Orientation: true
    Orientation Show Arrows: true
    Orientation Arrow Length: 0.5
    Flat 2D: false
    Unreliable: false
    Color: 255; 25; 25
""")
        rviz_node = Node(
            package='rviz2',
            executable='rviz2',
            name='rviz2',
            output='screen',
            arguments=['-d', default_rviz_config_path]
        )

        return [
            gazebo_launch,
            robot_state_publisher_node,
            joint_state_publisher_gui_node,
            spawn_robot_node,
            rviz_node
        ]

    def generate_launch_description():
        return LaunchDescription([OpaqueFunction(function=launch_setup)])
    ```
4.  **Add `ros_gz_sim` as a dependency to `my_humanoid_description/package.xml`:**
    ```xml
    <!-- ... -->
      <depend>ros_gz_sim</depend>
      <exec_depend>joint_state_publisher_gui</exec_depend>
    <!-- ... -->
    ```
5.  **Build both `my_humanoid_description` and `my_gazebo_worlds` packages and source your workspace:**
    ```bash
    cd <your_ros2_ws>
    colcon build --packages-select my_humanoid_description my_gazebo_worlds
    source install/setup.bash
    ```
6.  **Launch the combined Gazebo and RViz2 setup:**
    ```bash
    ros2 launch my_humanoid_description display_humanoid.launch.py
    ```
    You should see Gazebo GUI with your simple room and the humanoid robot. RViz2 should also launch, displaying the robot model, its camera feed, depth point cloud, LiDAR scan, and IMU data (as the robot falls to the ground).

## Expected Output

*   An updated humanoid URDF (or Xacro) that includes simulated LiDAR, RGB-D camera, and IMU sensors.
*   A Gazebo simulation where the humanoid robot with its sensors is spawned in your custom world.
*   RViz2 displaying the robot model and real-time data streams from the simulated sensors (images, point clouds, laser scans, IMU orientation/acceleration).
*   Understanding of how to configure sensor plugins in URDF/SDF and bridge their data to ROS 2.

## Assessment Questions

*   Why is it crucial to simulate sensor noise, and how can you configure it for an IMU sensor in Gazebo?
*   Describe the different types of output you would expect from a simulated RGB-D camera in Gazebo, and the corresponding ROS 2 message types.
*   How does the `ros_gz_sim` bridge facilitate the transfer of data from Gazebo's internal sensor topics to ROS 2 topics?
*   If you wanted to simulate a 3D LiDAR (e.g., a Velodyne-like sensor), what key parameters would you adjust in its SDF definition compared to a 2D LiDAR?

## Real-world Applications

*   **Autonomous Navigation Testing:** Generating realistic LiDAR and camera data in varied environments to train and test path planning and obstacle avoidance algorithms for humanoid robots.
*   **Perception Algorithm Development:** Providing diverse datasets for developing object detection, segmentation, and tracking algorithms without needing physical hardware.
*   **Sensor Fusion for State Estimation:** Combining simulated IMU, odometry, and visual data to develop robust localization and mapping (SLAM) systems.
*   **Humanoid Interaction Perception:** Using simulated RGB-D cameras to recognize gestures, human poses, or track objects for human-robot interaction studies.

## Edge Cases

*   **Performance Impact:** High-resolution cameras and dense LiDAR scans at high update rates can significantly slow down the Gazebo simulation.
*   **Incorrect Sensor Placement/Orientation:** Improper `pose` in the sensor definition can lead to misleading data (e.g., camera pointing backward).
*   **Environmental Factors:** Lack of texture for depth cameras, reflective surfaces for LiDAR, or feature-poor environments can challenge simulated perception algorithms.
*   **Synchronization Issues:** Discrepancies in `update_rate` between sensors or between Gazebo and ROS 2 can lead to unsynchronized data streams.

---

### **Key Entities**

*   **LiDAR (Light Detection and Ranging):** A sensor simulated in Gazebo as a `ray` or `gpu_ray` type, producing `sensor_msgs/msg/LaserScan` or `sensor_msgs/msg/PointCloud2` messages.
*   **IMU (Inertial Measurement Unit):** A sensor simulated in Gazebo as an `imu` type, providing `sensor_msgs/msg/Imu` messages (accelerometer, gyroscope, orientation).
*   **RGB-D Camera:** A sensor simulated as `camera` and `depth_camera` types in Gazebo, yielding `sensor_msgs/msg/Image` (RGB and depth) and `sensor_msgs/msg/PointCloud2` messages.
*   **Sensor Plugin:** An SDF element within a model's `link` that defines a simulated sensor and its properties, including how its data is published (often via `ros_gz_sim` bridge plugins).
*   **Sensor Noise:** Programmable parameters within Gazebo sensor definitions (e.g., `gaussian` noise) to mimic real-world sensor imperfections, critical for robust algorithm development.
*   **`ros_gz_sim` (ROS-Ignition Bridge):** A crucial package that enables bidirectional communication between ROS 2 topics and Gazebo (Ignition) simulation topics, allowing ROS 2 nodes to access simulated sensor data and send commands.
*   **`frame_id`:** A field in ROS 2 message headers (`std_msgs/Header`) that identifies the coordinate frame to which the data is relative, essential for TF2 integration.

---

### **References**

*   Open Robotics. (n.d.). *Gazebo Documentation: Sensors*. (Placeholder citation)
*   Open Robotics. (2022). *ROS 2 Documentation: sensor_msgs*. (Placeholder citation)
*   NVIDIA. (2023). *Isaac Sim Documentation: ROS 2 Sensors*. (Placeholder citation)
*   Quigley, M., et al. (2009). ROS: an open-source Robot Operating System. *OSROSE. Citeseer*. (Placeholder citation)
